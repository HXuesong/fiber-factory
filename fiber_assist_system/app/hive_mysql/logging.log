[2017-10-31 16:45:03,571] app.hive.AzkabanMonitor [INFO] URL: https://172.23.27.203:8443
[2017-10-31 16:45:03,738] app.hive.AzkabanMonitor [INFO] Success: Login https://172.23.27.203:8443 with user azkaban
[2017-10-31 16:45:03,738] app.hive.AzkabanMonitor [DEBUG] {'session.id': '5d92190d-cc4c-4fad-97ab-fdb6bb3087c9', 'status': 'success'}
[2017-10-31 16:45:03,758] app.hive.AzkabanMonitor [INFO] Success: Fetch Executions of a Flow - run_dataimport
[2017-10-31 16:45:03,758] app.hive.AzkabanMonitor [DEBUG] {'total': 2, 'executions': [{'submitTime': 1509438862311, 'submitUser': 'azkaban', 'startTime': 1509438862468, 'endTime': 1509438932606, 'flowId': 'run_dataimport', 'projectId': 68, 'execId': 1654, 'status': 'KILLED'}, {'submitTime': 1509438455131, 'submitUser': 'azkaban', 'startTime': 1509438455403, 'endTime': 1509438536090, 'flowId': 'run_dataimport', 'projectId': 68, 'execId': 1653, 'status': 'KILLED'}], 'length': 10, 'project': 'sql_to_hive', 'from': 0, 'projectId': 68, 'flow': 'run_dataimport'}
[2017-10-31 16:45:03,764] app.hive.AzkabanMonitor [INFO] Success: Fetch Execution Job Logs - 1654-run_dataimport
[2017-10-31 16:45:03,764] app.hive.AzkabanMonitor [DEBUG] {'offset': 0, 'data': '31-10-2017 16:34:22 CST run_dataimport INFO - Starting job run_dataimport at 1509438862472\n31-10-2017 16:34:22 CST run_dataimport INFO - azkaban.webserver.url property was not set\n31-10-2017 16:34:22 CST run_dataimport INFO - job JVM args: -Dazkaban.flowid=run_dataimport -Dazkaban.execid=1654 -Dazkaban.jobid=run_dataimport\n31-10-2017 16:34:22 CST run_dataimport INFO - Building command job executor. \n31-10-2017 16:34:22 CST run_dataimport INFO - Memory granted for job run_dataimport\n31-10-2017 16:34:22 CST run_dataimport INFO - 1 commands to execute.\n31-10-2017 16:34:22 CST run_dataimport INFO - cwd=/home/azkaban/azkaban_bk/azkaban-exec-server-3.35.0-22-g5517d50/executions/1654\n31-10-2017 16:34:22 CST run_dataimport INFO - effective user is: azkaban\n31-10-2017 16:34:22 CST run_dataimport INFO - Command: sh /home/sql_to_hive/dataimport.sh\n31-10-2017 16:34:22 CST run_dataimport INFO - Environment variables: {JOB_OUTPUT_PROP_FILE=/home/azkaban/azkaban_bk/azkaban-exec-server-3.35.0-22-g5517', 'length': 1000}
[2017-10-31 16:48:54,437] app.hive.AzkabanMonitor [INFO] URL: https://172.23.27.203:8443
[2017-10-31 16:48:54,597] app.hive.AzkabanMonitor [INFO] Success: Login https://172.23.27.203:8443 with user azkaban
[2017-10-31 16:48:54,598] app.hive.AzkabanMonitor [DEBUG] {'session.id': '54d8b3b0-25ae-45fb-9e75-32e7db5afc4b', 'status': 'success'}
[2017-10-31 16:48:54,618] app.hive.AzkabanMonitor [INFO] Success: Fetch Executions of a Flow - run_dataimport
[2017-10-31 16:48:54,618] app.hive.AzkabanMonitor [DEBUG] {'total': 2, 'executions': [{'submitTime': 1509438862311, 'submitUser': 'azkaban', 'startTime': 1509438862468, 'endTime': 1509438932606, 'flowId': 'run_dataimport', 'projectId': 68, 'execId': 1654, 'status': 'KILLED'}, {'submitTime': 1509438455131, 'submitUser': 'azkaban', 'startTime': 1509438455403, 'endTime': 1509438536090, 'flowId': 'run_dataimport', 'projectId': 68, 'execId': 1653, 'status': 'KILLED'}], 'length': 10, 'project': 'sql_to_hive', 'from': 0, 'projectId': 68, 'flow': 'run_dataimport'}
[2017-10-31 16:48:54,626] app.hive.AzkabanMonitor [INFO] Success: Fetch Execution Job Logs - 1654-run_dataimport
[2017-10-31 16:48:54,626] app.hive.AzkabanMonitor [DEBUG] {'offset': 0, 'data': '31-10-2017 16:34:22 CST run_dataimport INFO - Starting job run_dataimport at 1509438862472\n31-10-2017 16:34:22 CST run_dataimport INFO - azkaban.webserver.url property was not set\n31-10-2017 16:34:22 CST run_dataimport INFO - job JVM args: -Dazkaban.flowid=run_dataimport -Dazkaban.execid=1654 -Dazkaban.jobid=run_dataimport\n31-10-2017 16:34:22 CST run_dataimport INFO - Building command job executor. \n31-10-2017 16:34:22 CST run_dataimport INFO - Memory granted for job run_dataimport\n31-10-2017 16:34:22 CST run_dataimport INFO - 1 commands to execute.\n31-10-2017 16:34:22 CST run_dataimport INFO - cwd=/home/azkaban/azkaban_bk/azkaban-exec-server-3.35.0-22-g5517d50/executions/1654\n31-10-2017 16:34:22 CST run_dataimport INFO - effective user is: azkaban\n31-10-2017 16:34:22 CST run_dataimport INFO - Command: sh /home/sql_to_hive/dataimport.sh\n31-10-2017 16:34:22 CST run_dataimport INFO - Environment variables: {JOB_OUTPUT_PROP_FILE=/home/azkaban/azkaban_bk/azkaban-exec-server-3.35.0-22-g5517', 'length': 1000}
[2017-11-01 09:51:26,841] app.hive_mysql.AzkabanMonitor [INFO] URL: https://172.23.27.203:8443
[2017-11-01 09:51:27,049] app.hive_mysql.AzkabanMonitor [INFO] Success: Login https://172.23.27.203:8443 with user azkaban
[2017-11-01 09:51:27,049] app.hive_mysql.AzkabanMonitor [DEBUG] {'session.id': '106ad2cb-bc60-4cf9-99ab-74b701ffdb67', 'status': 'success'}
[2017-11-01 09:51:27,064] app.hive_mysql.AzkabanMonitor [INFO] Success: Fetch Executions of a Flow - run_dataimport
[2017-11-01 09:51:27,064] app.hive_mysql.AzkabanMonitor [DEBUG] {'total': 5, 'executions': [{'submitTime': 1509494450033, 'submitUser': 'azkaban', 'startTime': 1509494450241, 'endTime': 1509496877667, 'flowId': 'run_dataimport', 'projectId': 68, 'execId': 1657, 'status': 'SUCCEEDED'}, {'submitTime': 1509458449965, 'submitUser': 'azkaban', 'startTime': 1509458450181, 'endTime': 1509489787442, 'flowId': 'run_dataimport', 'projectId': 68, 'execId': 1656, 'status': 'SUCCEEDED'}, {'submitTime': 1509444049936, 'submitUser': 'azkaban', 'startTime': 1509444050155, 'endTime': 1509457451629, 'flowId': 'run_dataimport', 'projectId': 68, 'execId': 1655, 'status': 'SUCCEEDED'}, {'submitTime': 1509438862311, 'submitUser': 'azkaban', 'startTime': 1509438862468, 'endTime': 1509438932606, 'flowId': 'run_dataimport', 'projectId': 68, 'execId': 1654, 'status': 'KILLED'}, {'submitTime': 1509438455131, 'submitUser': 'azkaban', 'startTime': 1509438455403, 'endTime': 1509438536090, 'flowId': 'run_dataimport', 'projectId': 68, 'execId': 1653, 'status': 'KILLED'}], 'length': 10, 'project': 'sql_to_hive', 'from': 0, 'projectId': 68, 'flow': 'run_dataimport'}
[2017-11-01 09:51:27,073] app.hive_mysql.AzkabanMonitor [INFO] Success: Fetch Execution Job Logs - 1657-run_dataimport
[2017-11-01 09:51:27,073] app.hive_mysql.AzkabanMonitor [DEBUG] {'offset': 0, 'data': '01-11-2017 08:00:50 CST run_dataimport INFO - Starting job run_dataimport at 1509494450244\n01-11-2017 08:00:50 CST run_dataimport INFO - azkaban.webserver.url property was not set\n01-11-2017 08:00:50 CST run_dataimport INFO - job JVM args: -Dazkaban.flowid=run_dataimport -Dazkaban.execid=1657 -Dazkaban.jobid=run_dataimport\n01-11-2017 08:00:50 CST run_dataimport INFO - Building command job executor. \n01-11-2017 08:00:50 CST run_dataimport INFO - Memory granted for job run_dataimport\n01-11-2017 08:00:50 CST run_dataimport INFO - 1 commands to execute.\n01-11-2017 08:00:50 CST run_dataimport INFO - cwd=/home/azkaban/azkaban_bk/azkaban-exec-server-3.35.0-22-g5517d50/executions/1657\n01-11-2017 08:00:50 CST run_dataimport INFO - effective user is: azkaban\n01-11-2017 08:00:50 CST run_dataimport INFO - Command: sh /home/sql_to_hive/dataimport.sh\n01-11-2017 08:00:50 CST run_dataimport INFO - Environment variables: {JOB_OUTPUT_PROP_FILE=/home/azkaban/azkaban_bk/azkaban-exec-server-3.35.0-22-g5517', 'length': 1000}
[2017-11-01 09:52:54,994] app.hive_mysql.AzkabanMonitor [INFO] URL: https://172.23.27.203:8443
[2017-11-01 09:52:55,161] app.hive_mysql.AzkabanMonitor [INFO] Success: Login https://172.23.27.203:8443 with user azkaban
[2017-11-01 09:52:55,161] app.hive_mysql.AzkabanMonitor [DEBUG] {'session.id': '7b301dfa-5540-43ba-8f08-a0df3eb58465', 'status': 'success'}
[2017-11-01 09:52:55,171] app.hive_mysql.AzkabanMonitor [INFO] Success: Fetch Executions of a Flow - start-syn
[2017-11-01 09:52:55,172] app.hive_mysql.AzkabanMonitor [DEBUG] {'total': 5, 'executions': [{'submitTime': 1509438157461, 'submitUser': 'azkaban', 'startTime': 1509438157639, 'endTime': 1509438189521, 'flowId': 'start-syn', 'projectId': 66, 'execId': 1652, 'status': 'KILLED'}, {'submitTime': 1509438102634, 'submitUser': 'azkaban', 'startTime': 1509438102800, 'endTime': 1509438119797, 'flowId': 'start-syn', 'projectId': 66, 'execId': 1651, 'status': 'KILLED'}, {'submitTime': 1509435165409, 'submitUser': 'azkaban', 'startTime': 1509435165665, 'endTime': 1509435183654, 'flowId': 'start-syn', 'projectId': 66, 'execId': 1649, 'status': 'KILLED'}, {'submitTime': 1509435136471, 'submitUser': 'azkaban', 'startTime': 1509435136636, 'endTime': 1509435145535, 'flowId': 'start-syn', 'projectId': 66, 'execId': 1648, 'status': 'KILLED'}, {'submitTime': 1509435094803, 'submitUser': 'azkaban', 'startTime': 1509435095118, 'endTime': 1509435128253, 'flowId': 'start-syn', 'projectId': 66, 'execId': 1647, 'status': 'KILLED'}], 'length': 10, 'project': 'mysql_syn', 'from': 0, 'projectId': 66, 'flow': 'start-syn'}
[2017-11-01 09:52:55,181] app.hive_mysql.AzkabanMonitor [INFO] Success: Fetch Execution Job Logs - 1652-start-syn
[2017-11-01 09:52:55,181] app.hive_mysql.AzkabanMonitor [DEBUG] {'offset': 0, 'data': '31-10-2017 16:22:37 CST start-syn INFO - Starting job start-syn at 1509438157642\n31-10-2017 16:22:37 CST start-syn INFO - azkaban.webserver.url property was not set\n31-10-2017 16:22:37 CST start-syn INFO - job JVM args: -Dazkaban.flowid=start-syn -Dazkaban.execid=1652 -Dazkaban.jobid=start-syn\n31-10-2017 16:22:37 CST start-syn INFO - Building command job executor. \n31-10-2017 16:22:37 CST start-syn INFO - Memory granted for job start-syn\n31-10-2017 16:22:37 CST start-syn INFO - 1 commands to execute.\n31-10-2017 16:22:37 CST start-syn INFO - cwd=/home/azkaban/azkaban_bk/azkaban-exec-server-3.35.0-22-g5517d50/executions/1652\n31-10-2017 16:22:37 CST start-syn INFO - effective user is: azkaban\n31-10-2017 16:22:37 CST start-syn INFO - Command: sh /home/dm/sbin/start-syn.sh\n31-10-2017 16:22:37 CST start-syn INFO - Environment variables: {JOB_OUTPUT_PROP_FILE=/home/azkaban/azkaban_bk/azkaban-exec-server-3.35.0-22-g5517d50/executions/1652/start-syn_output_2097466626573528316_tmp, JOB_PROP_FILE=/home/azkaban/azkaban_bk/azkaban-exec-server-3.35.0-22-g5517d50/executions/1652/start-syn_props_4563183424789162727_tmp, KRB5CCNAME=/tmp/krb5cc__mysql_syn__start-syn__start-syn__1652__azkaban, JOB_NAME=start-syn}\n31-10-2017 16:22:37 CST start-syn INFO - Working directory: /home/azkaban/azkaban_bk/azkaban-exec-server-3.35.0-22-g5517d50/executions/1652\n31-10-2017 16:22:38 CST start-syn INFO - serverLatestID: 1004491 clientLatestID:993775\n31-10-2017 16:22:38 CST start-syn INFO - 2017-10-31 16:22:38 INFO  DBSyn2:53 - No data to insert\n31-10-2017 16:22:39 CST start-syn INFO - serverLatestID: 1004491 clientLatestID:993775\n31-10-2017 16:22:39 CST start-syn INFO - 2017-10-31 16:22:39 INFO  DBSyn2:53 - No data to insert\n31-10-2017 16:22:40 CST start-syn INFO - serverLatestID: 1004491 clientLatestID:993775\n31-10-2017 16:22:40 CST start-syn INFO - 2017-10-31 16:22:40 INFO  DBSyn2:53 - No data to insert\n31-10-2017 16:22:41 CST start-syn INFO - serverLatestID: 1004491 clientLatestID:993775\n31-10-2017 16:22:41 CST start-syn INFO - 2017-10-31 16:22:41 INFO  DBSyn2:53 - No data to insert\n31-10-2017 16:22:42 CST start-syn INFO - serverLatestID: 1004491 clientLatestID:993775\n31-10-2017 16:22:42 CST start-syn INFO - 2017-10-31 16:22:42 INFO  DBSyn2:53 - No data to insert\n31-10-2017 16:22:43 CST start-syn INFO - serverLatestID: 1004491 clientLatestID:993775\n31-10-2017 16:22:43 CST start-syn INFO - 2017-10-31 16:22:43 INFO  DBSyn2:53 - No data to insert\n31-10-2017 16:22:44 CST start-syn INFO - serverLatestID: 1004491 clientLatestID:993775\n31-10-2017 16:22:44 CST start-syn INFO - 2017-10-31 16:22:44 INFO  DBSyn2:53 - No data to insert\n31-10-2017 16:22:45 CST start-syn INFO - serverLatestID: 1004491 clientLatestID:993775\n31-10-2017 16:22:45 CST start-syn INFO - 2017-10-31 16:22:45 INFO  DBSyn2:53 - No data to insert\n31-10-2017 16:22:46 CST start-syn INFO - serverLatestID: 1004491 clientLatestID:993775\n31-10-2017 16:22:46 CST start-syn INFO - 2017-10-31 16:22:46 INFO  DBSyn2:53 - No data to insert\n31-10-2017 16:22:47 CST start-syn INFO - serverLatestID: 1004491 clientLatestID:993775\n31-10-2017 16:22:47 CST start-syn INFO - 2017-10-31 16:22:47 INFO  DBSyn2:53 - No data to insert\n31-10-2017 16:22:48 CST start-syn INFO - serverLatestID: 1004491 clientLatestID:993775\n31-10-2017 16:22:48 CST start-syn INFO - 2017-10-31 16:22:48 INFO  DBSyn2:53 - No data to insert\n31-10-2017 16:22:49 CST start-syn INFO - serverLatestID: 1004491 clientLatestID:993775\n31-10-2017 16:22:49 CST start-syn INFO - 2017-10-31 16:22:49 INFO  DBSyn2:53 - No data to insert\n31-10-2017 16:22:50 CST start-syn INFO - serverLatestID: 1004491 clientLatestID:993775\n31-10-2017 16:22:50 CST start-syn INFO - 2017-10-31 16:22:50 INFO  DBSyn2:53 - No data to insert\n31-10-2017 16:22:51 CST start-syn INFO - serverLatestID: 1004491 clientLatestID:993775\n31-10-2017 16:22:51 CST start-syn INFO - 2017-10-31 16:22:51 INFO  DBSyn2:53 - No data to insert\n31-10-2017 16:22:52 CST start-syn INFO - serverLatestID: 1004491 clientLatestID:993775\n31-10-2017 16:22:52 CST start-syn INFO - 2017-10-31 16:22:52 INFO  DBSyn2:53 - No data to insert\n31-10-2017 16:22:53 CST start-syn INFO - serverLatestID: 1004491 clientLatestID:993775\n31-10-2017 16:22:53 CST start-syn INFO - 2017-10-31 16:22:53 INFO  DBSyn2:53 - No data to insert\n31-10-2017 16:22:54 CST start-syn INFO - serverLatestID: 1004491 clientLatestID:993775\n31-10-2017 16:22:54 CST start-syn INFO - 2017-10-31 16:22:54 INFO  DBSyn2:53 - No data to insert\n31-10-2017 16:22:55 CST start-syn INFO - serverLatestID: 1004491 clientLatestID:993775\n31-10-2017 16:22:55 CST start-syn INFO - 2017-10-31 16:22:55 INFO  DBSyn2:53 - No data to insert\n31-10-2017 16:22:56 CST start-syn INFO - serverLatestID: 1004491 clientLatestID:993775\n31-10-2017 16:22:56 CST start-syn INFO - 2017-10-31 16:22:56 INFO  DBSyn2:53 - No data to insert\n31-10-2017 16:22:57 CST start-syn INFO - serverLatestID: 1004491 clientLatestID:993775\n31-10-2017 16:22:57 CST start-syn INFO - 2017-10-31 16:22:57 INFO  DBSyn2:53 - No data to insert\n31-10-2017 16:22:58 CST start-syn INFO - serverLatestID: 1004491 clientLatestID:993775\n31-10-2017 16:22:58 CST start-syn INFO - 2017-10-31 16:22:58 INFO  DBSyn2:53 - No data to insert\n31-10-2017 16:22:59 CST start-syn INFO - serverLatestID: 1004491 clientLatestID:993775\n31-10-2017 16:22:59 CST start-syn INFO - 2017-10-31 16:22:59 INFO  DBSyn2:53 - No data to insert\n31-10-2017 16:23:00 CST start-syn INFO - serverLatestID: 1004491 clientLatestID:993775\n31-10-2017 16:23:00 CST start-syn INFO - 2017-10-31 16:23:00 INFO  DBSyn2:53 - No data to insert\n31-10-2017 16:23:01 CST start-syn INFO - serverLatestID: 1004491 clientLatestID:993775\n31-10-2017 16:23:01 CST start-syn INFO - 2017-10-31 16:23:01 INFO  DBSyn2:53 - No data to insert\n31-10-2017 16:23:02 CST start-syn INFO - serverLatestID: 1004491 clientLatestID:993775\n31-10-2017 16:23:02 CST start-syn INFO - 2017-10-31 16:23:02 INFO  DBSyn2:53 - No data to insert\n31-10-2017 16:23:03 CST start-syn INFO - serverLatestID: 1004491 clientLatestID:993775\n31-10-2017 16:23:03 CST start-syn INFO - 2017-10-31 16:23:03 INFO  DBSyn2:53 - No data to insert\n31-10-2017 16:23:04 CST start-syn INFO - serverLatestID: 1004491 clientLatestID:993775\n31-10-2017 16:23:04 CST start-syn INFO - 2017-10-31 16:23:04 INFO  DBSyn2:53 - No data to insert\n31-10-2017 16:23:05 CST start-syn INFO - serverLatestID: 1004491 clientLatestID:993775\n31-10-2017 16:23:05 CST start-syn INFO - 2017-10-31 16:23:05 INFO  DBSyn2:53 - No data to insert\n31-10-2017 16:23:06 CST start-syn INFO - serverLatestID: 1004491 clientLatestID:993775\n31-10-2017 16:23:06 CST start-syn INFO - 2017-10-31 16:23:06 INFO  DBSyn2:53 - No data to insert\n31-10-2017 16:23:07 CST start-syn INFO - serverLatestID: 1004491 clientLatestID:993775\n31-10-2017 16:23:07 CST start-syn INFO - 2017-10-31 16:23:07 INFO  DBSyn2:53 - No data to insert\n31-10-2017 16:23:08 CST start-syn INFO - serverLatestID: 1004491 clientLatestID:993775\n31-10-2017 16:23:08 CST start-syn INFO - 2017-10-31 16:23:08 INFO  DBSyn2:53 - No data to insert\n31-10-2017 16:23:08 CST start-syn ERROR - Kill has been called.\n31-10-2017 16:23:09 CST start-syn INFO - serverLatestID: 1004491 clientLatestID:993775\n31-10-2017 16:23:09 CST start-syn INFO - Process completed unsuccessfully in 31 seconds.\n31-10-2017 16:23:09 CST start-syn ERROR - Job run killed!\njava.lang.RuntimeException: azkaban.jobExecutor.utils.process.ProcessFailureException\n\tat azkaban.jobExecutor.ProcessJob.run(ProcessJob.java:297)\n\tat azkaban.execapp.JobRunner.runJob(JobRunner.java:748)\n\tat azkaban.execapp.JobRunner.doRun(JobRunner.java:591)\n\tat azkaban.execapp.JobRunner.run(JobRunner.java:552)\n\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: azkaban.jobExecutor.utils.process.ProcessFailureException\n\tat azkaban.jobExecutor.utils.process.AzkabanProcess.run(AzkabanProcess.java:130)\n\tat azkaban.jobExecutor.ProcessJob.run(ProcessJob.java:289)\n\t... 8 more\n31-10-2017 16:23:09 CST start-syn ERROR - azkaban.jobExecutor.utils.process.ProcessFailureException cause: azkaban.jobExecutor.utils.process.ProcessFailureException\n31-10-2017 16:23:09 CST start-syn INFO - Finishing job start-syn at 1509438189439 with status KILLED\n', 'length': 8505}
[2017-11-01 10:15:40,148] app.hive_mysql.AzkabanMonitor [INFO] URL: https://172.23.27.203:8443
[2017-11-01 10:15:40,315] app.hive_mysql.AzkabanMonitor [INFO] Success: Login https://172.23.27.203:8443 with user azkaban
[2017-11-01 10:15:40,315] app.hive_mysql.AzkabanMonitor [DEBUG] {'session.id': 'db337f9b-1ffb-4ea2-ac1c-7afe8d8e6185', 'status': 'success'}
[2017-11-01 10:15:40,336] app.hive_mysql.AzkabanMonitor [INFO] Success: Fetch Executions of a Flow - start-syn
[2017-11-01 10:15:40,336] app.hive_mysql.AzkabanMonitor [DEBUG] {'total': 5, 'executions': [{'submitTime': 1509438157461, 'submitUser': 'azkaban', 'startTime': 1509438157639, 'endTime': 1509438189521, 'flowId': 'start-syn', 'projectId': 66, 'execId': 1652, 'status': 'KILLED'}, {'submitTime': 1509438102634, 'submitUser': 'azkaban', 'startTime': 1509438102800, 'endTime': 1509438119797, 'flowId': 'start-syn', 'projectId': 66, 'execId': 1651, 'status': 'KILLED'}, {'submitTime': 1509435165409, 'submitUser': 'azkaban', 'startTime': 1509435165665, 'endTime': 1509435183654, 'flowId': 'start-syn', 'projectId': 66, 'execId': 1649, 'status': 'KILLED'}, {'submitTime': 1509435136471, 'submitUser': 'azkaban', 'startTime': 1509435136636, 'endTime': 1509435145535, 'flowId': 'start-syn', 'projectId': 66, 'execId': 1648, 'status': 'KILLED'}, {'submitTime': 1509435094803, 'submitUser': 'azkaban', 'startTime': 1509435095118, 'endTime': 1509435128253, 'flowId': 'start-syn', 'projectId': 66, 'execId': 1647, 'status': 'KILLED'}], 'length': 10, 'project': 'mysql_syn', 'from': 0, 'projectId': 66, 'flow': 'start-syn'}
[2017-11-01 10:15:40,343] app.hive_mysql.AzkabanMonitor [INFO] Success: Fetch Execution Job Logs - 1652-start-syn
[2017-11-01 10:15:40,344] app.hive_mysql.AzkabanMonitor [DEBUG] {'offset': 0, 'data': '31-10-2017 16:22:37 CST start-syn INFO - Starting job start-syn at 1509438157642\n31-10-2017 16:22:37 CST start-syn INFO - azkaban.webserver.url property was not set\n31-10-2017 16:22:37 CST start-syn INFO - job JVM args: -Dazkaban.flowid=start-syn -Dazkaban.execid=1652 -Dazkaban.jobid=start-syn\n31-10-2017 16:22:37 CST start-syn INFO - Building command job executor. \n31-10-2017 16:22:37 CST start-syn INFO - Memory granted for job start-syn\n31-10-2017 16:22:37 CST start-syn INFO - 1 commands to execute.\n31-10-2017 16:22:37 CST start-syn INFO - cwd=/home/azkaban/azkaban_bk/azkaban-exec-server-3.35.0-22-g5517d50/executions/1652\n31-10-2017 16:22:37 CST start-syn INFO - effective user is: azkaban\n31-10-2017 16:22:37 CST start-syn INFO - Command: sh /home/dm/sbin/start-syn.sh\n31-10-2017 16:22:37 CST start-syn INFO - Environment variables: {JOB_OUTPUT_PROP_FILE=/home/azkaban/azkaban_bk/azkaban-exec-server-3.35.0-22-g5517d50/executions/1652/start-syn_output_2097466626573528316_tmp, JOB_PROP_FILE=/home/azkaban/azkaban_bk/azkaban-exec-server-3.35.0-22-g5517d50/executions/1652/start-syn_props_4563183424789162727_tmp, KRB5CCNAME=/tmp/krb5cc__mysql_syn__start-syn__start-syn__1652__azkaban, JOB_NAME=start-syn}\n31-10-2017 16:22:37 CST start-syn INFO - Working directory: /home/azkaban/azkaban_bk/azkaban-exec-server-3.35.0-22-g5517d50/executions/1652\n31-10-2017 16:22:38 CST start-syn INFO - serverLatestID: 1004491 clientLatestID:993775\n31-10-2017 16:22:38 CST start-syn INFO - 2017-10-31 16:22:38 INFO  DBSyn2:53 - No data to insert\n31-10-2017 16:22:39 CST start-syn INFO - serverLatestID: 1004491 clientLatestID:993775\n31-10-2017 16:22:39 CST start-syn INFO - 2017-10-31 16:22:39 INFO  DBSyn2:53 - No data to insert\n31-10-2017 16:22:40 CST start-syn INFO - serverLatestID: 1004491 clientLatestID:993775\n31-10-2017 16:22:40 CST start-syn INFO - 2017-10-31 16:22:40 INFO  DBSyn2:53 - No data to insert\n31-10-2017 16:22:41 CST start-syn INFO - serverLatestID: 1004491 clientLatestID:993775\n31-10-2017 16:22:41 CST start-syn INFO - 2017-10-31 16:22:41 INFO  DBSyn2:53 - No data to insert\n31-10-2017 16:22:42 CST start-syn INFO - serverLatestID: 1004491 clientLatestID:993775\n31-10-2017 16:22:42 CST start-syn INFO - 2017-10-31 16:22:42 INFO  DBSyn2:53 - No data to insert\n31-10-2017 16:22:43 CST start-syn INFO - serverLatestID: 1004491 clientLatestID:993775\n31-10-2017 16:22:43 CST start-syn INFO - 2017-10-31 16:22:43 INFO  DBSyn2:53 - No data to insert\n31-10-2017 16:22:44 CST start-syn INFO - serverLatestID: 1004491 clientLatestID:993775\n31-10-2017 16:22:44 CST start-syn INFO - 2017-10-31 16:22:44 INFO  DBSyn2:53 - No data to insert\n31-10-2017 16:22:45 CST start-syn INFO - serverLatestID: 1004491 clientLatestID:993775\n31-10-2017 16:22:45 CST start-syn INFO - 2017-10-31 16:22:45 INFO  DBSyn2:53 - No data to insert\n31-10-2017 16:22:46 CST start-syn INFO - serverLatestID: 1004491 clientLatestID:993775\n31-10-2017 16:22:46 CST start-syn INFO - 2017-10-31 16:22:46 INFO  DBSyn2:53 - No data to insert\n31-10-2017 16:22:47 CST start-syn INFO - serverLatestID: 1004491 clientLatestID:993775\n31-10-2017 16:22:47 CST start-syn INFO - 2017-10-31 16:22:47 INFO  DBSyn2:53 - No data to insert\n31-10-2017 16:22:48 CST start-syn INFO - serverLatestID: 1004491 clientLatestID:993775\n31-10-2017 16:22:48 CST start-syn INFO - 2017-10-31 16:22:48 INFO  DBSyn2:53 - No data to insert\n31-10-2017 16:22:49 CST start-syn INFO - serverLatestID: 1004491 clientLatestID:993775\n31-10-2017 16:22:49 CST start-syn INFO - 2017-10-31 16:22:49 INFO  DBSyn2:53 - No data to insert\n31-10-2017 16:22:50 CST start-syn INFO - serverLatestID: 1004491 clientLatestID:993775\n31-10-2017 16:22:50 CST start-syn INFO - 2017-10-31 16:22:50 INFO  DBSyn2:53 - No data to insert\n31-10-2017 16:22:51 CST start-syn INFO - serverLatestID: 1004491 clientLatestID:993775\n31-10-2017 16:22:51 CST start-syn INFO - 2017-10-31 16:22:51 INFO  DBSyn2:53 - No data to insert\n31-10-2017 16:22:52 CST start-syn INFO - serverLatestID: 1004491 clientLatestID:993775\n31-10-2017 16:22:52 CST start-syn INFO - 2017-10-31 16:22:52 INFO  DBSyn2:53 - No data to insert\n31-10-2017 16:22:53 CST start-syn INFO - serverLatestID: 1004491 clientLatestID:993775\n31-10-2017 16:22:53 CST start-syn INFO - 2017-10-31 16:22:53 INFO  DBSyn2:53 - No data to insert\n31-10-2017 16:22:54 CST start-syn INFO - serverLatestID: 1004491 clientLatestID:993775\n31-10-2017 16:22:54 CST start-syn INFO - 2017-10-31 16:22:54 INFO  DBSyn2:53 - No data to insert\n31-10-2017 16:22:55 CST start-syn INFO - serverLatestID: 1004491 clientLatestID:993775\n31-10-2017 16:22:55 CST start-syn INFO - 2017-10-31 16:22:55 INFO  DBSyn2:53 - No data to insert\n31-10-2017 16:22:56 CST start-syn INFO - serverLatestID: 1004491 clientLatestID:993775\n31-10-2017 16:22:56 CST start-syn INFO - 2017-10-31 16:22:56 INFO  DBSyn2:53 - No data to insert\n31-10-2017 16:22:57 CST start-syn INFO - serverLatestID: 1004491 clientLatestID:993775\n31-10-2017 16:22:57 CST start-syn INFO - 2017-10-31 16:22:57 INFO  DBSyn2:53 - No data to insert\n31-10-2017 16:22:58 CST start-syn INFO - serverLatestID: 1004491 clientLatestID:993775\n31-10-2017 16:22:58 CST start-syn INFO - 2017-10-31 16:22:58 INFO  DBSyn2:53 - No data to insert\n31-10-2017 16:22:59 CST start-syn INFO - serverLatestID: 1004491 clientLatestID:993775\n31-10-2017 16:22:59 CST start-syn INFO - 2017-10-31 16:22:59 INFO  DBSyn2:53 - No data to insert\n31-10-2017 16:23:00 CST start-syn INFO - serverLatestID: 1004491 clientLatestID:993775\n31-10-2017 16:23:00 CST start-syn INFO - 2017-10-31 16:23:00 INFO  DBSyn2:53 - No data to insert\n31-10-2017 16:23:01 CST start-syn INFO - serverLatestID: 1004491 clientLatestID:993775\n31-10-2017 16:23:01 CST start-syn INFO - 2017-10-31 16:23:01 INFO  DBSyn2:53 - No data to insert\n31-10-2017 16:23:02 CST start-syn INFO - serverLatestID: 1004491 clientLatestID:993775\n31-10-2017 16:23:02 CST start-syn INFO - 2017-10-31 16:23:02 INFO  DBSyn2:53 - No data to insert\n31-10-2017 16:23:03 CST start-syn INFO - serverLatestID: 1004491 clientLatestID:993775\n31-10-2017 16:23:03 CST start-syn INFO - 2017-10-31 16:23:03 INFO  DBSyn2:53 - No data to insert\n31-10-2017 16:23:04 CST start-syn INFO - serverLatestID: 1004491 clientLatestID:993775\n31-10-2017 16:23:04 CST start-syn INFO - 2017-10-31 16:23:04 INFO  DBSyn2:53 - No data to insert\n31-10-2017 16:23:05 CST start-syn INFO - serverLatestID: 1004491 clientLatestID:993775\n31-10-2017 16:23:05 CST start-syn INFO - 2017-10-31 16:23:05 INFO  DBSyn2:53 - No data to insert\n31-10-2017 16:23:06 CST start-syn INFO - serverLatestID: 1004491 clientLatestID:993775\n31-10-2017 16:23:06 CST start-syn INFO - 2017-10-31 16:23:06 INFO  DBSyn2:53 - No data to insert\n31-10-2017 16:23:07 CST start-syn INFO - serverLatestID: 1004491 clientLatestID:993775\n31-10-2017 16:23:07 CST start-syn INFO - 2017-10-31 16:23:07 INFO  DBSyn2:53 - No data to insert\n31-10-2017 16:23:08 CST start-syn INFO - serverLatestID: 1004491 clientLatestID:993775\n31-10-2017 16:23:08 CST start-syn INFO - 2017-10-31 16:23:08 INFO  DBSyn2:53 - No data to insert\n31-10-2017 16:23:08 CST start-syn ERROR - Kill has been called.\n31-10-2017 16:23:09 CST start-syn INFO - serverLatestID: 1004491 clientLatestID:993775\n31-10-2017 16:23:09 CST start-syn INFO - Process completed unsuccessfully in 31 seconds.\n31-10-2017 16:23:09 CST start-syn ERROR - Job run killed!\njava.lang.RuntimeException: azkaban.jobExecutor.utils.process.ProcessFailureException\n\tat azkaban.jobExecutor.ProcessJob.run(ProcessJob.java:297)\n\tat azkaban.execapp.JobRunner.runJob(JobRunner.java:748)\n\tat azkaban.execapp.JobRunner.doRun(JobRunner.java:591)\n\tat azkaban.execapp.JobRunner.run(JobRunner.java:552)\n\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: azkaban.jobExecutor.utils.process.ProcessFailureException\n\tat azkaban.jobExecutor.utils.process.AzkabanProcess.run(AzkabanProcess.java:130)\n\tat azkaban.jobExecutor.ProcessJob.run(ProcessJob.java:289)\n\t... 8 more\n31-10-2017 16:23:09 CST start-syn ERROR - azkaban.jobExecutor.utils.process.ProcessFailureException cause: azkaban.jobExecutor.utils.process.ProcessFailureException\n31-10-2017 16:23:09 CST start-syn INFO - Finishing job start-syn at 1509438189439 with status KILLED\n', 'length': 8505}
[2017-11-01 10:27:49,710] app.hive_mysql.AzkabanMonitor [INFO] URL: https://172.23.27.203:8443
[2017-11-01 10:27:49,883] app.hive_mysql.AzkabanMonitor [INFO] Success: Login https://172.23.27.203:8443 with user azkaban
[2017-11-01 10:27:49,883] app.hive_mysql.AzkabanMonitor [DEBUG] {'session.id': 'f40f1730-ac3b-427f-9f96-395e1894b5f7', 'status': 'success'}
[2017-11-01 10:27:49,894] app.hive_mysql.AzkabanMonitor [INFO] Success: Fetch Executions of a Flow - start-syn
[2017-11-01 10:27:49,894] app.hive_mysql.AzkabanMonitor [DEBUG] {'total': 5, 'executions': [{'submitTime': 1509438157461, 'submitUser': 'azkaban', 'startTime': 1509438157639, 'endTime': 1509438189521, 'flowId': 'start-syn', 'projectId': 66, 'execId': 1652, 'status': 'KILLED'}, {'submitTime': 1509438102634, 'submitUser': 'azkaban', 'startTime': 1509438102800, 'endTime': 1509438119797, 'flowId': 'start-syn', 'projectId': 66, 'execId': 1651, 'status': 'KILLED'}, {'submitTime': 1509435165409, 'submitUser': 'azkaban', 'startTime': 1509435165665, 'endTime': 1509435183654, 'flowId': 'start-syn', 'projectId': 66, 'execId': 1649, 'status': 'KILLED'}, {'submitTime': 1509435136471, 'submitUser': 'azkaban', 'startTime': 1509435136636, 'endTime': 1509435145535, 'flowId': 'start-syn', 'projectId': 66, 'execId': 1648, 'status': 'KILLED'}, {'submitTime': 1509435094803, 'submitUser': 'azkaban', 'startTime': 1509435095118, 'endTime': 1509435128253, 'flowId': 'start-syn', 'projectId': 66, 'execId': 1647, 'status': 'KILLED'}], 'length': 10, 'project': 'mysql_syn', 'from': 0, 'projectId': 66, 'flow': 'start-syn'}
[2017-11-01 10:27:49,902] app.hive_mysql.AzkabanMonitor [INFO] Success: Fetch Execution Job Logs - 1652-start-syn
[2017-11-01 10:27:49,902] app.hive_mysql.AzkabanMonitor [DEBUG] {'offset': 0, 'data': '31-10-2017 16:22:37 CST start-syn INFO - Starting job start-syn at 1509438157642\n31-10-2017 16:22:37 CST start-syn INFO - azkaban.webserver.url property was not set\n31-10-2017 16:22:37 CST start-syn INFO - job JVM args: -Dazkaban.flowid=start-syn -Dazkaban.execid=1652 -Dazkaban.jobid=start-syn\n31-10-2017 16:22:37 CST start-syn INFO - Building command job executor. \n31-10-2017 16:22:37 CST start-syn INFO - Memory granted for job start-syn\n31-10-2017 16:22:37 CST start-syn INFO - 1 commands to execute.\n31-10-2017 16:22:37 CST start-syn INFO - cwd=/home/azkaban/azkaban_bk/azkaban-exec-server-3.35.0-22-g5517d50/executions/1652\n31-10-2017 16:22:37 CST start-syn INFO - effective user is: azkaban\n31-10-2017 16:22:37 CST start-syn INFO - Command: sh /home/dm/sbin/start-syn.sh\n31-10-2017 16:22:37 CST start-syn INFO - Environment variables: {JOB_OUTPUT_PROP_FILE=/home/azkaban/azkaban_bk/azkaban-exec-server-3.35.0-22-g5517d50/executions/1652/start-syn_output_2097466626573528316_tmp, JOB_PROP_FILE=/home/azkaban/azkaban_bk/azkaban-exec-server-3.35.0-22-g5517d50/executions/1652/start-syn_props_4563183424789162727_tmp, KRB5CCNAME=/tmp/krb5cc__mysql_syn__start-syn__start-syn__1652__azkaban, JOB_NAME=start-syn}\n31-10-2017 16:22:37 CST start-syn INFO - Working directory: /home/azkaban/azkaban_bk/azkaban-exec-server-3.35.0-22-g5517d50/executions/1652\n31-10-2017 16:22:38 CST start-syn INFO - serverLatestID: 1004491 clientLatestID:993775\n31-10-2017 16:22:38 CST start-syn INFO - 2017-10-31 16:22:38 INFO  DBSyn2:53 - No data to insert\n31-10-2017 16:22:39 CST start-syn INFO - serverLatestID: 1004491 clientLatestID:993775\n31-10-2017 16:22:39 CST start-syn INFO - 2017-10-31 16:22:39 INFO  DBSyn2:53 - No data to insert\n31-10-2017 16:22:40 CST start-syn INFO - serverLatestID: 1004491 clientLatestID:993775\n31-10-2017 16:22:40 CST start-syn INFO - 2017-10-31 16:22:40 INFO  DBSyn2:53 - No data to insert\n31-10-2017 16:22:41 CST start-syn INFO - serverLatestID: 1004491 clientLatestID:993775\n31-10-2017 16:22:41 CST start-syn INFO - 2017-10-31 16:22:41 INFO  DBSyn2:53 - No data to insert\n31-10-2017 16:22:42 CST start-syn INFO - serverLatestID: 1004491 clientLatestID:993775\n31-10-2017 16:22:42 CST start-syn INFO - 2017-10-31 16:22:42 INFO  DBSyn2:53 - No data to insert\n31-10-2017 16:22:43 CST start-syn INFO - serverLatestID: 1004491 clientLatestID:993775\n31-10-2017 16:22:43 CST start-syn INFO - 2017-10-31 16:22:43 INFO  DBSyn2:53 - No data to insert\n31-10-2017 16:22:44 CST start-syn INFO - serverLatestID: 1004491 clientLatestID:993775\n31-10-2017 16:22:44 CST start-syn INFO - 2017-10-31 16:22:44 INFO  DBSyn2:53 - No data to insert\n31-10-2017 16:22:45 CST start-syn INFO - serverLatestID: 1004491 clientLatestID:993775\n31-10-2017 16:22:45 CST start-syn INFO - 2017-10-31 16:22:45 INFO  DBSyn2:53 - No data to insert\n31-10-2017 16:22:46 CST start-syn INFO - serverLatestID: 1004491 clientLatestID:993775\n31-10-2017 16:22:46 CST start-syn INFO - 2017-10-31 16:22:46 INFO  DBSyn2:53 - No data to insert\n31-10-2017 16:22:47 CST start-syn INFO - serverLatestID: 1004491 clientLatestID:993775\n31-10-2017 16:22:47 CST start-syn INFO - 2017-10-31 16:22:47 INFO  DBSyn2:53 - No data to insert\n31-10-2017 16:22:48 CST start-syn INFO - serverLatestID: 1004491 clientLatestID:993775\n31-10-2017 16:22:48 CST start-syn INFO - 2017-10-31 16:22:48 INFO  DBSyn2:53 - No data to insert\n31-10-2017 16:22:49 CST start-syn INFO - serverLatestID: 1004491 clientLatestID:993775\n31-10-2017 16:22:49 CST start-syn INFO - 2017-10-31 16:22:49 INFO  DBSyn2:53 - No data to insert\n31-10-2017 16:22:50 CST start-syn INFO - serverLatestID: 1004491 clientLatestID:993775\n31-10-2017 16:22:50 CST start-syn INFO - 2017-10-31 16:22:50 INFO  DBSyn2:53 - No data to insert\n31-10-2017 16:22:51 CST start-syn INFO - serverLatestID: 1004491 clientLatestID:993775\n31-10-2017 16:22:51 CST start-syn INFO - 2017-10-31 16:22:51 INFO  DBSyn2:53 - No data to insert\n31-10-2017 16:22:52 CST start-syn INFO - serverLatestID: 1004491 clientLatestID:993775\n31-10-2017 16:22:52 CST start-syn INFO - 2017-10-31 16:22:52 INFO  DBSyn2:53 - No data to insert\n31-10-2017 16:22:53 CST start-syn INFO - serverLatestID: 1004491 clientLatestID:993775\n31-10-2017 16:22:53 CST start-syn INFO - 2017-10-31 16:22:53 INFO  DBSyn2:53 - No data to insert\n31-10-2017 16:22:54 CST start-syn INFO - serverLatestID: 1004491 clientLatestID:993775\n31-10-2017 16:22:54 CST start-syn INFO - 2017-10-31 16:22:54 INFO  DBSyn2:53 - No data to insert\n31-10-2017 16:22:55 CST start-syn INFO - serverLatestID: 1004491 clientLatestID:993775\n31-10-2017 16:22:55 CST start-syn INFO - 2017-10-31 16:22:55 INFO  DBSyn2:53 - No data to insert\n31-10-2017 16:22:56 CST start-syn INFO - serverLatestID: 1004491 clientLatestID:993775\n31-10-2017 16:22:56 CST start-syn INFO - 2017-10-31 16:22:56 INFO  DBSyn2:53 - No data to insert\n31-10-2017 16:22:57 CST start-syn INFO - serverLatestID: 1004491 clientLatestID:993775\n31-10-2017 16:22:57 CST start-syn INFO - 2017-10-31 16:22:57 INFO  DBSyn2:53 - No data to insert\n31-10-2017 16:22:58 CST start-syn INFO - serverLatestID: 1004491 clientLatestID:993775\n31-10-2017 16:22:58 CST start-syn INFO - 2017-10-31 16:22:58 INFO  DBSyn2:53 - No data to insert\n31-10-2017 16:22:59 CST start-syn INFO - serverLatestID: 1004491 clientLatestID:993775\n31-10-2017 16:22:59 CST start-syn INFO - 2017-10-31 16:22:59 INFO  DBSyn2:53 - No data to insert\n31-10-2017 16:23:00 CST start-syn INFO - serverLatestID: 1004491 clientLatestID:993775\n31-10-2017 16:23:00 CST start-syn INFO - 2017-10-31 16:23:00 INFO  DBSyn2:53 - No data to insert\n31-10-2017 16:23:01 CST start-syn INFO - serverLatestID: 1004491 clientLatestID:993775\n31-10-2017 16:23:01 CST start-syn INFO - 2017-10-31 16:23:01 INFO  DBSyn2:53 - No data to insert\n31-10-2017 16:23:02 CST start-syn INFO - serverLatestID: 1004491 clientLatestID:993775\n31-10-2017 16:23:02 CST start-syn INFO - 2017-10-31 16:23:02 INFO  DBSyn2:53 - No data to insert\n31-10-2017 16:23:03 CST start-syn INFO - serverLatestID: 1004491 clientLatestID:993775\n31-10-2017 16:23:03 CST start-syn INFO - 2017-10-31 16:23:03 INFO  DBSyn2:53 - No data to insert\n31-10-2017 16:23:04 CST start-syn INFO - serverLatestID: 1004491 clientLatestID:993775\n31-10-2017 16:23:04 CST start-syn INFO - 2017-10-31 16:23:04 INFO  DBSyn2:53 - No data to insert\n31-10-2017 16:23:05 CST start-syn INFO - serverLatestID: 1004491 clientLatestID:993775\n31-10-2017 16:23:05 CST start-syn INFO - 2017-10-31 16:23:05 INFO  DBSyn2:53 - No data to insert\n31-10-2017 16:23:06 CST start-syn INFO - serverLatestID: 1004491 clientLatestID:993775\n31-10-2017 16:23:06 CST start-syn INFO - 2017-10-31 16:23:06 INFO  DBSyn2:53 - No data to insert\n31-10-2017 16:23:07 CST start-syn INFO - serverLatestID: 1004491 clientLatestID:993775\n31-10-2017 16:23:07 CST start-syn INFO - 2017-10-31 16:23:07 INFO  DBSyn2:53 - No data to insert\n31-10-2017 16:23:08 CST start-syn INFO - serverLatestID: 1004491 clientLatestID:993775\n31-10-2017 16:23:08 CST start-syn INFO - 2017-10-31 16:23:08 INFO  DBSyn2:53 - No data to insert\n31-10-2017 16:23:08 CST start-syn ERROR - Kill has been called.\n31-10-2017 16:23:09 CST start-syn INFO - serverLatestID: 1004491 clientLatestID:993775\n31-10-2017 16:23:09 CST start-syn INFO - Process completed unsuccessfully in 31 seconds.\n31-10-2017 16:23:09 CST start-syn ERROR - Job run killed!\njava.lang.RuntimeException: azkaban.jobExecutor.utils.process.ProcessFailureException\n\tat azkaban.jobExecutor.ProcessJob.run(ProcessJob.java:297)\n\tat azkaban.execapp.JobRunner.runJob(JobRunner.java:748)\n\tat azkaban.execapp.JobRunner.doRun(JobRunner.java:591)\n\tat azkaban.execapp.JobRunner.run(JobRunner.java:552)\n\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: azkaban.jobExecutor.utils.process.ProcessFailureException\n\tat azkaban.jobExecutor.utils.process.AzkabanProcess.run(AzkabanProcess.java:130)\n\tat azkaban.jobExecutor.ProcessJob.run(ProcessJob.java:289)\n\t... 8 more\n31-10-2017 16:23:09 CST start-syn ERROR - azkaban.jobExecutor.utils.process.ProcessFailureException cause: azkaban.jobExecutor.utils.process.ProcessFailureException\n31-10-2017 16:23:09 CST start-syn INFO - Finishing job start-syn at 1509438189439 with status KILLED\n', 'length': 8505}
[2017-11-01 10:27:56,578] app.hive_mysql.AzkabanMonitor [INFO] URL: https://172.23.27.203:8443
[2017-11-01 10:27:56,762] app.hive_mysql.AzkabanMonitor [INFO] Success: Login https://172.23.27.203:8443 with user azkaban
[2017-11-01 10:27:56,763] app.hive_mysql.AzkabanMonitor [DEBUG] {'session.id': '86b0752b-5be3-4f82-9e6f-d2bc42ae4f43', 'status': 'success'}
[2017-11-01 10:27:56,772] app.hive_mysql.AzkabanMonitor [INFO] Success: Fetch Executions of a Flow - start-syn
[2017-11-01 10:27:56,773] app.hive_mysql.AzkabanMonitor [DEBUG] {'total': 5, 'executions': [{'submitTime': 1509438157461, 'submitUser': 'azkaban', 'startTime': 1509438157639, 'endTime': 1509438189521, 'flowId': 'start-syn', 'projectId': 66, 'execId': 1652, 'status': 'KILLED'}, {'submitTime': 1509438102634, 'submitUser': 'azkaban', 'startTime': 1509438102800, 'endTime': 1509438119797, 'flowId': 'start-syn', 'projectId': 66, 'execId': 1651, 'status': 'KILLED'}, {'submitTime': 1509435165409, 'submitUser': 'azkaban', 'startTime': 1509435165665, 'endTime': 1509435183654, 'flowId': 'start-syn', 'projectId': 66, 'execId': 1649, 'status': 'KILLED'}, {'submitTime': 1509435136471, 'submitUser': 'azkaban', 'startTime': 1509435136636, 'endTime': 1509435145535, 'flowId': 'start-syn', 'projectId': 66, 'execId': 1648, 'status': 'KILLED'}, {'submitTime': 1509435094803, 'submitUser': 'azkaban', 'startTime': 1509435095118, 'endTime': 1509435128253, 'flowId': 'start-syn', 'projectId': 66, 'execId': 1647, 'status': 'KILLED'}], 'length': 10, 'project': 'mysql_syn', 'from': 0, 'projectId': 66, 'flow': 'start-syn'}
[2017-11-01 10:27:56,779] app.hive_mysql.AzkabanMonitor [INFO] Success: Fetch Execution Job Logs - 1652-start-syn
[2017-11-01 10:27:56,779] app.hive_mysql.AzkabanMonitor [DEBUG] {'offset': 0, 'data': '31-10-2017 16:22:37 CST start-syn INFO - Starting job start-syn at 1509438157642\n31-10-2017 16:22:37 CST start-syn INFO - azkaban.webserver.url property was not set\n31-10-2017 16:22:37 CST start-syn INFO - job JVM args: -Dazkaban.flowid=start-syn -Dazkaban.execid=1652 -Dazkaban.jobid=start-syn\n31-10-2017 16:22:37 CST start-syn INFO - Building command job executor. \n31-10-2017 16:22:37 CST start-syn INFO - Memory granted for job start-syn\n31-10-2017 16:22:37 CST start-syn INFO - 1 commands to execute.\n31-10-2017 16:22:37 CST start-syn INFO - cwd=/home/azkaban/azkaban_bk/azkaban-exec-server-3.35.0-22-g5517d50/executions/1652\n31-10-2017 16:22:37 CST start-syn INFO - effective user is: azkaban\n31-10-2017 16:22:37 CST start-syn INFO - Command: sh /home/dm/sbin/start-syn.sh\n31-10-2017 16:22:37 CST start-syn INFO - Environment variables: {JOB_OUTPUT_PROP_FILE=/home/azkaban/azkaban_bk/azkaban-exec-server-3.35.0-22-g5517d50/executions/1652/start-syn_output_2097466626573528316_tmp, JOB_PROP_FILE=/home/azkaban/azkaban_bk/azkaban-exec-server-3.35.0-22-g5517d50/executions/1652/start-syn_props_4563183424789162727_tmp, KRB5CCNAME=/tmp/krb5cc__mysql_syn__start-syn__start-syn__1652__azkaban, JOB_NAME=start-syn}\n31-10-2017 16:22:37 CST start-syn INFO - Working directory: /home/azkaban/azkaban_bk/azkaban-exec-server-3.35.0-22-g5517d50/executions/1652\n31-10-2017 16:22:38 CST start-syn INFO - serverLatestID: 1004491 clientLatestID:993775\n31-10-2017 16:22:38 CST start-syn INFO - 2017-10-31 16:22:38 INFO  DBSyn2:53 - No data to insert\n31-10-2017 16:22:39 CST start-syn INFO - serverLatestID: 1004491 clientLatestID:993775\n31-10-2017 16:22:39 CST start-syn INFO - 2017-10-31 16:22:39 INFO  DBSyn2:53 - No data to insert\n31-10-2017 16:22:40 CST start-syn INFO - serverLatestID: 1004491 clientLatestID:993775\n31-10-2017 16:22:40 CST start-syn INFO - 2017-10-31 16:22:40 INFO  DBSyn2:53 - No data to insert\n31-10-2017 16:22:41 CST start-syn INFO - serverLatestID: 1004491 clientLatestID:993775\n31-10-2017 16:22:41 CST start-syn INFO - 2017-10-31 16:22:41 INFO  DBSyn2:53 - No data to insert\n31-10-2017 16:22:42 CST start-syn INFO - serverLatestID: 1004491 clientLatestID:993775\n31-10-2017 16:22:42 CST start-syn INFO - 2017-10-31 16:22:42 INFO  DBSyn2:53 - No data to insert\n31-10-2017 16:22:43 CST start-syn INFO - serverLatestID: 1004491 clientLatestID:993775\n31-10-2017 16:22:43 CST start-syn INFO - 2017-10-31 16:22:43 INFO  DBSyn2:53 - No data to insert\n31-10-2017 16:22:44 CST start-syn INFO - serverLatestID: 1004491 clientLatestID:993775\n31-10-2017 16:22:44 CST start-syn INFO - 2017-10-31 16:22:44 INFO  DBSyn2:53 - No data to insert\n31-10-2017 16:22:45 CST start-syn INFO - serverLatestID: 1004491 clientLatestID:993775\n31-10-2017 16:22:45 CST start-syn INFO - 2017-10-31 16:22:45 INFO  DBSyn2:53 - No data to insert\n31-10-2017 16:22:46 CST start-syn INFO - serverLatestID: 1004491 clientLatestID:993775\n31-10-2017 16:22:46 CST start-syn INFO - 2017-10-31 16:22:46 INFO  DBSyn2:53 - No data to insert\n31-10-2017 16:22:47 CST start-syn INFO - serverLatestID: 1004491 clientLatestID:993775\n31-10-2017 16:22:47 CST start-syn INFO - 2017-10-31 16:22:47 INFO  DBSyn2:53 - No data to insert\n31-10-2017 16:22:48 CST start-syn INFO - serverLatestID: 1004491 clientLatestID:993775\n31-10-2017 16:22:48 CST start-syn INFO - 2017-10-31 16:22:48 INFO  DBSyn2:53 - No data to insert\n31-10-2017 16:22:49 CST start-syn INFO - serverLatestID: 1004491 clientLatestID:993775\n31-10-2017 16:22:49 CST start-syn INFO - 2017-10-31 16:22:49 INFO  DBSyn2:53 - No data to insert\n31-10-2017 16:22:50 CST start-syn INFO - serverLatestID: 1004491 clientLatestID:993775\n31-10-2017 16:22:50 CST start-syn INFO - 2017-10-31 16:22:50 INFO  DBSyn2:53 - No data to insert\n31-10-2017 16:22:51 CST start-syn INFO - serverLatestID: 1004491 clientLatestID:993775\n31-10-2017 16:22:51 CST start-syn INFO - 2017-10-31 16:22:51 INFO  DBSyn2:53 - No data to insert\n31-10-2017 16:22:52 CST start-syn INFO - serverLatestID: 1004491 clientLatestID:993775\n31-10-2017 16:22:52 CST start-syn INFO - 2017-10-31 16:22:52 INFO  DBSyn2:53 - No data to insert\n31-10-2017 16:22:53 CST start-syn INFO - serverLatestID: 1004491 clientLatestID:993775\n31-10-2017 16:22:53 CST start-syn INFO - 2017-10-31 16:22:53 INFO  DBSyn2:53 - No data to insert\n31-10-2017 16:22:54 CST start-syn INFO - serverLatestID: 1004491 clientLatestID:993775\n31-10-2017 16:22:54 CST start-syn INFO - 2017-10-31 16:22:54 INFO  DBSyn2:53 - No data to insert\n31-10-2017 16:22:55 CST start-syn INFO - serverLatestID: 1004491 clientLatestID:993775\n31-10-2017 16:22:55 CST start-syn INFO - 2017-10-31 16:22:55 INFO  DBSyn2:53 - No data to insert\n31-10-2017 16:22:56 CST start-syn INFO - serverLatestID: 1004491 clientLatestID:993775\n31-10-2017 16:22:56 CST start-syn INFO - 2017-10-31 16:22:56 INFO  DBSyn2:53 - No data to insert\n31-10-2017 16:22:57 CST start-syn INFO - serverLatestID: 1004491 clientLatestID:993775\n31-10-2017 16:22:57 CST start-syn INFO - 2017-10-31 16:22:57 INFO  DBSyn2:53 - No data to insert\n31-10-2017 16:22:58 CST start-syn INFO - serverLatestID: 1004491 clientLatestID:993775\n31-10-2017 16:22:58 CST start-syn INFO - 2017-10-31 16:22:58 INFO  DBSyn2:53 - No data to insert\n31-10-2017 16:22:59 CST start-syn INFO - serverLatestID: 1004491 clientLatestID:993775\n31-10-2017 16:22:59 CST start-syn INFO - 2017-10-31 16:22:59 INFO  DBSyn2:53 - No data to insert\n31-10-2017 16:23:00 CST start-syn INFO - serverLatestID: 1004491 clientLatestID:993775\n31-10-2017 16:23:00 CST start-syn INFO - 2017-10-31 16:23:00 INFO  DBSyn2:53 - No data to insert\n31-10-2017 16:23:01 CST start-syn INFO - serverLatestID: 1004491 clientLatestID:993775\n31-10-2017 16:23:01 CST start-syn INFO - 2017-10-31 16:23:01 INFO  DBSyn2:53 - No data to insert\n31-10-2017 16:23:02 CST start-syn INFO - serverLatestID: 1004491 clientLatestID:993775\n31-10-2017 16:23:02 CST start-syn INFO - 2017-10-31 16:23:02 INFO  DBSyn2:53 - No data to insert\n31-10-2017 16:23:03 CST start-syn INFO - serverLatestID: 1004491 clientLatestID:993775\n31-10-2017 16:23:03 CST start-syn INFO - 2017-10-31 16:23:03 INFO  DBSyn2:53 - No data to insert\n31-10-2017 16:23:04 CST start-syn INFO - serverLatestID: 1004491 clientLatestID:993775\n31-10-2017 16:23:04 CST start-syn INFO - 2017-10-31 16:23:04 INFO  DBSyn2:53 - No data to insert\n31-10-2017 16:23:05 CST start-syn INFO - serverLatestID: 1004491 clientLatestID:993775\n31-10-2017 16:23:05 CST start-syn INFO - 2017-10-31 16:23:05 INFO  DBSyn2:53 - No data to insert\n31-10-2017 16:23:06 CST start-syn INFO - serverLatestID: 1004491 clientLatestID:993775\n31-10-2017 16:23:06 CST start-syn INFO - 2017-10-31 16:23:06 INFO  DBSyn2:53 - No data to insert\n31-10-2017 16:23:07 CST start-syn INFO - serverLatestID: 1004491 clientLatestID:993775\n31-10-2017 16:23:07 CST start-syn INFO - 2017-10-31 16:23:07 INFO  DBSyn2:53 - No data to insert\n31-10-2017 16:23:08 CST start-syn INFO - serverLatestID: 1004491 clientLatestID:993775\n31-10-2017 16:23:08 CST start-syn INFO - 2017-10-31 16:23:08 INFO  DBSyn2:53 - No data to insert\n31-10-2017 16:23:08 CST start-syn ERROR - Kill has been called.\n31-10-2017 16:23:09 CST start-syn INFO - serverLatestID: 1004491 clientLatestID:993775\n31-10-2017 16:23:09 CST start-syn INFO - Process completed unsuccessfully in 31 seconds.\n31-10-2017 16:23:09 CST start-syn ERROR - Job run killed!\njava.lang.RuntimeException: azkaban.jobExecutor.utils.process.ProcessFailureException\n\tat azkaban.jobExecutor.ProcessJob.run(ProcessJob.java:297)\n\tat azkaban.execapp.JobRunner.runJob(JobRunner.java:748)\n\tat azkaban.execapp.JobRunner.doRun(JobRunner.java:591)\n\tat azkaban.execapp.JobRunner.run(JobRunner.java:552)\n\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: azkaban.jobExecutor.utils.process.ProcessFailureException\n\tat azkaban.jobExecutor.utils.process.AzkabanProcess.run(AzkabanProcess.java:130)\n\tat azkaban.jobExecutor.ProcessJob.run(ProcessJob.java:289)\n\t... 8 more\n31-10-2017 16:23:09 CST start-syn ERROR - azkaban.jobExecutor.utils.process.ProcessFailureException cause: azkaban.jobExecutor.utils.process.ProcessFailureException\n31-10-2017 16:23:09 CST start-syn INFO - Finishing job start-syn at 1509438189439 with status KILLED\n', 'length': 8505}
[2017-11-01 10:35:20,072] app.hive_mysql.AzkabanMonitor [INFO] URL: https://172.23.27.203:8443
[2017-11-01 10:35:20,247] app.hive_mysql.AzkabanMonitor [INFO] Success: Login https://172.23.27.203:8443 with user azkaban
[2017-11-01 10:35:20,248] app.hive_mysql.AzkabanMonitor [DEBUG] {'session.id': '49044831-23ca-4d85-9fbe-f28ebee3e3a8', 'status': 'success'}
[2017-11-01 10:35:20,258] app.hive_mysql.AzkabanMonitor [INFO] Success: Fetch Executions of a Flow - run_dataimport
[2017-11-01 10:35:20,258] app.hive_mysql.AzkabanMonitor [DEBUG] {'total': 6, 'executions': [{'submitTime': 1509501650049, 'submitUser': 'azkaban', 'startTime': 1509501650215, 'endTime': -1, 'flowId': 'run_dataimport', 'projectId': 68, 'execId': 1658, 'status': 'RUNNING'}, {'submitTime': 1509494450033, 'submitUser': 'azkaban', 'startTime': 1509494450241, 'endTime': 1509496877667, 'flowId': 'run_dataimport', 'projectId': 68, 'execId': 1657, 'status': 'SUCCEEDED'}, {'submitTime': 1509458449965, 'submitUser': 'azkaban', 'startTime': 1509458450181, 'endTime': 1509489787442, 'flowId': 'run_dataimport', 'projectId': 68, 'execId': 1656, 'status': 'SUCCEEDED'}, {'submitTime': 1509444049936, 'submitUser': 'azkaban', 'startTime': 1509444050155, 'endTime': 1509457451629, 'flowId': 'run_dataimport', 'projectId': 68, 'execId': 1655, 'status': 'SUCCEEDED'}, {'submitTime': 1509438862311, 'submitUser': 'azkaban', 'startTime': 1509438862468, 'endTime': 1509438932606, 'flowId': 'run_dataimport', 'projectId': 68, 'execId': 1654, 'status': 'KILLED'}, {'submitTime': 1509438455131, 'submitUser': 'azkaban', 'startTime': 1509438455403, 'endTime': 1509438536090, 'flowId': 'run_dataimport', 'projectId': 68, 'execId': 1653, 'status': 'KILLED'}], 'length': 10, 'project': 'sql_to_hive', 'from': 0, 'projectId': 68, 'flow': 'run_dataimport'}
[2017-11-01 10:36:45,642] app.hive_mysql.AzkabanMonitor [INFO] URL: https://172.23.27.203:8443
[2017-11-01 10:36:45,809] app.hive_mysql.AzkabanMonitor [INFO] Success: Login https://172.23.27.203:8443 with user azkaban
[2017-11-01 10:36:45,809] app.hive_mysql.AzkabanMonitor [DEBUG] {'session.id': 'e4b172c1-478b-46a9-adc0-dc73ab3b7d28', 'status': 'success'}
[2017-11-01 10:36:45,820] app.hive_mysql.AzkabanMonitor [INFO] Success: Fetch Executions of a Flow - run_dataimport
[2017-11-01 10:36:45,821] app.hive_mysql.AzkabanMonitor [DEBUG] {'total': 6, 'executions': [{'submitTime': 1509501650049, 'submitUser': 'azkaban', 'startTime': 1509501650215, 'endTime': -1, 'flowId': 'run_dataimport', 'projectId': 68, 'execId': 1658, 'status': 'RUNNING'}, {'submitTime': 1509494450033, 'submitUser': 'azkaban', 'startTime': 1509494450241, 'endTime': 1509496877667, 'flowId': 'run_dataimport', 'projectId': 68, 'execId': 1657, 'status': 'SUCCEEDED'}, {'submitTime': 1509458449965, 'submitUser': 'azkaban', 'startTime': 1509458450181, 'endTime': 1509489787442, 'flowId': 'run_dataimport', 'projectId': 68, 'execId': 1656, 'status': 'SUCCEEDED'}, {'submitTime': 1509444049936, 'submitUser': 'azkaban', 'startTime': 1509444050155, 'endTime': 1509457451629, 'flowId': 'run_dataimport', 'projectId': 68, 'execId': 1655, 'status': 'SUCCEEDED'}, {'submitTime': 1509438862311, 'submitUser': 'azkaban', 'startTime': 1509438862468, 'endTime': 1509438932606, 'flowId': 'run_dataimport', 'projectId': 68, 'execId': 1654, 'status': 'KILLED'}, {'submitTime': 1509438455131, 'submitUser': 'azkaban', 'startTime': 1509438455403, 'endTime': 1509438536090, 'flowId': 'run_dataimport', 'projectId': 68, 'execId': 1653, 'status': 'KILLED'}], 'length': 10, 'project': 'sql_to_hive', 'from': 0, 'projectId': 68, 'flow': 'run_dataimport'}
[2017-11-01 10:36:46,182] app.hive_mysql.AzkabanMonitor [INFO] Success: Fetch Execution Job Logs - 1658-run_dataimport
[2017-11-01 10:36:46,182] app.hive_mysql.AzkabanMonitor [DEBUG] {'offset': 0, 'data': "01-11-2017 10:00:50 CST run_dataimport INFO - Starting job run_dataimport at 1509501650218\n01-11-2017 10:00:50 CST run_dataimport INFO - azkaban.webserver.url property was not set\n01-11-2017 10:00:50 CST run_dataimport INFO - job JVM args: -Dazkaban.flowid=run_dataimport -Dazkaban.execid=1658 -Dazkaban.jobid=run_dataimport\n01-11-2017 10:00:50 CST run_dataimport INFO - Building command job executor. \n01-11-2017 10:00:50 CST run_dataimport INFO - Memory granted for job run_dataimport\n01-11-2017 10:00:50 CST run_dataimport INFO - 1 commands to execute.\n01-11-2017 10:00:50 CST run_dataimport INFO - cwd=/home/azkaban/azkaban_bk/azkaban-exec-server-3.35.0-22-g5517d50/executions/1658\n01-11-2017 10:00:50 CST run_dataimport INFO - effective user is: azkaban\n01-11-2017 10:00:50 CST run_dataimport INFO - Command: sh /home/sql_to_hive/dataimport.sh\n01-11-2017 10:00:50 CST run_dataimport INFO - Environment variables: {JOB_OUTPUT_PROP_FILE=/home/azkaban/azkaban_bk/azkaban-exec-server-3.35.0-22-g5517d50/executions/1658/run_dataimport_output_2861162274243800284_tmp, JOB_PROP_FILE=/home/azkaban/azkaban_bk/azkaban-exec-server-3.35.0-22-g5517d50/executions/1658/run_dataimport_props_4685474620571842989_tmp, KRB5CCNAME=/tmp/krb5cc__sql_to_hive__run_dataimport__run_dataimport__1658__azkaban, JOB_NAME=run_dataimport}\n01-11-2017 10:00:50 CST run_dataimport INFO - Working directory: /home/azkaban/azkaban_bk/azkaban-exec-server-3.35.0-22-g5517d50/executions/1658\n01-11-2017 10:00:50 CST run_dataimport INFO - log4j:WARN No appenders could be found for logger (org.apache.hive.jdbc.Utils).\n01-11-2017 10:00:50 CST run_dataimport INFO - log4j:WARN Please initialize the log4j system properly.\n01-11-2017 10:00:50 CST run_dataimport INFO - log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.\n01-11-2017 10:00:50 CST run_dataimport INFO - SLF4J: Failed to load class &quot;org.slf4j.impl.StaticLoggerBinder&quot;.\n01-11-2017 10:00:50 CST run_dataimport INFO - SLF4J: Defaulting to no-operation (NOP) logger implementation\n01-11-2017 10:00:50 CST run_dataimport INFO - SLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.\n01-11-2017 10:01:33 CST run_dataimport INFO - 1,32309400,31612700,696700sqlmax:32309400\n01-11-2017 10:01:33 CST run_dataimport INFO - hivemax:31612700\n01-11-2017 10:01:33 CST run_dataimport INFO - counts:696700\n01-11-2017 10:01:33 CST run_dataimport INFO - 14,46700\n01-11-2017 10:01:33 CST run_dataimport INFO - 14\n01-11-2017 10:01:33 CST run_dataimport INFO - Warning: /opt/cloudera/parcels/CDH-5.8.0-1.cdh5.8.0.p0.42/bin/../lib/sqoop/../accumulo does not exist! Accumulo imports will fail.\n01-11-2017 10:01:33 CST run_dataimport INFO - Please set $ACCUMULO_HOME to the root of your Accumulo installation.\n01-11-2017 10:01:35 CST run_dataimport INFO - 17/11/01 10:01:35 INFO sqoop.Sqoop: Running Sqoop version: 1.4.6-cdh5.8.0\n01-11-2017 10:01:35 CST run_dataimport INFO - 17/11/01 10:01:35 WARN sqoop.ConnFactory: Parameter --driver is set to an explicit driver however appropriate connection manager is not being set (via --connection-manager). Sqoop is going to fall back to org.apache.sqoop.manager.GenericJdbcManager. Please specify explicitly which connection manager should be used next time.\n01-11-2017 10:01:35 CST run_dataimport INFO - 17/11/01 10:01:35 INFO manager.SqlManager: Using default fetchSize of 1000\n01-11-2017 10:01:35 CST run_dataimport INFO - 17/11/01 10:01:35 INFO tool.CodeGenTool: Beginning code generation\n01-11-2017 10:01:36 CST run_dataimport INFO - 17/11/01 10:01:36 INFO manager.SqlManager: Executing SQL statement: select top 50000 * from jzj.dbo.jzj_test where uid&gt;=(select uid from (select top 1 uid from jzj.dbo.jzj_test where uid&gt;31612700 and uid&lt;=32309400) a) and uid &lt;=32309400 and  (1 = 0) \n01-11-2017 10:01:36 CST run_dataimport INFO - 17/11/01 10:01:36 INFO manager.SqlManager: Executing SQL statement: select top 50000 * from jzj.dbo.jzj_test where uid&gt;=(select uid from (select top 1 uid from jzj.dbo.jzj_test where uid&gt;31612700 and uid&lt;=32309400) a) and uid &lt;=32309400 and  (1 = 0) \n01-11-2017 10:01:36 CST run_dataimport INFO - 17/11/01 10:01:36 INFO orm.CompilationManager: HADOOP_MAPRED_HOME is /opt/cloudera/parcels/CDH/lib/hadoop-mapreduce\n01-11-2017 10:01:38 CST run_dataimport INFO - Note: /tmp/sqoop-root/compile/ff02930c93108caafd8dbc9f2c064c43/QueryResult.java uses or overrides a deprecated API.\n01-11-2017 10:01:38 CST run_dataimport INFO - Note: Recompile with -Xlint:deprecation for details.\n01-11-2017 10:01:38 CST run_dataimport INFO - 17/11/01 10:01:38 INFO orm.CompilationManager: Writing jar file: /tmp/sqoop-root/compile/ff02930c93108caafd8dbc9f2c064c43/QueryResult.jar\n01-11-2017 10:01:39 CST run_dataimport INFO - 17/11/01 10:01:39 INFO tool.ImportTool: Maximal id query for free form incremental import: SELECT MAX(uid) FROM (select top 50000 * from jzj.dbo.jzj_test where uid&gt;=(select uid from (select top 1 uid from jzj.dbo.jzj_test where uid&gt;31612700 and uid&lt;=32309400) a) and uid &lt;=32309400 and (1 = 1)) sqoop_import_query_alias\n01-11-2017 10:01:39 CST run_dataimport INFO - 17/11/01 10:01:39 INFO tool.ImportTool: Incremental import based on column uid\n01-11-2017 10:01:39 CST run_dataimport INFO - 17/11/01 10:01:39 INFO tool.ImportTool: Upper bound value: 31662700\n01-11-2017 10:01:39 CST run_dataimport INFO - 17/11/01 10:01:39 INFO mapreduce.ImportJobBase: Beginning query import.\n01-11-2017 10:01:39 CST run_dataimport INFO - 17/11/01 10:01:39 INFO Configuration.deprecation: mapred.jar is deprecated. Instead, use mapreduce.job.jar\n01-11-2017 10:01:39 CST run_dataimport INFO - 17/11/01 10:01:39 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n01-11-2017 10:01:39 CST run_dataimport INFO - 17/11/01 10:01:39 INFO client.RMProxy: Connecting to ResourceManager at master/172.23.27.203:8032\n01-11-2017 10:01:45 CST run_dataimport INFO - 17/11/01 10:01:45 INFO db.DBInputFormat: Using read commited transaction isolation\n01-11-2017 10:01:45 CST run_dataimport INFO - 17/11/01 10:01:45 INFO db.DataDrivenDBInputFormat: BoundingValsQuery: SELECT MIN(uid), MAX(uid) FROM (select top 50000 * from jzj.dbo.jzj_test where uid&gt;=(select uid from (select top 1 uid from jzj.dbo.jzj_test where uid&gt;31612700 and uid&lt;=32309400) a) and uid &lt;=32309400 and uid &lt;= 31662700 AND  (1 = 1) ) AS t1\n01-11-2017 10:01:46 CST run_dataimport INFO - 17/11/01 10:01:46 INFO mapreduce.JobSubmitter: number of splits:4\n01-11-2017 10:01:46 CST run_dataimport INFO - 17/11/01 10:01:46 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1508671365448_1897\n01-11-2017 10:01:47 CST run_dataimport INFO - 17/11/01 10:01:47 INFO impl.YarnClientImpl: Submitted application application_1508671365448_1897\n01-11-2017 10:01:47 CST run_dataimport INFO - 17/11/01 10:01:47 INFO mapreduce.Job: The url to track the job: http://master:8088/proxy/application_1508671365448_1897/\n01-11-2017 10:01:47 CST run_dataimport INFO - 17/11/01 10:01:47 INFO mapreduce.Job: Running job: job_1508671365448_1897\n01-11-2017 10:01:54 CST run_dataimport INFO - 17/11/01 10:01:54 INFO mapreduce.Job: Job job_1508671365448_1897 running in uber mode : false\n01-11-2017 10:01:54 CST run_dataimport INFO - 17/11/01 10:01:54 INFO mapreduce.Job:  map 0% reduce 0%\n01-11-2017 10:02:02 CST run_dataimport INFO - 17/11/01 10:02:02 INFO mapreduce.Job:  map 25% reduce 0%\n01-11-2017 10:02:03 CST run_dataimport INFO - 17/11/01 10:02:03 INFO mapreduce.Job:  map 100% reduce 0%\n01-11-2017 10:02:05 CST run_dataimport INFO - 17/11/01 10:02:05 INFO mapreduce.Job: Job job_1508671365448_1897 completed successfully\n01-11-2017 10:02:05 CST run_dataimport INFO - 17/11/01 10:02:05 INFO mapreduce.Job: Counters: 30\n01-11-2017 10:02:05 CST run_dataimport INFO - \tFile System Counters\n01-11-2017 10:02:05 CST run_dataimport INFO - \t\tFILE: Number of bytes read=0\n01-11-2017 10:02:05 CST run_dataimport INFO - \t\tFILE: Number of bytes written=587012\n01-11-2017 10:02:05 CST run_dataimport INFO - \t\tFILE: Number of read operations=0\n01-11-2017 10:02:05 CST run_dataimport INFO - \t\tFILE: Number of large read operations=0\n01-11-2017 10:02:05 CST run_dataimport INFO - \t\tFILE: Number of write operations=0\n01-11-2017 10:02:05 CST run_dataimport INFO - \t\tHDFS: Number of bytes read=462\n01-11-2017 10:02:05 CST run_dataimport INFO - \t\tHDFS: Number of bytes written=23450000\n01-11-2017 10:02:05 CST run_dataimport INFO - \t\tHDFS: Number of read operations=16\n01-11-2017 10:02:05 CST run_dataimport INFO - \t\tHDFS: Number of large read operations=0\n01-11-2017 10:02:05 CST run_dataimport INFO - \t\tHDFS: Number of write operations=8\n01-11-2017 10:02:05 CST run_dataimport INFO - \tJob Counters \n01-11-2017 10:02:05 CST run_dataimport INFO - \t\tLaunched map tasks=4\n01-11-2017 10:02:05 CST run_dataimport INFO - \t\tOther local map tasks=4\n01-11-2017 10:02:05 CST run_dataimport INFO - \t\tTotal time spent by all maps in occupied slots (ms)=26571\n01-11-2017 10:02:05 CST run_dataimport INFO - \t\tTotal time spent by all reduces in occupied slots (ms)=0\n01-11-2017 10:02:05 CST run_dataimport INFO - \t\tTotal time spent by all map tasks (ms)=26571\n01-11-2017 10:02:05 CST run_dataimport INFO - \t\tTotal vcore-seconds taken by all map tasks=26571\n01-11-2017 10:02:05 CST run_dataimport INFO - \t\tTotal megabyte-seconds taken by all map tasks=27208704\n01-11-2017 10:02:05 CST run_dataimport INFO - \tMap-Reduce Framework\n01-11-2017 10:02:05 CST run_dataimport INFO - \t\tMap input records=50000\n01-11-2017 10:02:05 CST run_dataimport INFO - \t\tMap output records=50000\n01-11-2017 10:02:05 CST run_dataimport INFO - \t\tInput split bytes=462\n01-11-2017 10:02:05 CST run_dataimport INFO - \t\tSpilled Records=0\n01-11-2017 10:02:05 CST run_dataimport INFO - \t\tFailed Shuffles=0\n01-11-2017 10:02:05 CST run_dataimport INFO - \t\tMerged Map outputs=0\n01-11-2017 10:02:05 CST run_dataimport INFO - \t\tGC time elapsed (ms)=211\n01-11-2017 10:02:05 CST run_dataimport INFO - \t\tCPU time spent (ms)=24550\n01-11-2017 10:02:05 CST run_dataimport INFO - \t\tPhysical memory (bytes) snapshot=1433231360\n01-11-2017 10:02:05 CST run_dataimport INFO - \t\tVirtual memory (bytes) snapshot=6511280128\n01-11-2017 10:02:05 CST run_dataimport INFO - \t\tTotal committed heap usage (bytes)=3296722944\n01-11-2017 10:02:05 CST run_dataimport INFO - \tFile Input Format Counters \n01-11-2017 10:02:05 CST run_dataimport INFO - \t\tBytes Read=0\n01-11-2017 10:02:05 CST run_dataimport INFO - \tFile Output Format Counters \n01-11-2017 10:02:05 CST run_dataimport INFO - \t\tBytes Written=23450000\n01-11-2017 10:02:05 CST run_dataimport INFO - 17/11/01 10:02:05 INFO mapreduce.ImportJobBase: Transferred 22.3637 MB in 26.2622 seconds (871.9906 KB/sec)\n01-11-2017 10:02:05 CST run_dataimport INFO - 17/11/01 10:02:05 INFO mapreduce.ImportJobBase: Retrieved 50000 records.\n01-11-2017 10:02:05 CST run_dataimport INFO - 17/11/01 10:02:05 INFO util.AppendUtils: Appending to directory sql_to_hive\n01-11-2017 10:02:06 CST run_dataimport INFO - 17/11/01 10:02:06 INFO util.AppendUtils: Using found partition 2660\n01-11-2017 10:02:06 CST run_dataimport INFO - 17/11/01 10:02:06 INFO tool.ImportTool: Incremental import complete! To run another incremental import of all data following this import, supply the following arguments:\n01-11-2017 10:02:06 CST run_dataimport INFO - 17/11/01 10:02:06 INFO tool.ImportTool:  --incremental append\n01-11-2017 10:02:06 CST run_dataimport INFO - 17/11/01 10:02:06 INFO tool.ImportTool:   --check-column uid\n01-11-2017 10:02:06 CST run_dataimport INFO - 17/11/01 10:02:06 INFO tool.ImportTool:   --last-value 31662700\n01-11-2017 10:02:06 CST run_dataimport INFO - 17/11/01 10:02:06 INFO tool.ImportTool: (Consider saving this with 'sqoop job --create')\n01-11-2017 10:02:06 CST run_dataimport INFO - 13\n01-11-2017 10:02:06 CST run_dataimport INFO - Warning: /opt/cloudera/parcels/CDH-5.8.0-1.cdh5.8.0.p0.42/bin/../lib/sqoop/../accumulo does not exist! Accumulo imports will fail.\n01-11-2017 10:02:06 CST run_dataimport INFO - Please set $ACCUMULO_HOME to the root of your Accumulo installation.\n01-11-2017 10:02:08 CST run_dataimport INFO - 17/11/01 10:02:08 INFO sqoop.Sqoop: Running Sqoop version: 1.4.6-cdh5.8.0\n01-11-2017 10:02:08 CST run_dataimport INFO - 17/11/01 10:02:08 WARN sqoop.ConnFactory: Parameter --driver is set to an explicit driver however appropriate connection manager is not being set (via --connection-manager). Sqoop is going to fall back to org.apache.sqoop.manager.GenericJdbcManager. Please specify explicitly which connection manager should be used next time.\n01-11-2017 10:02:08 CST run_dataimport INFO - 17/11/01 10:02:08 INFO manager.SqlManager: Using default fetchSize of 1000\n01-11-2017 10:02:08 CST run_dataimport INFO - 17/11/01 10:02:08 INFO tool.CodeGenTool: Beginning code generation\n01-11-2017 10:02:09 CST run_dataimport INFO - 17/11/01 10:02:09 INFO manager.SqlManager: Executing SQL statement: select top 50000 * from jzj.dbo.jzj_test where uid&gt;(select uid from (select top 50000 uid from jzj.dbo.jzj_test where uid&gt;31612700 and uid&lt;=32309400) a except select uid from (select top 49999 uid from jzj.dbo.jzj_test where uid&gt;31612700 and uid&lt;=32309400) a) and uid &lt;=32309400 and  (1 = 0) \n01-11-2017 10:02:09 CST run_dataimport INFO - 17/11/01 10:02:09 INFO manager.SqlManager: Executing SQL statement: select top 50000 * from jzj.dbo.jzj_test where uid&gt;(select uid from (select top 50000 uid from jzj.dbo.jzj_test where uid&gt;31612700 and uid&lt;=32309400) a except select uid from (select top 49999 uid from jzj.dbo.jzj_test where uid&gt;31612700 and uid&lt;=32309400) a) and uid &lt;=32309400 and  (1 = 0) \n01-11-2017 10:02:09 CST run_dataimport INFO - 17/11/01 10:02:09 INFO orm.CompilationManager: HADOOP_MAPRED_HOME is /opt/cloudera/parcels/CDH/lib/hadoop-mapreduce\n01-11-2017 10:02:11 CST run_dataimport INFO - Note: /tmp/sqoop-root/compile/9de72518c89c5280b88b044ac6fd6c80/QueryResult.java uses or overrides a deprecated API.\n01-11-2017 10:02:11 CST run_dataimport INFO - Note: Recompile with -Xlint:deprecation for details.\n01-11-2017 10:02:11 CST run_dataimport INFO - 17/11/01 10:02:11 INFO orm.CompilationManager: Writing jar file: /tmp/sqoop-root/compile/9de72518c89c5280b88b044ac6fd6c80/QueryResult.jar\n01-11-2017 10:02:12 CST run_dataimport INFO - 17/11/01 10:02:12 INFO tool.ImportTool: Maximal id query for free form incremental import: SELECT MAX(uid) FROM (select top 50000 * from jzj.dbo.jzj_test where uid&gt;(select uid from (select top 50000 uid from jzj.dbo.jzj_test where uid&gt;31612700 and uid&lt;=32309400) a except select uid from (select top 49999 uid from jzj.dbo.jzj_test where uid&gt;31612700 and uid&lt;=32309400) a) and uid &lt;=32309400 and (1 = 1)) sqoop_import_query_alias\n01-11-2017 10:04:47 CST run_dataimport INFO - 17/11/01 10:04:47 INFO tool.ImportTool: Incremental import based on column uid\n01-11-2017 10:04:47 CST run_dataimport INFO - 17/11/01 10:04:47 INFO tool.ImportTool: Upper bound value: 31712700\n01-11-2017 10:04:47 CST run_dataimport INFO - 17/11/01 10:04:47 INFO mapreduce.ImportJobBase: Beginning query import.\n01-11-2017 10:04:47 CST run_dataimport INFO - 17/11/01 10:04:47 INFO Configuration.deprecation: mapred.jar is deprecated. Instead, use mapreduce.job.jar\n01-11-2017 10:04:47 CST run_dataimport INFO - 17/11/01 10:04:47 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n01-11-2017 10:04:47 CST run_dataimport INFO - 17/11/01 10:04:47 INFO client.RMProxy: Connecting to ResourceManager at master/172.23.27.203:8032\n01-11-2017 10:04:52 CST run_dataimport INFO - 17/11/01 10:04:52 INFO db.DBInputFormat: Using read commited transaction isolation\n01-11-2017 10:04:52 CST run_dataimport INFO - 17/11/01 10:04:52 INFO db.DataDrivenDBInputFormat: BoundingValsQuery: SELECT MIN(uid), MAX(uid) FROM (select top 50000 * from jzj.dbo.jzj_test where uid&gt;(select uid from (select top 50000 uid from jzj.dbo.jzj_test where uid&gt;31612700 and uid&lt;=32309400) a except select uid from (select top 49999 uid from jzj.dbo.jzj_test where uid&gt;31612700 and uid&lt;=32309400) a) and uid &lt;=32309400 and uid &lt;= 31712700 AND  (1 = 1) ) AS t1\n01-11-2017 10:07:26 CST run_dataimport INFO - 17/11/01 10:07:26 INFO mapreduce.JobSubmitter: number of splits:4\n01-11-2017 10:07:27 CST run_dataimport INFO - 17/11/01 10:07:27 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1508671365448_1898\n01-11-2017 10:07:27 CST run_dataimport INFO - 17/11/01 10:07:27 INFO impl.YarnClientImpl: Submitted application application_1508671365448_1898\n01-11-2017 10:07:27 CST run_dataimport INFO - 17/11/01 10:07:27 INFO mapreduce.Job: The url to track the job: http://master:8088/proxy/application_1508671365448_1898/\n01-11-2017 10:07:27 CST run_dataimport INFO - 17/11/01 10:07:27 INFO mapreduce.Job: Running job: job_1508671365448_1898\n01-11-2017 10:07:34 CST run_dataimport INFO - 17/11/01 10:07:34 INFO mapreduce.Job: Job job_1508671365448_1898 running in uber mode : false\n01-11-2017 10:07:34 CST run_dataimport INFO - 17/11/01 10:07:34 INFO mapreduce.Job:  map 0% reduce 0%\n01-11-2017 10:10:17 CST run_dataimport INFO - 17/11/01 10:10:17 INFO mapreduce.Job:  map 25% reduce 0%\n01-11-2017 10:10:19 CST run_dataimport INFO - 17/11/01 10:10:19 INFO mapreduce.Job:  map 75% reduce 0%\n01-11-2017 10:10:20 CST run_dataimport INFO - 17/11/01 10:10:20 INFO mapreduce.Job:  map 100% reduce 0%\n01-11-2017 10:10:21 CST run_dataimport INFO - 17/11/01 10:10:21 INFO mapreduce.Job: Job job_1508671365448_1898 completed successfully\n01-11-2017 10:10:21 CST run_dataimport INFO - 17/11/01 10:10:21 INFO mapreduce.Job: Counters: 30\n01-11-2017 10:10:21 CST run_dataimport INFO - \tFile System Counters\n01-11-2017 10:10:21 CST run_dataimport INFO - \t\tFILE: Number of bytes read=0\n01-11-2017 10:10:21 CST run_dataimport INFO - \t\tFILE: Number of bytes written=588412\n01-11-2017 10:10:21 CST run_dataimport INFO - \t\tFILE: Number of read operations=0\n01-11-2017 10:10:21 CST run_dataimport INFO - \t\tFILE: Number of large read operations=0\n01-11-2017 10:10:21 CST run_dataimport INFO - \t\tFILE: Number of write operations=0\n01-11-2017 10:10:21 CST run_dataimport INFO - \t\tHDFS: Number of bytes read=462\n01-11-2017 10:10:21 CST run_dataimport INFO - \t\tHDFS: Number of bytes written=23450000\n01-11-2017 10:10:21 CST run_dataimport INFO - \t\tHDFS: Number of read operations=16\n01-11-2017 10:10:21 CST run_dataimport INFO - \t\tHDFS: Number of large read operations=0\n01-11-2017 10:10:21 CST run_dataimport INFO - \t\tHDFS: Number of write operations=8\n01-11-2017 10:10:21 CST run_dataimport INFO - \tJob Counters \n01-11-2017 10:10:21 CST run_dataimport INFO - \t\tLaunched map tasks=4\n01-11-2017 10:10:21 CST run_dataimport INFO - \t\tOther local map tasks=4\n01-11-2017 10:10:21 CST run_dataimport INFO - \t\tTotal time spent by all maps in occupied slots (ms)=645979\n01-11-2017 10:10:21 CST run_dataimport INFO - \t\tTotal time spent by all reduces in occupied slots (ms)=0\n01-11-2017 10:10:21 CST run_dataimport INFO - \t\tTotal time spent by all map tasks (ms)=645979\n01-11-2017 10:10:21 CST run_dataimport INFO - \t\tTotal vcore-seconds taken by all map tasks=645979\n01-11-2017 10:10:21 CST run_dataimport INFO - \t\tTotal megabyte-seconds taken by all map tasks=661482496\n01-11-2017 10:10:21 CST run_dataimport INFO - \tMap-Reduce Framework\n01-11-2017 10:10:21 CST run_dataimport INFO - \t\tMap input records=50000\n01-11-2017 10:10:21 CST run_dataimport INFO - \t\tMap output records=50000\n01-11-2017 10:10:21 CST run_dataimport INFO - \t\tInput split bytes=462\n01-11-2017 10:10:21 CST run_dataimport INFO - \t\tSpilled Records=0\n01-11-2017 10:10:21 CST run_dataimport INFO - \t\tFailed Shuffles=0\n01-11-2017 10:10:21 CST run_dataimport INFO - \t\tMerged Map outputs=0\n01-11-2017 10:10:21 CST run_dataimport INFO - \t\tGC time elapsed (ms)=299\n01-11-2017 10:10:21 CST run_dataimport INFO - \t\tCPU time spent (ms)=26430\n01-11-2017 10:10:21 CST run_dataimport INFO - \t\tPhysical memory (bytes) snapshot=1455120384\n01-11-2017 10:10:21 CST run_dataimport INFO - \t\tVirtual memory (bytes) snapshot=6527934464\n01-11-2017 10:10:21 CST run_dataimport INFO - \t\tTotal committed heap usage (bytes)=3296722944\n01-11-2017 10:10:21 CST run_dataimport INFO - \tFile Input Format Counters \n01-11-2017 10:10:21 CST run_dataimport INFO - \t\tBytes Read=0\n01-11-2017 10:10:21 CST run_dataimport INFO - \tFile Output Format Counters \n01-11-2017 10:10:21 CST run_dataimport INFO - \t\tBytes Written=23450000\n01-11-2017 10:10:21 CST run_dataimport INFO - 17/11/01 10:10:21 INFO mapreduce.ImportJobBase: Transferred 22.3637 MB in 333.7909 seconds (68.607 KB/sec)\n01-11-2017 10:10:21 CST run_dataimport INFO - 17/11/01 10:10:21 INFO mapreduce.ImportJobBase: Retrieved 50000 records.\n01-11-2017 10:10:21 CST run_dataimport INFO - 17/11/01 10:10:21 INFO util.AppendUtils: Appending to directory sql_to_hive\n01-11-2017 10:10:21 CST run_dataimport INFO - 17/11/01 10:10:21 INFO util.AppendUtils: Using found partition 2664\n01-11-2017 10:10:21 CST run_dataimport INFO - 17/11/01 10:10:21 INFO tool.ImportTool: Incremental import complete! To run another incremental import of all data following this import, supply the following arguments:\n01-11-2017 10:10:21 CST run_dataimport INFO - 17/11/01 10:10:21 INFO tool.ImportTool:  --incremental append\n01-11-2017 10:10:21 CST run_dataimport INFO - 17/11/01 10:10:21 INFO tool.ImportTool:   --check-column uid\n01-11-2017 10:10:21 CST run_dataimport INFO - 17/11/01 10:10:21 INFO tool.ImportTool:   --last-value 31712700\n01-11-2017 10:10:21 CST run_dataimport INFO - 17/11/01 10:10:21 INFO tool.ImportTool: (Consider saving this with 'sqoop job --create')\n01-11-2017 10:10:22 CST run_dataimport INFO - 12\n01-11-2017 10:10:22 CST run_dataimport INFO - Warning: /opt/cloudera/parcels/CDH-5.8.0-1.cdh5.8.0.p0.42/bin/../lib/sqoop/../accumulo does not exist! Accumulo imports will fail.\n01-11-2017 10:10:22 CST run_dataimport INFO - Please set $ACCUMULO_HOME to the root of your Accumulo installation.\n01-11-2017 10:10:24 CST run_dataimport INFO - 17/11/01 10:10:24 INFO sqoop.Sqoop: Running Sqoop version: 1.4.6-cdh5.8.0\n01-11-2017 10:10:24 CST run_dataimport INFO - 17/11/01 10:10:24 WARN sqoop.ConnFactory: Parameter --driver is set to an explicit driver however appropriate connection manager is not being set (via --connection-manager). Sqoop is going to fall back to org.apache.sqoop.manager.GenericJdbcManager. Please specify explicitly which connection manager should be used next time.\n01-11-2017 10:10:24 CST run_dataimport INFO - 17/11/01 10:10:24 INFO manager.SqlManager: Using default fetchSize of 1000\n01-11-2017 10:10:24 CST run_dataimport INFO - 17/11/01 10:10:24 INFO tool.CodeGenTool: Beginning code generation\n01-11-2017 10:10:24 CST run_dataimport INFO - 17/11/01 10:10:24 INFO manager.SqlManager: Executing SQL statement: select top 50000 * from jzj.dbo.jzj_test where uid&gt;(select uid from (select top 100000 uid from jzj.dbo.jzj_test where uid&gt;31612700 and uid&lt;=32309400) a except select uid from (select top 99999 uid from jzj.dbo.jzj_test where uid&gt;31612700 and uid&lt;=32309400) a) and uid &lt;=32309400 and  (1 = 0) \n01-11-2017 10:10:24 CST run_dataimport INFO - 17/11/01 10:10:24 INFO manager.SqlManager: Executing SQL statement: select top 50000 * from jzj.dbo.jzj_test where uid&gt;(select uid from (select top 100000 uid from jzj.dbo.jzj_test where uid&gt;31612700 and uid&lt;=32309400) a except select uid from (select top 99999 uid from jzj.dbo.jzj_test where uid&gt;31612700 and uid&lt;=32309400) a) and uid &lt;=32309400 and  (1 = 0) \n01-11-2017 10:10:25 CST run_dataimport INFO - 17/11/01 10:10:25 INFO orm.CompilationManager: HADOOP_MAPRED_HOME is /opt/cloudera/parcels/CDH/lib/hadoop-mapreduce\n01-11-2017 10:10:27 CST run_dataimport INFO - Note: /tmp/sqoop-root/compile/9f82a203e0f4d154a66e65e3115fe8a9/QueryResult.java uses or overrides a deprecated API.\n01-11-2017 10:10:27 CST run_dataimport INFO - Note: Recompile with -Xlint:deprecation for details.\n01-11-2017 10:10:27 CST run_dataimport INFO - 17/11/01 10:10:27 INFO orm.CompilationManager: Writing jar file: /tmp/sqoop-root/compile/9f82a203e0f4d154a66e65e3115fe8a9/QueryResult.jar\n01-11-2017 10:10:28 CST run_dataimport INFO - 17/11/01 10:10:28 INFO tool.ImportTool: Maximal id query for free form incremental import: SELECT MAX(uid) FROM (select top 50000 * from jzj.dbo.jzj_test where uid&gt;(select uid from (select top 100000 uid from jzj.dbo.jzj_test where uid&gt;31612700 and uid&lt;=32309400) a except select uid from (select top 99999 uid from jzj.dbo.jzj_test where uid&gt;31612700 and uid&lt;=32309400) a) and uid &lt;=32309400 and (1 = 1)) sqoop_import_query_alias\n01-11-2017 10:20:33 CST run_dataimport INFO - 17/11/01 10:20:33 INFO tool.ImportTool: Incremental import based on column uid\n01-11-2017 10:20:33 CST run_dataimport INFO - 17/11/01 10:20:33 INFO tool.ImportTool: Upper bound value: 31762700\n01-11-2017 10:20:33 CST run_dataimport INFO - 17/11/01 10:20:33 INFO mapreduce.ImportJobBase: Beginning query import.\n01-11-2017 10:20:33 CST run_dataimport INFO - 17/11/01 10:20:33 INFO Configuration.deprecation: mapred.jar is deprecated. Instead, use mapreduce.job.jar\n01-11-2017 10:20:33 CST run_dataimport INFO - 17/11/01 10:20:33 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n01-11-2017 10:20:33 CST run_dataimport INFO - 17/11/01 10:20:33 INFO client.RMProxy: Connecting to ResourceManager at master/172.23.27.203:8032\n01-11-2017 10:20:40 CST run_dataimport INFO - 17/11/01 10:20:40 INFO db.DBInputFormat: Using read commited transaction isolation\n01-11-2017 10:20:40 CST run_dataimport INFO - 17/11/01 10:20:40 INFO db.DataDrivenDBInputFormat: BoundingValsQuery: SELECT MIN(uid), MAX(uid) FROM (select top 50000 * from jzj.dbo.jzj_test where uid&gt;(select uid from (select top 100000 uid from jzj.dbo.jzj_test where uid&gt;31612700 and uid&lt;=32309400) a except select uid from (select top 99999 uid from jzj.dbo.jzj_test where uid&gt;31612700 and uid&lt;=32309400) a) and uid &lt;=32309400 and uid &lt;= 31762700 AND  (1 = 1) ) AS t1\n01-11-2017 10:30:36 CST run_dataimport INFO - 17/11/01 10:30:36 INFO mapreduce.JobSubmitter: number of splits:4\n01-11-2017 10:30:37 CST run_dataimport INFO - 17/11/01 10:30:37 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1508671365448_1899\n01-11-2017 10:30:37 CST run_dataimport INFO - 17/11/01 10:30:37 INFO impl.YarnClientImpl: Submitted application application_1508671365448_1899\n01-11-2017 10:30:37 CST run_dataimport INFO - 17/11/01 10:30:37 INFO mapreduce.Job: The url to track the job: http://master:8088/proxy/application_1508671365448_1899/\n01-11-2017 10:30:37 CST run_dataimport INFO - 17/11/01 10:30:37 INFO mapreduce.Job: Running job: job_1508671365448_1899\n01-11-2017 10:30:44 CST run_dataimport INFO - 17/11/01 10:30:44 INFO mapreduce.Job: Job job_1508671365448_1899 running in uber mode : false\n01-11-2017 10:30:44 CST run_dataimport INFO - 17/11/01 10:30:44 INFO mapreduce.Job:  map 0% reduce 0%\n", 'length': 26858}
[2017-11-01 10:50:55,229] app.hive_mysql.AzkabanMonitor [INFO] URL: https://172.23.27.203:8443
[2017-11-01 10:50:55,405] app.hive_mysql.AzkabanMonitor [INFO] Success: Login https://172.23.27.203:8443 with user azkaban
[2017-11-01 10:50:55,406] app.hive_mysql.AzkabanMonitor [DEBUG] {'session.id': 'd0d0fff0-5303-4c61-b732-6243d46c5043', 'status': 'success'}
[2017-11-01 10:50:55,417] app.hive_mysql.AzkabanMonitor [INFO] Success: Fetch Executions of a Flow - run_dataimport
[2017-11-01 10:50:55,417] app.hive_mysql.AzkabanMonitor [DEBUG] {'total': 6, 'executions': [{'submitTime': 1509501650049, 'submitUser': 'azkaban', 'startTime': 1509501650215, 'endTime': -1, 'flowId': 'run_dataimport', 'projectId': 68, 'execId': 1658, 'status': 'RUNNING'}, {'submitTime': 1509494450033, 'submitUser': 'azkaban', 'startTime': 1509494450241, 'endTime': 1509496877667, 'flowId': 'run_dataimport', 'projectId': 68, 'execId': 1657, 'status': 'SUCCEEDED'}, {'submitTime': 1509458449965, 'submitUser': 'azkaban', 'startTime': 1509458450181, 'endTime': 1509489787442, 'flowId': 'run_dataimport', 'projectId': 68, 'execId': 1656, 'status': 'SUCCEEDED'}, {'submitTime': 1509444049936, 'submitUser': 'azkaban', 'startTime': 1509444050155, 'endTime': 1509457451629, 'flowId': 'run_dataimport', 'projectId': 68, 'execId': 1655, 'status': 'SUCCEEDED'}, {'submitTime': 1509438862311, 'submitUser': 'azkaban', 'startTime': 1509438862468, 'endTime': 1509438932606, 'flowId': 'run_dataimport', 'projectId': 68, 'execId': 1654, 'status': 'KILLED'}, {'submitTime': 1509438455131, 'submitUser': 'azkaban', 'startTime': 1509438455403, 'endTime': 1509438536090, 'flowId': 'run_dataimport', 'projectId': 68, 'execId': 1653, 'status': 'KILLED'}], 'length': 10, 'project': 'sql_to_hive', 'from': 0, 'projectId': 68, 'flow': 'run_dataimport'}
[2017-11-01 10:50:55,730] app.hive_mysql.AzkabanMonitor [INFO] Success: Fetch Execution Job Logs - 1658-run_dataimport
[2017-11-01 10:50:55,730] app.hive_mysql.AzkabanMonitor [DEBUG] {'offset': 0, 'data': "01-11-2017 10:00:50 CST run_dataimport INFO - Starting job run_dataimport at 1509501650218\n01-11-2017 10:00:50 CST run_dataimport INFO - azkaban.webserver.url property was not set\n01-11-2017 10:00:50 CST run_dataimport INFO - job JVM args: -Dazkaban.flowid=run_dataimport -Dazkaban.execid=1658 -Dazkaban.jobid=run_dataimport\n01-11-2017 10:00:50 CST run_dataimport INFO - Building command job executor. \n01-11-2017 10:00:50 CST run_dataimport INFO - Memory granted for job run_dataimport\n01-11-2017 10:00:50 CST run_dataimport INFO - 1 commands to execute.\n01-11-2017 10:00:50 CST run_dataimport INFO - cwd=/home/azkaban/azkaban_bk/azkaban-exec-server-3.35.0-22-g5517d50/executions/1658\n01-11-2017 10:00:50 CST run_dataimport INFO - effective user is: azkaban\n01-11-2017 10:00:50 CST run_dataimport INFO - Command: sh /home/sql_to_hive/dataimport.sh\n01-11-2017 10:00:50 CST run_dataimport INFO - Environment variables: {JOB_OUTPUT_PROP_FILE=/home/azkaban/azkaban_bk/azkaban-exec-server-3.35.0-22-g5517d50/executions/1658/run_dataimport_output_2861162274243800284_tmp, JOB_PROP_FILE=/home/azkaban/azkaban_bk/azkaban-exec-server-3.35.0-22-g5517d50/executions/1658/run_dataimport_props_4685474620571842989_tmp, KRB5CCNAME=/tmp/krb5cc__sql_to_hive__run_dataimport__run_dataimport__1658__azkaban, JOB_NAME=run_dataimport}\n01-11-2017 10:00:50 CST run_dataimport INFO - Working directory: /home/azkaban/azkaban_bk/azkaban-exec-server-3.35.0-22-g5517d50/executions/1658\n01-11-2017 10:00:50 CST run_dataimport INFO - log4j:WARN No appenders could be found for logger (org.apache.hive.jdbc.Utils).\n01-11-2017 10:00:50 CST run_dataimport INFO - log4j:WARN Please initialize the log4j system properly.\n01-11-2017 10:00:50 CST run_dataimport INFO - log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.\n01-11-2017 10:00:50 CST run_dataimport INFO - SLF4J: Failed to load class &quot;org.slf4j.impl.StaticLoggerBinder&quot;.\n01-11-2017 10:00:50 CST run_dataimport INFO - SLF4J: Defaulting to no-operation (NOP) logger implementation\n01-11-2017 10:00:50 CST run_dataimport INFO - SLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.\n01-11-2017 10:01:33 CST run_dataimport INFO - 1,32309400,31612700,696700sqlmax:32309400\n01-11-2017 10:01:33 CST run_dataimport INFO - hivemax:31612700\n01-11-2017 10:01:33 CST run_dataimport INFO - counts:696700\n01-11-2017 10:01:33 CST run_dataimport INFO - 14,46700\n01-11-2017 10:01:33 CST run_dataimport INFO - 14\n01-11-2017 10:01:33 CST run_dataimport INFO - Warning: /opt/cloudera/parcels/CDH-5.8.0-1.cdh5.8.0.p0.42/bin/../lib/sqoop/../accumulo does not exist! Accumulo imports will fail.\n01-11-2017 10:01:33 CST run_dataimport INFO - Please set $ACCUMULO_HOME to the root of your Accumulo installation.\n01-11-2017 10:01:35 CST run_dataimport INFO - 17/11/01 10:01:35 INFO sqoop.Sqoop: Running Sqoop version: 1.4.6-cdh5.8.0\n01-11-2017 10:01:35 CST run_dataimport INFO - 17/11/01 10:01:35 WARN sqoop.ConnFactory: Parameter --driver is set to an explicit driver however appropriate connection manager is not being set (via --connection-manager). Sqoop is going to fall back to org.apache.sqoop.manager.GenericJdbcManager. Please specify explicitly which connection manager should be used next time.\n01-11-2017 10:01:35 CST run_dataimport INFO - 17/11/01 10:01:35 INFO manager.SqlManager: Using default fetchSize of 1000\n01-11-2017 10:01:35 CST run_dataimport INFO - 17/11/01 10:01:35 INFO tool.CodeGenTool: Beginning code generation\n01-11-2017 10:01:36 CST run_dataimport INFO - 17/11/01 10:01:36 INFO manager.SqlManager: Executing SQL statement: select top 50000 * from jzj.dbo.jzj_test where uid&gt;=(select uid from (select top 1 uid from jzj.dbo.jzj_test where uid&gt;31612700 and uid&lt;=32309400) a) and uid &lt;=32309400 and  (1 = 0) \n01-11-2017 10:01:36 CST run_dataimport INFO - 17/11/01 10:01:36 INFO manager.SqlManager: Executing SQL statement: select top 50000 * from jzj.dbo.jzj_test where uid&gt;=(select uid from (select top 1 uid from jzj.dbo.jzj_test where uid&gt;31612700 and uid&lt;=32309400) a) and uid &lt;=32309400 and  (1 = 0) \n01-11-2017 10:01:36 CST run_dataimport INFO - 17/11/01 10:01:36 INFO orm.CompilationManager: HADOOP_MAPRED_HOME is /opt/cloudera/parcels/CDH/lib/hadoop-mapreduce\n01-11-2017 10:01:38 CST run_dataimport INFO - Note: /tmp/sqoop-root/compile/ff02930c93108caafd8dbc9f2c064c43/QueryResult.java uses or overrides a deprecated API.\n01-11-2017 10:01:38 CST run_dataimport INFO - Note: Recompile with -Xlint:deprecation for details.\n01-11-2017 10:01:38 CST run_dataimport INFO - 17/11/01 10:01:38 INFO orm.CompilationManager: Writing jar file: /tmp/sqoop-root/compile/ff02930c93108caafd8dbc9f2c064c43/QueryResult.jar\n01-11-2017 10:01:39 CST run_dataimport INFO - 17/11/01 10:01:39 INFO tool.ImportTool: Maximal id query for free form incremental import: SELECT MAX(uid) FROM (select top 50000 * from jzj.dbo.jzj_test where uid&gt;=(select uid from (select top 1 uid from jzj.dbo.jzj_test where uid&gt;31612700 and uid&lt;=32309400) a) and uid &lt;=32309400 and (1 = 1)) sqoop_import_query_alias\n01-11-2017 10:01:39 CST run_dataimport INFO - 17/11/01 10:01:39 INFO tool.ImportTool: Incremental import based on column uid\n01-11-2017 10:01:39 CST run_dataimport INFO - 17/11/01 10:01:39 INFO tool.ImportTool: Upper bound value: 31662700\n01-11-2017 10:01:39 CST run_dataimport INFO - 17/11/01 10:01:39 INFO mapreduce.ImportJobBase: Beginning query import.\n01-11-2017 10:01:39 CST run_dataimport INFO - 17/11/01 10:01:39 INFO Configuration.deprecation: mapred.jar is deprecated. Instead, use mapreduce.job.jar\n01-11-2017 10:01:39 CST run_dataimport INFO - 17/11/01 10:01:39 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n01-11-2017 10:01:39 CST run_dataimport INFO - 17/11/01 10:01:39 INFO client.RMProxy: Connecting to ResourceManager at master/172.23.27.203:8032\n01-11-2017 10:01:45 CST run_dataimport INFO - 17/11/01 10:01:45 INFO db.DBInputFormat: Using read commited transaction isolation\n01-11-2017 10:01:45 CST run_dataimport INFO - 17/11/01 10:01:45 INFO db.DataDrivenDBInputFormat: BoundingValsQuery: SELECT MIN(uid), MAX(uid) FROM (select top 50000 * from jzj.dbo.jzj_test where uid&gt;=(select uid from (select top 1 uid from jzj.dbo.jzj_test where uid&gt;31612700 and uid&lt;=32309400) a) and uid &lt;=32309400 and uid &lt;= 31662700 AND  (1 = 1) ) AS t1\n01-11-2017 10:01:46 CST run_dataimport INFO - 17/11/01 10:01:46 INFO mapreduce.JobSubmitter: number of splits:4\n01-11-2017 10:01:46 CST run_dataimport INFO - 17/11/01 10:01:46 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1508671365448_1897\n01-11-2017 10:01:47 CST run_dataimport INFO - 17/11/01 10:01:47 INFO impl.YarnClientImpl: Submitted application application_1508671365448_1897\n01-11-2017 10:01:47 CST run_dataimport INFO - 17/11/01 10:01:47 INFO mapreduce.Job: The url to track the job: http://master:8088/proxy/application_1508671365448_1897/\n01-11-2017 10:01:47 CST run_dataimport INFO - 17/11/01 10:01:47 INFO mapreduce.Job: Running job: job_1508671365448_1897\n01-11-2017 10:01:54 CST run_dataimport INFO - 17/11/01 10:01:54 INFO mapreduce.Job: Job job_1508671365448_1897 running in uber mode : false\n01-11-2017 10:01:54 CST run_dataimport INFO - 17/11/01 10:01:54 INFO mapreduce.Job:  map 0% reduce 0%\n01-11-2017 10:02:02 CST run_dataimport INFO - 17/11/01 10:02:02 INFO mapreduce.Job:  map 25% reduce 0%\n01-11-2017 10:02:03 CST run_dataimport INFO - 17/11/01 10:02:03 INFO mapreduce.Job:  map 100% reduce 0%\n01-11-2017 10:02:05 CST run_dataimport INFO - 17/11/01 10:02:05 INFO mapreduce.Job: Job job_1508671365448_1897 completed successfully\n01-11-2017 10:02:05 CST run_dataimport INFO - 17/11/01 10:02:05 INFO mapreduce.Job: Counters: 30\n01-11-2017 10:02:05 CST run_dataimport INFO - \tFile System Counters\n01-11-2017 10:02:05 CST run_dataimport INFO - \t\tFILE: Number of bytes read=0\n01-11-2017 10:02:05 CST run_dataimport INFO - \t\tFILE: Number of bytes written=587012\n01-11-2017 10:02:05 CST run_dataimport INFO - \t\tFILE: Number of read operations=0\n01-11-2017 10:02:05 CST run_dataimport INFO - \t\tFILE: Number of large read operations=0\n01-11-2017 10:02:05 CST run_dataimport INFO - \t\tFILE: Number of write operations=0\n01-11-2017 10:02:05 CST run_dataimport INFO - \t\tHDFS: Number of bytes read=462\n01-11-2017 10:02:05 CST run_dataimport INFO - \t\tHDFS: Number of bytes written=23450000\n01-11-2017 10:02:05 CST run_dataimport INFO - \t\tHDFS: Number of read operations=16\n01-11-2017 10:02:05 CST run_dataimport INFO - \t\tHDFS: Number of large read operations=0\n01-11-2017 10:02:05 CST run_dataimport INFO - \t\tHDFS: Number of write operations=8\n01-11-2017 10:02:05 CST run_dataimport INFO - \tJob Counters \n01-11-2017 10:02:05 CST run_dataimport INFO - \t\tLaunched map tasks=4\n01-11-2017 10:02:05 CST run_dataimport INFO - \t\tOther local map tasks=4\n01-11-2017 10:02:05 CST run_dataimport INFO - \t\tTotal time spent by all maps in occupied slots (ms)=26571\n01-11-2017 10:02:05 CST run_dataimport INFO - \t\tTotal time spent by all reduces in occupied slots (ms)=0\n01-11-2017 10:02:05 CST run_dataimport INFO - \t\tTotal time spent by all map tasks (ms)=26571\n01-11-2017 10:02:05 CST run_dataimport INFO - \t\tTotal vcore-seconds taken by all map tasks=26571\n01-11-2017 10:02:05 CST run_dataimport INFO - \t\tTotal megabyte-seconds taken by all map tasks=27208704\n01-11-2017 10:02:05 CST run_dataimport INFO - \tMap-Reduce Framework\n01-11-2017 10:02:05 CST run_dataimport INFO - \t\tMap input records=50000\n01-11-2017 10:02:05 CST run_dataimport INFO - \t\tMap output records=50000\n01-11-2017 10:02:05 CST run_dataimport INFO - \t\tInput split bytes=462\n01-11-2017 10:02:05 CST run_dataimport INFO - \t\tSpilled Records=0\n01-11-2017 10:02:05 CST run_dataimport INFO - \t\tFailed Shuffles=0\n01-11-2017 10:02:05 CST run_dataimport INFO - \t\tMerged Map outputs=0\n01-11-2017 10:02:05 CST run_dataimport INFO - \t\tGC time elapsed (ms)=211\n01-11-2017 10:02:05 CST run_dataimport INFO - \t\tCPU time spent (ms)=24550\n01-11-2017 10:02:05 CST run_dataimport INFO - \t\tPhysical memory (bytes) snapshot=1433231360\n01-11-2017 10:02:05 CST run_dataimport INFO - \t\tVirtual memory (bytes) snapshot=6511280128\n01-11-2017 10:02:05 CST run_dataimport INFO - \t\tTotal committed heap usage (bytes)=3296722944\n01-11-2017 10:02:05 CST run_dataimport INFO - \tFile Input Format Counters \n01-11-2017 10:02:05 CST run_dataimport INFO - \t\tBytes Read=0\n01-11-2017 10:02:05 CST run_dataimport INFO - \tFile Output Format Counters \n01-11-2017 10:02:05 CST run_dataimport INFO - \t\tBytes Written=23450000\n01-11-2017 10:02:05 CST run_dataimport INFO - 17/11/01 10:02:05 INFO mapreduce.ImportJobBase: Transferred 22.3637 MB in 26.2622 seconds (871.9906 KB/sec)\n01-11-2017 10:02:05 CST run_dataimport INFO - 17/11/01 10:02:05 INFO mapreduce.ImportJobBase: Retrieved 50000 records.\n01-11-2017 10:02:05 CST run_dataimport INFO - 17/11/01 10:02:05 INFO util.AppendUtils: Appending to directory sql_to_hive\n01-11-2017 10:02:06 CST run_dataimport INFO - 17/11/01 10:02:06 INFO util.AppendUtils: Using found partition 2660\n01-11-2017 10:02:06 CST run_dataimport INFO - 17/11/01 10:02:06 INFO tool.ImportTool: Incremental import complete! To run another incremental import of all data following this import, supply the following arguments:\n01-11-2017 10:02:06 CST run_dataimport INFO - 17/11/01 10:02:06 INFO tool.ImportTool:  --incremental append\n01-11-2017 10:02:06 CST run_dataimport INFO - 17/11/01 10:02:06 INFO tool.ImportTool:   --check-column uid\n01-11-2017 10:02:06 CST run_dataimport INFO - 17/11/01 10:02:06 INFO tool.ImportTool:   --last-value 31662700\n01-11-2017 10:02:06 CST run_dataimport INFO - 17/11/01 10:02:06 INFO tool.ImportTool: (Consider saving this with 'sqoop job --create')\n01-11-2017 10:02:06 CST run_dataimport INFO - 13\n01-11-2017 10:02:06 CST run_dataimport INFO - Warning: /opt/cloudera/parcels/CDH-5.8.0-1.cdh5.8.0.p0.42/bin/../lib/sqoop/../accumulo does not exist! Accumulo imports will fail.\n01-11-2017 10:02:06 CST run_dataimport INFO - Please set $ACCUMULO_HOME to the root of your Accumulo installation.\n01-11-2017 10:02:08 CST run_dataimport INFO - 17/11/01 10:02:08 INFO sqoop.Sqoop: Running Sqoop version: 1.4.6-cdh5.8.0\n01-11-2017 10:02:08 CST run_dataimport INFO - 17/11/01 10:02:08 WARN sqoop.ConnFactory: Parameter --driver is set to an explicit driver however appropriate connection manager is not being set (via --connection-manager). Sqoop is going to fall back to org.apache.sqoop.manager.GenericJdbcManager. Please specify explicitly which connection manager should be used next time.\n01-11-2017 10:02:08 CST run_dataimport INFO - 17/11/01 10:02:08 INFO manager.SqlManager: Using default fetchSize of 1000\n01-11-2017 10:02:08 CST run_dataimport INFO - 17/11/01 10:02:08 INFO tool.CodeGenTool: Beginning code generation\n01-11-2017 10:02:09 CST run_dataimport INFO - 17/11/01 10:02:09 INFO manager.SqlManager: Executing SQL statement: select top 50000 * from jzj.dbo.jzj_test where uid&gt;(select uid from (select top 50000 uid from jzj.dbo.jzj_test where uid&gt;31612700 and uid&lt;=32309400) a except select uid from (select top 49999 uid from jzj.dbo.jzj_test where uid&gt;31612700 and uid&lt;=32309400) a) and uid &lt;=32309400 and  (1 = 0) \n01-11-2017 10:02:09 CST run_dataimport INFO - 17/11/01 10:02:09 INFO manager.SqlManager: Executing SQL statement: select top 50000 * from jzj.dbo.jzj_test where uid&gt;(select uid from (select top 50000 uid from jzj.dbo.jzj_test where uid&gt;31612700 and uid&lt;=32309400) a except select uid from (select top 49999 uid from jzj.dbo.jzj_test where uid&gt;31612700 and uid&lt;=32309400) a) and uid &lt;=32309400 and  (1 = 0) \n01-11-2017 10:02:09 CST run_dataimport INFO - 17/11/01 10:02:09 INFO orm.CompilationManager: HADOOP_MAPRED_HOME is /opt/cloudera/parcels/CDH/lib/hadoop-mapreduce\n01-11-2017 10:02:11 CST run_dataimport INFO - Note: /tmp/sqoop-root/compile/9de72518c89c5280b88b044ac6fd6c80/QueryResult.java uses or overrides a deprecated API.\n01-11-2017 10:02:11 CST run_dataimport INFO - Note: Recompile with -Xlint:deprecation for details.\n01-11-2017 10:02:11 CST run_dataimport INFO - 17/11/01 10:02:11 INFO orm.CompilationManager: Writing jar file: /tmp/sqoop-root/compile/9de72518c89c5280b88b044ac6fd6c80/QueryResult.jar\n01-11-2017 10:02:12 CST run_dataimport INFO - 17/11/01 10:02:12 INFO tool.ImportTool: Maximal id query for free form incremental import: SELECT MAX(uid) FROM (select top 50000 * from jzj.dbo.jzj_test where uid&gt;(select uid from (select top 50000 uid from jzj.dbo.jzj_test where uid&gt;31612700 and uid&lt;=32309400) a except select uid from (select top 49999 uid from jzj.dbo.jzj_test where uid&gt;31612700 and uid&lt;=32309400) a) and uid &lt;=32309400 and (1 = 1)) sqoop_import_query_alias\n01-11-2017 10:04:47 CST run_dataimport INFO - 17/11/01 10:04:47 INFO tool.ImportTool: Incremental import based on column uid\n01-11-2017 10:04:47 CST run_dataimport INFO - 17/11/01 10:04:47 INFO tool.ImportTool: Upper bound value: 31712700\n01-11-2017 10:04:47 CST run_dataimport INFO - 17/11/01 10:04:47 INFO mapreduce.ImportJobBase: Beginning query import.\n01-11-2017 10:04:47 CST run_dataimport INFO - 17/11/01 10:04:47 INFO Configuration.deprecation: mapred.jar is deprecated. Instead, use mapreduce.job.jar\n01-11-2017 10:04:47 CST run_dataimport INFO - 17/11/01 10:04:47 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n01-11-2017 10:04:47 CST run_dataimport INFO - 17/11/01 10:04:47 INFO client.RMProxy: Connecting to ResourceManager at master/172.23.27.203:8032\n01-11-2017 10:04:52 CST run_dataimport INFO - 17/11/01 10:04:52 INFO db.DBInputFormat: Using read commited transaction isolation\n01-11-2017 10:04:52 CST run_dataimport INFO - 17/11/01 10:04:52 INFO db.DataDrivenDBInputFormat: BoundingValsQuery: SELECT MIN(uid), MAX(uid) FROM (select top 50000 * from jzj.dbo.jzj_test where uid&gt;(select uid from (select top 50000 uid from jzj.dbo.jzj_test where uid&gt;31612700 and uid&lt;=32309400) a except select uid from (select top 49999 uid from jzj.dbo.jzj_test where uid&gt;31612700 and uid&lt;=32309400) a) and uid &lt;=32309400 and uid &lt;= 31712700 AND  (1 = 1) ) AS t1\n01-11-2017 10:07:26 CST run_dataimport INFO - 17/11/01 10:07:26 INFO mapreduce.JobSubmitter: number of splits:4\n01-11-2017 10:07:27 CST run_dataimport INFO - 17/11/01 10:07:27 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1508671365448_1898\n01-11-2017 10:07:27 CST run_dataimport INFO - 17/11/01 10:07:27 INFO impl.YarnClientImpl: Submitted application application_1508671365448_1898\n01-11-2017 10:07:27 CST run_dataimport INFO - 17/11/01 10:07:27 INFO mapreduce.Job: The url to track the job: http://master:8088/proxy/application_1508671365448_1898/\n01-11-2017 10:07:27 CST run_dataimport INFO - 17/11/01 10:07:27 INFO mapreduce.Job: Running job: job_1508671365448_1898\n01-11-2017 10:07:34 CST run_dataimport INFO - 17/11/01 10:07:34 INFO mapreduce.Job: Job job_1508671365448_1898 running in uber mode : false\n01-11-2017 10:07:34 CST run_dataimport INFO - 17/11/01 10:07:34 INFO mapreduce.Job:  map 0% reduce 0%\n01-11-2017 10:10:17 CST run_dataimport INFO - 17/11/01 10:10:17 INFO mapreduce.Job:  map 25% reduce 0%\n01-11-2017 10:10:19 CST run_dataimport INFO - 17/11/01 10:10:19 INFO mapreduce.Job:  map 75% reduce 0%\n01-11-2017 10:10:20 CST run_dataimport INFO - 17/11/01 10:10:20 INFO mapreduce.Job:  map 100% reduce 0%\n01-11-2017 10:10:21 CST run_dataimport INFO - 17/11/01 10:10:21 INFO mapreduce.Job: Job job_1508671365448_1898 completed successfully\n01-11-2017 10:10:21 CST run_dataimport INFO - 17/11/01 10:10:21 INFO mapreduce.Job: Counters: 30\n01-11-2017 10:10:21 CST run_dataimport INFO - \tFile System Counters\n01-11-2017 10:10:21 CST run_dataimport INFO - \t\tFILE: Number of bytes read=0\n01-11-2017 10:10:21 CST run_dataimport INFO - \t\tFILE: Number of bytes written=588412\n01-11-2017 10:10:21 CST run_dataimport INFO - \t\tFILE: Number of read operations=0\n01-11-2017 10:10:21 CST run_dataimport INFO - \t\tFILE: Number of large read operations=0\n01-11-2017 10:10:21 CST run_dataimport INFO - \t\tFILE: Number of write operations=0\n01-11-2017 10:10:21 CST run_dataimport INFO - \t\tHDFS: Number of bytes read=462\n01-11-2017 10:10:21 CST run_dataimport INFO - \t\tHDFS: Number of bytes written=23450000\n01-11-2017 10:10:21 CST run_dataimport INFO - \t\tHDFS: Number of read operations=16\n01-11-2017 10:10:21 CST run_dataimport INFO - \t\tHDFS: Number of large read operations=0\n01-11-2017 10:10:21 CST run_dataimport INFO - \t\tHDFS: Number of write operations=8\n01-11-2017 10:10:21 CST run_dataimport INFO - \tJob Counters \n01-11-2017 10:10:21 CST run_dataimport INFO - \t\tLaunched map tasks=4\n01-11-2017 10:10:21 CST run_dataimport INFO - \t\tOther local map tasks=4\n01-11-2017 10:10:21 CST run_dataimport INFO - \t\tTotal time spent by all maps in occupied slots (ms)=645979\n01-11-2017 10:10:21 CST run_dataimport INFO - \t\tTotal time spent by all reduces in occupied slots (ms)=0\n01-11-2017 10:10:21 CST run_dataimport INFO - \t\tTotal time spent by all map tasks (ms)=645979\n01-11-2017 10:10:21 CST run_dataimport INFO - \t\tTotal vcore-seconds taken by all map tasks=645979\n01-11-2017 10:10:21 CST run_dataimport INFO - \t\tTotal megabyte-seconds taken by all map tasks=661482496\n01-11-2017 10:10:21 CST run_dataimport INFO - \tMap-Reduce Framework\n01-11-2017 10:10:21 CST run_dataimport INFO - \t\tMap input records=50000\n01-11-2017 10:10:21 CST run_dataimport INFO - \t\tMap output records=50000\n01-11-2017 10:10:21 CST run_dataimport INFO - \t\tInput split bytes=462\n01-11-2017 10:10:21 CST run_dataimport INFO - \t\tSpilled Records=0\n01-11-2017 10:10:21 CST run_dataimport INFO - \t\tFailed Shuffles=0\n01-11-2017 10:10:21 CST run_dataimport INFO - \t\tMerged Map outputs=0\n01-11-2017 10:10:21 CST run_dataimport INFO - \t\tGC time elapsed (ms)=299\n01-11-2017 10:10:21 CST run_dataimport INFO - \t\tCPU time spent (ms)=26430\n01-11-2017 10:10:21 CST run_dataimport INFO - \t\tPhysical memory (bytes) snapshot=1455120384\n01-11-2017 10:10:21 CST run_dataimport INFO - \t\tVirtual memory (bytes) snapshot=6527934464\n01-11-2017 10:10:21 CST run_dataimport INFO - \t\tTotal committed heap usage (bytes)=3296722944\n01-11-2017 10:10:21 CST run_dataimport INFO - \tFile Input Format Counters \n01-11-2017 10:10:21 CST run_dataimport INFO - \t\tBytes Read=0\n01-11-2017 10:10:21 CST run_dataimport INFO - \tFile Output Format Counters \n01-11-2017 10:10:21 CST run_dataimport INFO - \t\tBytes Written=23450000\n01-11-2017 10:10:21 CST run_dataimport INFO - 17/11/01 10:10:21 INFO mapreduce.ImportJobBase: Transferred 22.3637 MB in 333.7909 seconds (68.607 KB/sec)\n01-11-2017 10:10:21 CST run_dataimport INFO - 17/11/01 10:10:21 INFO mapreduce.ImportJobBase: Retrieved 50000 records.\n01-11-2017 10:10:21 CST run_dataimport INFO - 17/11/01 10:10:21 INFO util.AppendUtils: Appending to directory sql_to_hive\n01-11-2017 10:10:21 CST run_dataimport INFO - 17/11/01 10:10:21 INFO util.AppendUtils: Using found partition 2664\n01-11-2017 10:10:21 CST run_dataimport INFO - 17/11/01 10:10:21 INFO tool.ImportTool: Incremental import complete! To run another incremental import of all data following this import, supply the following arguments:\n01-11-2017 10:10:21 CST run_dataimport INFO - 17/11/01 10:10:21 INFO tool.ImportTool:  --incremental append\n01-11-2017 10:10:21 CST run_dataimport INFO - 17/11/01 10:10:21 INFO tool.ImportTool:   --check-column uid\n01-11-2017 10:10:21 CST run_dataimport INFO - 17/11/01 10:10:21 INFO tool.ImportTool:   --last-value 31712700\n01-11-2017 10:10:21 CST run_dataimport INFO - 17/11/01 10:10:21 INFO tool.ImportTool: (Consider saving this with 'sqoop job --create')\n01-11-2017 10:10:22 CST run_dataimport INFO - 12\n01-11-2017 10:10:22 CST run_dataimport INFO - Warning: /opt/cloudera/parcels/CDH-5.8.0-1.cdh5.8.0.p0.42/bin/../lib/sqoop/../accumulo does not exist! Accumulo imports will fail.\n01-11-2017 10:10:22 CST run_dataimport INFO - Please set $ACCUMULO_HOME to the root of your Accumulo installation.\n01-11-2017 10:10:24 CST run_dataimport INFO - 17/11/01 10:10:24 INFO sqoop.Sqoop: Running Sqoop version: 1.4.6-cdh5.8.0\n01-11-2017 10:10:24 CST run_dataimport INFO - 17/11/01 10:10:24 WARN sqoop.ConnFactory: Parameter --driver is set to an explicit driver however appropriate connection manager is not being set (via --connection-manager). Sqoop is going to fall back to org.apache.sqoop.manager.GenericJdbcManager. Please specify explicitly which connection manager should be used next time.\n01-11-2017 10:10:24 CST run_dataimport INFO - 17/11/01 10:10:24 INFO manager.SqlManager: Using default fetchSize of 1000\n01-11-2017 10:10:24 CST run_dataimport INFO - 17/11/01 10:10:24 INFO tool.CodeGenTool: Beginning code generation\n01-11-2017 10:10:24 CST run_dataimport INFO - 17/11/01 10:10:24 INFO manager.SqlManager: Executing SQL statement: select top 50000 * from jzj.dbo.jzj_test where uid&gt;(select uid from (select top 100000 uid from jzj.dbo.jzj_test where uid&gt;31612700 and uid&lt;=32309400) a except select uid from (select top 99999 uid from jzj.dbo.jzj_test where uid&gt;31612700 and uid&lt;=32309400) a) and uid &lt;=32309400 and  (1 = 0) \n01-11-2017 10:10:24 CST run_dataimport INFO - 17/11/01 10:10:24 INFO manager.SqlManager: Executing SQL statement: select top 50000 * from jzj.dbo.jzj_test where uid&gt;(select uid from (select top 100000 uid from jzj.dbo.jzj_test where uid&gt;31612700 and uid&lt;=32309400) a except select uid from (select top 99999 uid from jzj.dbo.jzj_test where uid&gt;31612700 and uid&lt;=32309400) a) and uid &lt;=32309400 and  (1 = 0) \n01-11-2017 10:10:25 CST run_dataimport INFO - 17/11/01 10:10:25 INFO orm.CompilationManager: HADOOP_MAPRED_HOME is /opt/cloudera/parcels/CDH/lib/hadoop-mapreduce\n01-11-2017 10:10:27 CST run_dataimport INFO - Note: /tmp/sqoop-root/compile/9f82a203e0f4d154a66e65e3115fe8a9/QueryResult.java uses or overrides a deprecated API.\n01-11-2017 10:10:27 CST run_dataimport INFO - Note: Recompile with -Xlint:deprecation for details.\n01-11-2017 10:10:27 CST run_dataimport INFO - 17/11/01 10:10:27 INFO orm.CompilationManager: Writing jar file: /tmp/sqoop-root/compile/9f82a203e0f4d154a66e65e3115fe8a9/QueryResult.jar\n01-11-2017 10:10:28 CST run_dataimport INFO - 17/11/01 10:10:28 INFO tool.ImportTool: Maximal id query for free form incremental import: SELECT MAX(uid) FROM (select top 50000 * from jzj.dbo.jzj_test where uid&gt;(select uid from (select top 100000 uid from jzj.dbo.jzj_test where uid&gt;31612700 and uid&lt;=32309400) a except select uid from (select top 99999 uid from jzj.dbo.jzj_test where uid&gt;31612700 and uid&lt;=32309400) a) and uid &lt;=32309400 and (1 = 1)) sqoop_import_query_alias\n01-11-2017 10:20:33 CST run_dataimport INFO - 17/11/01 10:20:33 INFO tool.ImportTool: Incremental import based on column uid\n01-11-2017 10:20:33 CST run_dataimport INFO - 17/11/01 10:20:33 INFO tool.ImportTool: Upper bound value: 31762700\n01-11-2017 10:20:33 CST run_dataimport INFO - 17/11/01 10:20:33 INFO mapreduce.ImportJobBase: Beginning query import.\n01-11-2017 10:20:33 CST run_dataimport INFO - 17/11/01 10:20:33 INFO Configuration.deprecation: mapred.jar is deprecated. Instead, use mapreduce.job.jar\n01-11-2017 10:20:33 CST run_dataimport INFO - 17/11/01 10:20:33 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n01-11-2017 10:20:33 CST run_dataimport INFO - 17/11/01 10:20:33 INFO client.RMProxy: Connecting to ResourceManager at master/172.23.27.203:8032\n01-11-2017 10:20:40 CST run_dataimport INFO - 17/11/01 10:20:40 INFO db.DBInputFormat: Using read commited transaction isolation\n01-11-2017 10:20:40 CST run_dataimport INFO - 17/11/01 10:20:40 INFO db.DataDrivenDBInputFormat: BoundingValsQuery: SELECT MIN(uid), MAX(uid) FROM (select top 50000 * from jzj.dbo.jzj_test where uid&gt;(select uid from (select top 100000 uid from jzj.dbo.jzj_test where uid&gt;31612700 and uid&lt;=32309400) a except select uid from (select top 99999 uid from jzj.dbo.jzj_test where uid&gt;31612700 and uid&lt;=32309400) a) and uid &lt;=32309400 and uid &lt;= 31762700 AND  (1 = 1) ) AS t1\n01-11-2017 10:30:36 CST run_dataimport INFO - 17/11/01 10:30:36 INFO mapreduce.JobSubmitter: number of splits:4\n01-11-2017 10:30:37 CST run_dataimport INFO - 17/11/01 10:30:37 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1508671365448_1899\n01-11-2017 10:30:37 CST run_dataimport INFO - 17/11/01 10:30:37 INFO impl.YarnClientImpl: Submitted application application_1508671365448_1899\n01-11-2017 10:30:37 CST run_dataimport INFO - 17/11/01 10:30:37 INFO mapreduce.Job: The url to track the job: http://master:8088/proxy/application_1508671365448_1899/\n01-11-2017 10:30:37 CST run_dataimport INFO - 17/11/01 10:30:37 INFO mapreduce.Job: Running job: job_1508671365448_1899\n01-11-2017 10:30:44 CST run_dataimport INFO - 17/11/01 10:30:44 INFO mapreduce.Job: Job job_1508671365448_1899 running in uber mode : false\n01-11-2017 10:30:44 CST run_dataimport INFO - 17/11/01 10:30:44 INFO mapreduce.Job:  map 0% reduce 0%\n01-11-2017 10:40:52 CST run_dataimport INFO - 17/11/01 10:40:52 INFO mapreduce.Job:  map 25% reduce 0%\n01-11-2017 10:40:53 CST run_dataimport INFO - 17/11/01 10:40:53 INFO mapreduce.Job:  map 50% reduce 0%\n01-11-2017 10:41:00 CST run_dataimport INFO - 17/11/01 10:41:00 INFO mapreduce.Job:  map 75% reduce 0%\n01-11-2017 10:41:02 CST run_dataimport INFO - 17/11/01 10:41:02 INFO mapreduce.Job:  map 100% reduce 0%\n01-11-2017 10:41:03 CST run_dataimport INFO - 17/11/01 10:41:03 INFO mapreduce.Job: Job job_1508671365448_1899 completed successfully\n01-11-2017 10:41:03 CST run_dataimport INFO - 17/11/01 10:41:03 INFO mapreduce.Job: Counters: 30\n01-11-2017 10:41:03 CST run_dataimport INFO - \tFile System Counters\n01-11-2017 10:41:03 CST run_dataimport INFO - \t\tFILE: Number of bytes read=0\n01-11-2017 10:41:03 CST run_dataimport INFO - \t\tFILE: Number of bytes written=588428\n01-11-2017 10:41:03 CST run_dataimport INFO - \t\tFILE: Number of read operations=0\n01-11-2017 10:41:03 CST run_dataimport INFO - \t\tFILE: Number of large read operations=0\n01-11-2017 10:41:03 CST run_dataimport INFO - \t\tFILE: Number of write operations=0\n01-11-2017 10:41:03 CST run_dataimport INFO - \t\tHDFS: Number of bytes read=462\n01-11-2017 10:41:03 CST run_dataimport INFO - \t\tHDFS: Number of bytes written=23450000\n01-11-2017 10:41:03 CST run_dataimport INFO - \t\tHDFS: Number of read operations=16\n01-11-2017 10:41:03 CST run_dataimport INFO - \t\tHDFS: Number of large read operations=0\n01-11-2017 10:41:03 CST run_dataimport INFO - \t\tHDFS: Number of write operations=8\n01-11-2017 10:41:03 CST run_dataimport INFO - \tJob Counters \n01-11-2017 10:41:03 CST run_dataimport INFO - \t\tLaunched map tasks=4\n01-11-2017 10:41:03 CST run_dataimport INFO - \t\tOther local map tasks=4\n01-11-2017 10:41:03 CST run_dataimport INFO - \t\tTotal time spent by all maps in occupied slots (ms)=2440026\n01-11-2017 10:41:03 CST run_dataimport INFO - \t\tTotal time spent by all reduces in occupied slots (ms)=0\n01-11-2017 10:41:03 CST run_dataimport INFO - \t\tTotal time spent by all map tasks (ms)=2440026\n01-11-2017 10:41:03 CST run_dataimport INFO - \t\tTotal vcore-seconds taken by all map tasks=2440026\n01-11-2017 10:41:03 CST run_dataimport INFO - \t\tTotal megabyte-seconds taken by all map tasks=2498586624\n01-11-2017 10:41:03 CST run_dataimport INFO - \tMap-Reduce Framework\n01-11-2017 10:41:03 CST run_dataimport INFO - \t\tMap input records=50000\n01-11-2017 10:41:03 CST run_dataimport INFO - \t\tMap output records=50000\n01-11-2017 10:41:03 CST run_dataimport INFO - \t\tInput split bytes=462\n01-11-2017 10:41:03 CST run_dataimport INFO - \t\tSpilled Records=0\n01-11-2017 10:41:03 CST run_dataimport INFO - \t\tFailed Shuffles=0\n01-11-2017 10:41:03 CST run_dataimport INFO - \t\tMerged Map outputs=0\n01-11-2017 10:41:03 CST run_dataimport INFO - \t\tGC time elapsed (ms)=327\n01-11-2017 10:41:03 CST run_dataimport INFO - \t\tCPU time spent (ms)=33410\n01-11-2017 10:41:03 CST run_dataimport INFO - \t\tPhysical memory (bytes) snapshot=1476767744\n01-11-2017 10:41:03 CST run_dataimport INFO - \t\tVirtual memory (bytes) snapshot=6458535936\n01-11-2017 10:41:03 CST run_dataimport INFO - \t\tTotal committed heap usage (bytes)=3296722944\n01-11-2017 10:41:03 CST run_dataimport INFO - \tFile Input Format Counters \n01-11-2017 10:41:03 CST run_dataimport INFO - \t\tBytes Read=0\n01-11-2017 10:41:03 CST run_dataimport INFO - \tFile Output Format Counters \n01-11-2017 10:41:03 CST run_dataimport INFO - \t\tBytes Written=23450000\n01-11-2017 10:41:03 CST run_dataimport INFO - 17/11/01 10:41:03 INFO mapreduce.ImportJobBase: Transferred 22.3637 MB in 1,229.8978 seconds (18.6198 KB/sec)\n01-11-2017 10:41:03 CST run_dataimport INFO - 17/11/01 10:41:03 INFO mapreduce.ImportJobBase: Retrieved 50000 records.\n01-11-2017 10:41:03 CST run_dataimport INFO - 17/11/01 10:41:03 INFO util.AppendUtils: Appending to directory sql_to_hive\n01-11-2017 10:41:03 CST run_dataimport INFO - 17/11/01 10:41:03 INFO util.AppendUtils: Using found partition 2668\n01-11-2017 10:41:04 CST run_dataimport INFO - 17/11/01 10:41:04 INFO tool.ImportTool: Incremental import complete! To run another incremental import of all data following this import, supply the following arguments:\n01-11-2017 10:41:04 CST run_dataimport INFO - 17/11/01 10:41:04 INFO tool.ImportTool:  --incremental append\n01-11-2017 10:41:04 CST run_dataimport INFO - 17/11/01 10:41:04 INFO tool.ImportTool:   --check-column uid\n01-11-2017 10:41:04 CST run_dataimport INFO - 17/11/01 10:41:04 INFO tool.ImportTool:   --last-value 31762700\n01-11-2017 10:41:04 CST run_dataimport INFO - 17/11/01 10:41:04 INFO tool.ImportTool: (Consider saving this with 'sqoop job --create')\n01-11-2017 10:41:04 CST run_dataimport INFO - 11\n01-11-2017 10:41:04 CST run_dataimport INFO - Warning: /opt/cloudera/parcels/CDH-5.8.0-1.cdh5.8.0.p0.42/bin/../lib/sqoop/../accumulo does not exist! Accumulo imports will fail.\n01-11-2017 10:41:04 CST run_dataimport INFO - Please set $ACCUMULO_HOME to the root of your Accumulo installation.\n01-11-2017 10:41:06 CST run_dataimport INFO - 17/11/01 10:41:06 INFO sqoop.Sqoop: Running Sqoop version: 1.4.6-cdh5.8.0\n01-11-2017 10:41:06 CST run_dataimport INFO - 17/11/01 10:41:06 WARN sqoop.ConnFactory: Parameter --driver is set to an explicit driver however appropriate connection manager is not being set (via --connection-manager). Sqoop is going to fall back to org.apache.sqoop.manager.GenericJdbcManager. Please specify explicitly which connection manager should be used next time.\n01-11-2017 10:41:06 CST run_dataimport INFO - 17/11/01 10:41:06 INFO manager.SqlManager: Using default fetchSize of 1000\n01-11-2017 10:41:06 CST run_dataimport INFO - 17/11/01 10:41:06 INFO tool.CodeGenTool: Beginning code generation\n01-11-2017 10:41:07 CST run_dataimport INFO - 17/11/01 10:41:07 INFO manager.SqlManager: Executing SQL statement: select top 50000 * from jzj.dbo.jzj_test where uid&gt;(select uid from (select top 150000 uid from jzj.dbo.jzj_test where uid&gt;31612700 and uid&lt;=32309400) a except select uid from (select top 149999 uid from jzj.dbo.jzj_test where uid&gt;31612700 and uid&lt;=32309400) a) and uid &lt;=32309400 and  (1 = 0) \n01-11-2017 10:41:07 CST run_dataimport INFO - 17/11/01 10:41:07 INFO manager.SqlManager: Executing SQL statement: select top 50000 * from jzj.dbo.jzj_test where uid&gt;(select uid from (select top 150000 uid from jzj.dbo.jzj_test where uid&gt;31612700 and uid&lt;=32309400) a except select uid from (select top 149999 uid from jzj.dbo.jzj_test where uid&gt;31612700 and uid&lt;=32309400) a) and uid &lt;=32309400 and  (1 = 0) \n01-11-2017 10:41:07 CST run_dataimport INFO - 17/11/01 10:41:07 INFO orm.CompilationManager: HADOOP_MAPRED_HOME is /opt/cloudera/parcels/CDH/lib/hadoop-mapreduce\n01-11-2017 10:41:09 CST run_dataimport INFO - Note: /tmp/sqoop-root/compile/61fb0da5f39ceec0b287b16e9e75fd1b/QueryResult.java uses or overrides a deprecated API.\n01-11-2017 10:41:09 CST run_dataimport INFO - Note: Recompile with -Xlint:deprecation for details.\n01-11-2017 10:41:09 CST run_dataimport INFO - 17/11/01 10:41:09 INFO orm.CompilationManager: Writing jar file: /tmp/sqoop-root/compile/61fb0da5f39ceec0b287b16e9e75fd1b/QueryResult.jar\n01-11-2017 10:41:10 CST run_dataimport INFO - 17/11/01 10:41:10 INFO tool.ImportTool: Maximal id query for free form incremental import: SELECT MAX(uid) FROM (select top 50000 * from jzj.dbo.jzj_test where uid&gt;(select uid from (select top 150000 uid from jzj.dbo.jzj_test where uid&gt;31612700 and uid&lt;=32309400) a except select uid from (select top 149999 uid from jzj.dbo.jzj_test where uid&gt;31612700 and uid&lt;=32309400) a) and uid &lt;=32309400 and (1 = 1)) sqoop_import_query_alias\n", 'length': 34468}
[2017-11-01 10:52:48,238] app.hive_mysql.AzkabanMonitor [INFO] URL: https://172.23.27.203:8443
[2017-11-01 10:52:48,417] app.hive_mysql.AzkabanMonitor [INFO] Success: Login https://172.23.27.203:8443 with user azkaban
[2017-11-01 10:52:48,417] app.hive_mysql.AzkabanMonitor [DEBUG] {'session.id': 'eb094791-788c-411a-91e1-908fa8b402c1', 'status': 'success'}
[2017-11-01 10:52:48,430] app.hive_mysql.AzkabanMonitor [INFO] Success: Fetch Executions of a Flow - run_dataimport
[2017-11-01 10:52:48,431] app.hive_mysql.AzkabanMonitor [DEBUG] {'total': 6, 'executions': [{'submitTime': 1509501650049, 'submitUser': 'azkaban', 'startTime': 1509501650215, 'endTime': -1, 'flowId': 'run_dataimport', 'projectId': 68, 'execId': 1658, 'status': 'RUNNING'}, {'submitTime': 1509494450033, 'submitUser': 'azkaban', 'startTime': 1509494450241, 'endTime': 1509496877667, 'flowId': 'run_dataimport', 'projectId': 68, 'execId': 1657, 'status': 'SUCCEEDED'}, {'submitTime': 1509458449965, 'submitUser': 'azkaban', 'startTime': 1509458450181, 'endTime': 1509489787442, 'flowId': 'run_dataimport', 'projectId': 68, 'execId': 1656, 'status': 'SUCCEEDED'}, {'submitTime': 1509444049936, 'submitUser': 'azkaban', 'startTime': 1509444050155, 'endTime': 1509457451629, 'flowId': 'run_dataimport', 'projectId': 68, 'execId': 1655, 'status': 'SUCCEEDED'}, {'submitTime': 1509438862311, 'submitUser': 'azkaban', 'startTime': 1509438862468, 'endTime': 1509438932606, 'flowId': 'run_dataimport', 'projectId': 68, 'execId': 1654, 'status': 'KILLED'}, {'submitTime': 1509438455131, 'submitUser': 'azkaban', 'startTime': 1509438455403, 'endTime': 1509438536090, 'flowId': 'run_dataimport', 'projectId': 68, 'execId': 1653, 'status': 'KILLED'}], 'length': 10, 'project': 'sql_to_hive', 'from': 0, 'projectId': 68, 'flow': 'run_dataimport'}
[2017-11-01 10:52:48,795] app.hive_mysql.AzkabanMonitor [INFO] Success: Fetch Execution Job Logs - 1658-run_dataimport
[2017-11-01 10:52:48,796] app.hive_mysql.AzkabanMonitor [DEBUG] {'offset': 0, 'data': "01-11-2017 10:00:50 CST run_dataimport INFO - Starting job run_dataimport at 1509501650218\n01-11-2017 10:00:50 CST run_dataimport INFO - azkaban.webserver.url property was not set\n01-11-2017 10:00:50 CST run_dataimport INFO - job JVM args: -Dazkaban.flowid=run_dataimport -Dazkaban.execid=1658 -Dazkaban.jobid=run_dataimport\n01-11-2017 10:00:50 CST run_dataimport INFO - Building command job executor. \n01-11-2017 10:00:50 CST run_dataimport INFO - Memory granted for job run_dataimport\n01-11-2017 10:00:50 CST run_dataimport INFO - 1 commands to execute.\n01-11-2017 10:00:50 CST run_dataimport INFO - cwd=/home/azkaban/azkaban_bk/azkaban-exec-server-3.35.0-22-g5517d50/executions/1658\n01-11-2017 10:00:50 CST run_dataimport INFO - effective user is: azkaban\n01-11-2017 10:00:50 CST run_dataimport INFO - Command: sh /home/sql_to_hive/dataimport.sh\n01-11-2017 10:00:50 CST run_dataimport INFO - Environment variables: {JOB_OUTPUT_PROP_FILE=/home/azkaban/azkaban_bk/azkaban-exec-server-3.35.0-22-g5517d50/executions/1658/run_dataimport_output_2861162274243800284_tmp, JOB_PROP_FILE=/home/azkaban/azkaban_bk/azkaban-exec-server-3.35.0-22-g5517d50/executions/1658/run_dataimport_props_4685474620571842989_tmp, KRB5CCNAME=/tmp/krb5cc__sql_to_hive__run_dataimport__run_dataimport__1658__azkaban, JOB_NAME=run_dataimport}\n01-11-2017 10:00:50 CST run_dataimport INFO - Working directory: /home/azkaban/azkaban_bk/azkaban-exec-server-3.35.0-22-g5517d50/executions/1658\n01-11-2017 10:00:50 CST run_dataimport INFO - log4j:WARN No appenders could be found for logger (org.apache.hive.jdbc.Utils).\n01-11-2017 10:00:50 CST run_dataimport INFO - log4j:WARN Please initialize the log4j system properly.\n01-11-2017 10:00:50 CST run_dataimport INFO - log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.\n01-11-2017 10:00:50 CST run_dataimport INFO - SLF4J: Failed to load class &quot;org.slf4j.impl.StaticLoggerBinder&quot;.\n01-11-2017 10:00:50 CST run_dataimport INFO - SLF4J: Defaulting to no-operation (NOP) logger implementation\n01-11-2017 10:00:50 CST run_dataimport INFO - SLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.\n01-11-2017 10:01:33 CST run_dataimport INFO - 1,32309400,31612700,696700sqlmax:32309400\n01-11-2017 10:01:33 CST run_dataimport INFO - hivemax:31612700\n01-11-2017 10:01:33 CST run_dataimport INFO - counts:696700\n01-11-2017 10:01:33 CST run_dataimport INFO - 14,46700\n01-11-2017 10:01:33 CST run_dataimport INFO - 14\n01-11-2017 10:01:33 CST run_dataimport INFO - Warning: /opt/cloudera/parcels/CDH-5.8.0-1.cdh5.8.0.p0.42/bin/../lib/sqoop/../accumulo does not exist! Accumulo imports will fail.\n01-11-2017 10:01:33 CST run_dataimport INFO - Please set $ACCUMULO_HOME to the root of your Accumulo installation.\n01-11-2017 10:01:35 CST run_dataimport INFO - 17/11/01 10:01:35 INFO sqoop.Sqoop: Running Sqoop version: 1.4.6-cdh5.8.0\n01-11-2017 10:01:35 CST run_dataimport INFO - 17/11/01 10:01:35 WARN sqoop.ConnFactory: Parameter --driver is set to an explicit driver however appropriate connection manager is not being set (via --connection-manager). Sqoop is going to fall back to org.apache.sqoop.manager.GenericJdbcManager. Please specify explicitly which connection manager should be used next time.\n01-11-2017 10:01:35 CST run_dataimport INFO - 17/11/01 10:01:35 INFO manager.SqlManager: Using default fetchSize of 1000\n01-11-2017 10:01:35 CST run_dataimport INFO - 17/11/01 10:01:35 INFO tool.CodeGenTool: Beginning code generation\n01-11-2017 10:01:36 CST run_dataimport INFO - 17/11/01 10:01:36 INFO manager.SqlManager: Executing SQL statement: select top 50000 * from jzj.dbo.jzj_test where uid&gt;=(select uid from (select top 1 uid from jzj.dbo.jzj_test where uid&gt;31612700 and uid&lt;=32309400) a) and uid &lt;=32309400 and  (1 = 0) \n01-11-2017 10:01:36 CST run_dataimport INFO - 17/11/01 10:01:36 INFO manager.SqlManager: Executing SQL statement: select top 50000 * from jzj.dbo.jzj_test where uid&gt;=(select uid from (select top 1 uid from jzj.dbo.jzj_test where uid&gt;31612700 and uid&lt;=32309400) a) and uid &lt;=32309400 and  (1 = 0) \n01-11-2017 10:01:36 CST run_dataimport INFO - 17/11/01 10:01:36 INFO orm.CompilationManager: HADOOP_MAPRED_HOME is /opt/cloudera/parcels/CDH/lib/hadoop-mapreduce\n01-11-2017 10:01:38 CST run_dataimport INFO - Note: /tmp/sqoop-root/compile/ff02930c93108caafd8dbc9f2c064c43/QueryResult.java uses or overrides a deprecated API.\n01-11-2017 10:01:38 CST run_dataimport INFO - Note: Recompile with -Xlint:deprecation for details.\n01-11-2017 10:01:38 CST run_dataimport INFO - 17/11/01 10:01:38 INFO orm.CompilationManager: Writing jar file: /tmp/sqoop-root/compile/ff02930c93108caafd8dbc9f2c064c43/QueryResult.jar\n01-11-2017 10:01:39 CST run_dataimport INFO - 17/11/01 10:01:39 INFO tool.ImportTool: Maximal id query for free form incremental import: SELECT MAX(uid) FROM (select top 50000 * from jzj.dbo.jzj_test where uid&gt;=(select uid from (select top 1 uid from jzj.dbo.jzj_test where uid&gt;31612700 and uid&lt;=32309400) a) and uid &lt;=32309400 and (1 = 1)) sqoop_import_query_alias\n01-11-2017 10:01:39 CST run_dataimport INFO - 17/11/01 10:01:39 INFO tool.ImportTool: Incremental import based on column uid\n01-11-2017 10:01:39 CST run_dataimport INFO - 17/11/01 10:01:39 INFO tool.ImportTool: Upper bound value: 31662700\n01-11-2017 10:01:39 CST run_dataimport INFO - 17/11/01 10:01:39 INFO mapreduce.ImportJobBase: Beginning query import.\n01-11-2017 10:01:39 CST run_dataimport INFO - 17/11/01 10:01:39 INFO Configuration.deprecation: mapred.jar is deprecated. Instead, use mapreduce.job.jar\n01-11-2017 10:01:39 CST run_dataimport INFO - 17/11/01 10:01:39 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n01-11-2017 10:01:39 CST run_dataimport INFO - 17/11/01 10:01:39 INFO client.RMProxy: Connecting to ResourceManager at master/172.23.27.203:8032\n01-11-2017 10:01:45 CST run_dataimport INFO - 17/11/01 10:01:45 INFO db.DBInputFormat: Using read commited transaction isolation\n01-11-2017 10:01:45 CST run_dataimport INFO - 17/11/01 10:01:45 INFO db.DataDrivenDBInputFormat: BoundingValsQuery: SELECT MIN(uid), MAX(uid) FROM (select top 50000 * from jzj.dbo.jzj_test where uid&gt;=(select uid from (select top 1 uid from jzj.dbo.jzj_test where uid&gt;31612700 and uid&lt;=32309400) a) and uid &lt;=32309400 and uid &lt;= 31662700 AND  (1 = 1) ) AS t1\n01-11-2017 10:01:46 CST run_dataimport INFO - 17/11/01 10:01:46 INFO mapreduce.JobSubmitter: number of splits:4\n01-11-2017 10:01:46 CST run_dataimport INFO - 17/11/01 10:01:46 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1508671365448_1897\n01-11-2017 10:01:47 CST run_dataimport INFO - 17/11/01 10:01:47 INFO impl.YarnClientImpl: Submitted application application_1508671365448_1897\n01-11-2017 10:01:47 CST run_dataimport INFO - 17/11/01 10:01:47 INFO mapreduce.Job: The url to track the job: http://master:8088/proxy/application_1508671365448_1897/\n01-11-2017 10:01:47 CST run_dataimport INFO - 17/11/01 10:01:47 INFO mapreduce.Job: Running job: job_1508671365448_1897\n01-11-2017 10:01:54 CST run_dataimport INFO - 17/11/01 10:01:54 INFO mapreduce.Job: Job job_1508671365448_1897 running in uber mode : false\n01-11-2017 10:01:54 CST run_dataimport INFO - 17/11/01 10:01:54 INFO mapreduce.Job:  map 0% reduce 0%\n01-11-2017 10:02:02 CST run_dataimport INFO - 17/11/01 10:02:02 INFO mapreduce.Job:  map 25% reduce 0%\n01-11-2017 10:02:03 CST run_dataimport INFO - 17/11/01 10:02:03 INFO mapreduce.Job:  map 100% reduce 0%\n01-11-2017 10:02:05 CST run_dataimport INFO - 17/11/01 10:02:05 INFO mapreduce.Job: Job job_1508671365448_1897 completed successfully\n01-11-2017 10:02:05 CST run_dataimport INFO - 17/11/01 10:02:05 INFO mapreduce.Job: Counters: 30\n01-11-2017 10:02:05 CST run_dataimport INFO - \tFile System Counters\n01-11-2017 10:02:05 CST run_dataimport INFO - \t\tFILE: Number of bytes read=0\n01-11-2017 10:02:05 CST run_dataimport INFO - \t\tFILE: Number of bytes written=587012\n01-11-2017 10:02:05 CST run_dataimport INFO - \t\tFILE: Number of read operations=0\n01-11-2017 10:02:05 CST run_dataimport INFO - \t\tFILE: Number of large read operations=0\n01-11-2017 10:02:05 CST run_dataimport INFO - \t\tFILE: Number of write operations=0\n01-11-2017 10:02:05 CST run_dataimport INFO - \t\tHDFS: Number of bytes read=462\n01-11-2017 10:02:05 CST run_dataimport INFO - \t\tHDFS: Number of bytes written=23450000\n01-11-2017 10:02:05 CST run_dataimport INFO - \t\tHDFS: Number of read operations=16\n01-11-2017 10:02:05 CST run_dataimport INFO - \t\tHDFS: Number of large read operations=0\n01-11-2017 10:02:05 CST run_dataimport INFO - \t\tHDFS: Number of write operations=8\n01-11-2017 10:02:05 CST run_dataimport INFO - \tJob Counters \n01-11-2017 10:02:05 CST run_dataimport INFO - \t\tLaunched map tasks=4\n01-11-2017 10:02:05 CST run_dataimport INFO - \t\tOther local map tasks=4\n01-11-2017 10:02:05 CST run_dataimport INFO - \t\tTotal time spent by all maps in occupied slots (ms)=26571\n01-11-2017 10:02:05 CST run_dataimport INFO - \t\tTotal time spent by all reduces in occupied slots (ms)=0\n01-11-2017 10:02:05 CST run_dataimport INFO - \t\tTotal time spent by all map tasks (ms)=26571\n01-11-2017 10:02:05 CST run_dataimport INFO - \t\tTotal vcore-seconds taken by all map tasks=26571\n01-11-2017 10:02:05 CST run_dataimport INFO - \t\tTotal megabyte-seconds taken by all map tasks=27208704\n01-11-2017 10:02:05 CST run_dataimport INFO - \tMap-Reduce Framework\n01-11-2017 10:02:05 CST run_dataimport INFO - \t\tMap input records=50000\n01-11-2017 10:02:05 CST run_dataimport INFO - \t\tMap output records=50000\n01-11-2017 10:02:05 CST run_dataimport INFO - \t\tInput split bytes=462\n01-11-2017 10:02:05 CST run_dataimport INFO - \t\tSpilled Records=0\n01-11-2017 10:02:05 CST run_dataimport INFO - \t\tFailed Shuffles=0\n01-11-2017 10:02:05 CST run_dataimport INFO - \t\tMerged Map outputs=0\n01-11-2017 10:02:05 CST run_dataimport INFO - \t\tGC time elapsed (ms)=211\n01-11-2017 10:02:05 CST run_dataimport INFO - \t\tCPU time spent (ms)=24550\n01-11-2017 10:02:05 CST run_dataimport INFO - \t\tPhysical memory (bytes) snapshot=1433231360\n01-11-2017 10:02:05 CST run_dataimport INFO - \t\tVirtual memory (bytes) snapshot=6511280128\n01-11-2017 10:02:05 CST run_dataimport INFO - \t\tTotal committed heap usage (bytes)=3296722944\n01-11-2017 10:02:05 CST run_dataimport INFO - \tFile Input Format Counters \n01-11-2017 10:02:05 CST run_dataimport INFO - \t\tBytes Read=0\n01-11-2017 10:02:05 CST run_dataimport INFO - \tFile Output Format Counters \n01-11-2017 10:02:05 CST run_dataimport INFO - \t\tBytes Written=23450000\n01-11-2017 10:02:05 CST run_dataimport INFO - 17/11/01 10:02:05 INFO mapreduce.ImportJobBase: Transferred 22.3637 MB in 26.2622 seconds (871.9906 KB/sec)\n01-11-2017 10:02:05 CST run_dataimport INFO - 17/11/01 10:02:05 INFO mapreduce.ImportJobBase: Retrieved 50000 records.\n01-11-2017 10:02:05 CST run_dataimport INFO - 17/11/01 10:02:05 INFO util.AppendUtils: Appending to directory sql_to_hive\n01-11-2017 10:02:06 CST run_dataimport INFO - 17/11/01 10:02:06 INFO util.AppendUtils: Using found partition 2660\n01-11-2017 10:02:06 CST run_dataimport INFO - 17/11/01 10:02:06 INFO tool.ImportTool: Incremental import complete! To run another incremental import of all data following this import, supply the following arguments:\n01-11-2017 10:02:06 CST run_dataimport INFO - 17/11/01 10:02:06 INFO tool.ImportTool:  --incremental append\n01-11-2017 10:02:06 CST run_dataimport INFO - 17/11/01 10:02:06 INFO tool.ImportTool:   --check-column uid\n01-11-2017 10:02:06 CST run_dataimport INFO - 17/11/01 10:02:06 INFO tool.ImportTool:   --last-value 31662700\n01-11-2017 10:02:06 CST run_dataimport INFO - 17/11/01 10:02:06 INFO tool.ImportTool: (Consider saving this with 'sqoop job --create')\n01-11-2017 10:02:06 CST run_dataimport INFO - 13\n01-11-2017 10:02:06 CST run_dataimport INFO - Warning: /opt/cloudera/parcels/CDH-5.8.0-1.cdh5.8.0.p0.42/bin/../lib/sqoop/../accumulo does not exist! Accumulo imports will fail.\n01-11-2017 10:02:06 CST run_dataimport INFO - Please set $ACCUMULO_HOME to the root of your Accumulo installation.\n01-11-2017 10:02:08 CST run_dataimport INFO - 17/11/01 10:02:08 INFO sqoop.Sqoop: Running Sqoop version: 1.4.6-cdh5.8.0\n01-11-2017 10:02:08 CST run_dataimport INFO - 17/11/01 10:02:08 WARN sqoop.ConnFactory: Parameter --driver is set to an explicit driver however appropriate connection manager is not being set (via --connection-manager). Sqoop is going to fall back to org.apache.sqoop.manager.GenericJdbcManager. Please specify explicitly which connection manager should be used next time.\n01-11-2017 10:02:08 CST run_dataimport INFO - 17/11/01 10:02:08 INFO manager.SqlManager: Using default fetchSize of 1000\n01-11-2017 10:02:08 CST run_dataimport INFO - 17/11/01 10:02:08 INFO tool.CodeGenTool: Beginning code generation\n01-11-2017 10:02:09 CST run_dataimport INFO - 17/11/01 10:02:09 INFO manager.SqlManager: Executing SQL statement: select top 50000 * from jzj.dbo.jzj_test where uid&gt;(select uid from (select top 50000 uid from jzj.dbo.jzj_test where uid&gt;31612700 and uid&lt;=32309400) a except select uid from (select top 49999 uid from jzj.dbo.jzj_test where uid&gt;31612700 and uid&lt;=32309400) a) and uid &lt;=32309400 and  (1 = 0) \n01-11-2017 10:02:09 CST run_dataimport INFO - 17/11/01 10:02:09 INFO manager.SqlManager: Executing SQL statement: select top 50000 * from jzj.dbo.jzj_test where uid&gt;(select uid from (select top 50000 uid from jzj.dbo.jzj_test where uid&gt;31612700 and uid&lt;=32309400) a except select uid from (select top 49999 uid from jzj.dbo.jzj_test where uid&gt;31612700 and uid&lt;=32309400) a) and uid &lt;=32309400 and  (1 = 0) \n01-11-2017 10:02:09 CST run_dataimport INFO - 17/11/01 10:02:09 INFO orm.CompilationManager: HADOOP_MAPRED_HOME is /opt/cloudera/parcels/CDH/lib/hadoop-mapreduce\n01-11-2017 10:02:11 CST run_dataimport INFO - Note: /tmp/sqoop-root/compile/9de72518c89c5280b88b044ac6fd6c80/QueryResult.java uses or overrides a deprecated API.\n01-11-2017 10:02:11 CST run_dataimport INFO - Note: Recompile with -Xlint:deprecation for details.\n01-11-2017 10:02:11 CST run_dataimport INFO - 17/11/01 10:02:11 INFO orm.CompilationManager: Writing jar file: /tmp/sqoop-root/compile/9de72518c89c5280b88b044ac6fd6c80/QueryResult.jar\n01-11-2017 10:02:12 CST run_dataimport INFO - 17/11/01 10:02:12 INFO tool.ImportTool: Maximal id query for free form incremental import: SELECT MAX(uid) FROM (select top 50000 * from jzj.dbo.jzj_test where uid&gt;(select uid from (select top 50000 uid from jzj.dbo.jzj_test where uid&gt;31612700 and uid&lt;=32309400) a except select uid from (select top 49999 uid from jzj.dbo.jzj_test where uid&gt;31612700 and uid&lt;=32309400) a) and uid &lt;=32309400 and (1 = 1)) sqoop_import_query_alias\n01-11-2017 10:04:47 CST run_dataimport INFO - 17/11/01 10:04:47 INFO tool.ImportTool: Incremental import based on column uid\n01-11-2017 10:04:47 CST run_dataimport INFO - 17/11/01 10:04:47 INFO tool.ImportTool: Upper bound value: 31712700\n01-11-2017 10:04:47 CST run_dataimport INFO - 17/11/01 10:04:47 INFO mapreduce.ImportJobBase: Beginning query import.\n01-11-2017 10:04:47 CST run_dataimport INFO - 17/11/01 10:04:47 INFO Configuration.deprecation: mapred.jar is deprecated. Instead, use mapreduce.job.jar\n01-11-2017 10:04:47 CST run_dataimport INFO - 17/11/01 10:04:47 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n01-11-2017 10:04:47 CST run_dataimport INFO - 17/11/01 10:04:47 INFO client.RMProxy: Connecting to ResourceManager at master/172.23.27.203:8032\n01-11-2017 10:04:52 CST run_dataimport INFO - 17/11/01 10:04:52 INFO db.DBInputFormat: Using read commited transaction isolation\n01-11-2017 10:04:52 CST run_dataimport INFO - 17/11/01 10:04:52 INFO db.DataDrivenDBInputFormat: BoundingValsQuery: SELECT MIN(uid), MAX(uid) FROM (select top 50000 * from jzj.dbo.jzj_test where uid&gt;(select uid from (select top 50000 uid from jzj.dbo.jzj_test where uid&gt;31612700 and uid&lt;=32309400) a except select uid from (select top 49999 uid from jzj.dbo.jzj_test where uid&gt;31612700 and uid&lt;=32309400) a) and uid &lt;=32309400 and uid &lt;= 31712700 AND  (1 = 1) ) AS t1\n01-11-2017 10:07:26 CST run_dataimport INFO - 17/11/01 10:07:26 INFO mapreduce.JobSubmitter: number of splits:4\n01-11-2017 10:07:27 CST run_dataimport INFO - 17/11/01 10:07:27 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1508671365448_1898\n01-11-2017 10:07:27 CST run_dataimport INFO - 17/11/01 10:07:27 INFO impl.YarnClientImpl: Submitted application application_1508671365448_1898\n01-11-2017 10:07:27 CST run_dataimport INFO - 17/11/01 10:07:27 INFO mapreduce.Job: The url to track the job: http://master:8088/proxy/application_1508671365448_1898/\n01-11-2017 10:07:27 CST run_dataimport INFO - 17/11/01 10:07:27 INFO mapreduce.Job: Running job: job_1508671365448_1898\n01-11-2017 10:07:34 CST run_dataimport INFO - 17/11/01 10:07:34 INFO mapreduce.Job: Job job_1508671365448_1898 running in uber mode : false\n01-11-2017 10:07:34 CST run_dataimport INFO - 17/11/01 10:07:34 INFO mapreduce.Job:  map 0% reduce 0%\n01-11-2017 10:10:17 CST run_dataimport INFO - 17/11/01 10:10:17 INFO mapreduce.Job:  map 25% reduce 0%\n01-11-2017 10:10:19 CST run_dataimport INFO - 17/11/01 10:10:19 INFO mapreduce.Job:  map 75% reduce 0%\n01-11-2017 10:10:20 CST run_dataimport INFO - 17/11/01 10:10:20 INFO mapreduce.Job:  map 100% reduce 0%\n01-11-2017 10:10:21 CST run_dataimport INFO - 17/11/01 10:10:21 INFO mapreduce.Job: Job job_1508671365448_1898 completed successfully\n01-11-2017 10:10:21 CST run_dataimport INFO - 17/11/01 10:10:21 INFO mapreduce.Job: Counters: 30\n01-11-2017 10:10:21 CST run_dataimport INFO - \tFile System Counters\n01-11-2017 10:10:21 CST run_dataimport INFO - \t\tFILE: Number of bytes read=0\n01-11-2017 10:10:21 CST run_dataimport INFO - \t\tFILE: Number of bytes written=588412\n01-11-2017 10:10:21 CST run_dataimport INFO - \t\tFILE: Number of read operations=0\n01-11-2017 10:10:21 CST run_dataimport INFO - \t\tFILE: Number of large read operations=0\n01-11-2017 10:10:21 CST run_dataimport INFO - \t\tFILE: Number of write operations=0\n01-11-2017 10:10:21 CST run_dataimport INFO - \t\tHDFS: Number of bytes read=462\n01-11-2017 10:10:21 CST run_dataimport INFO - \t\tHDFS: Number of bytes written=23450000\n01-11-2017 10:10:21 CST run_dataimport INFO - \t\tHDFS: Number of read operations=16\n01-11-2017 10:10:21 CST run_dataimport INFO - \t\tHDFS: Number of large read operations=0\n01-11-2017 10:10:21 CST run_dataimport INFO - \t\tHDFS: Number of write operations=8\n01-11-2017 10:10:21 CST run_dataimport INFO - \tJob Counters \n01-11-2017 10:10:21 CST run_dataimport INFO - \t\tLaunched map tasks=4\n01-11-2017 10:10:21 CST run_dataimport INFO - \t\tOther local map tasks=4\n01-11-2017 10:10:21 CST run_dataimport INFO - \t\tTotal time spent by all maps in occupied slots (ms)=645979\n01-11-2017 10:10:21 CST run_dataimport INFO - \t\tTotal time spent by all reduces in occupied slots (ms)=0\n01-11-2017 10:10:21 CST run_dataimport INFO - \t\tTotal time spent by all map tasks (ms)=645979\n01-11-2017 10:10:21 CST run_dataimport INFO - \t\tTotal vcore-seconds taken by all map tasks=645979\n01-11-2017 10:10:21 CST run_dataimport INFO - \t\tTotal megabyte-seconds taken by all map tasks=661482496\n01-11-2017 10:10:21 CST run_dataimport INFO - \tMap-Reduce Framework\n01-11-2017 10:10:21 CST run_dataimport INFO - \t\tMap input records=50000\n01-11-2017 10:10:21 CST run_dataimport INFO - \t\tMap output records=50000\n01-11-2017 10:10:21 CST run_dataimport INFO - \t\tInput split bytes=462\n01-11-2017 10:10:21 CST run_dataimport INFO - \t\tSpilled Records=0\n01-11-2017 10:10:21 CST run_dataimport INFO - \t\tFailed Shuffles=0\n01-11-2017 10:10:21 CST run_dataimport INFO - \t\tMerged Map outputs=0\n01-11-2017 10:10:21 CST run_dataimport INFO - \t\tGC time elapsed (ms)=299\n01-11-2017 10:10:21 CST run_dataimport INFO - \t\tCPU time spent (ms)=26430\n01-11-2017 10:10:21 CST run_dataimport INFO - \t\tPhysical memory (bytes) snapshot=1455120384\n01-11-2017 10:10:21 CST run_dataimport INFO - \t\tVirtual memory (bytes) snapshot=6527934464\n01-11-2017 10:10:21 CST run_dataimport INFO - \t\tTotal committed heap usage (bytes)=3296722944\n01-11-2017 10:10:21 CST run_dataimport INFO - \tFile Input Format Counters \n01-11-2017 10:10:21 CST run_dataimport INFO - \t\tBytes Read=0\n01-11-2017 10:10:21 CST run_dataimport INFO - \tFile Output Format Counters \n01-11-2017 10:10:21 CST run_dataimport INFO - \t\tBytes Written=23450000\n01-11-2017 10:10:21 CST run_dataimport INFO - 17/11/01 10:10:21 INFO mapreduce.ImportJobBase: Transferred 22.3637 MB in 333.7909 seconds (68.607 KB/sec)\n01-11-2017 10:10:21 CST run_dataimport INFO - 17/11/01 10:10:21 INFO mapreduce.ImportJobBase: Retrieved 50000 records.\n01-11-2017 10:10:21 CST run_dataimport INFO - 17/11/01 10:10:21 INFO util.AppendUtils: Appending to directory sql_to_hive\n01-11-2017 10:10:21 CST run_dataimport INFO - 17/11/01 10:10:21 INFO util.AppendUtils: Using found partition 2664\n01-11-2017 10:10:21 CST run_dataimport INFO - 17/11/01 10:10:21 INFO tool.ImportTool: Incremental import complete! To run another incremental import of all data following this import, supply the following arguments:\n01-11-2017 10:10:21 CST run_dataimport INFO - 17/11/01 10:10:21 INFO tool.ImportTool:  --incremental append\n01-11-2017 10:10:21 CST run_dataimport INFO - 17/11/01 10:10:21 INFO tool.ImportTool:   --check-column uid\n01-11-2017 10:10:21 CST run_dataimport INFO - 17/11/01 10:10:21 INFO tool.ImportTool:   --last-value 31712700\n01-11-2017 10:10:21 CST run_dataimport INFO - 17/11/01 10:10:21 INFO tool.ImportTool: (Consider saving this with 'sqoop job --create')\n01-11-2017 10:10:22 CST run_dataimport INFO - 12\n01-11-2017 10:10:22 CST run_dataimport INFO - Warning: /opt/cloudera/parcels/CDH-5.8.0-1.cdh5.8.0.p0.42/bin/../lib/sqoop/../accumulo does not exist! Accumulo imports will fail.\n01-11-2017 10:10:22 CST run_dataimport INFO - Please set $ACCUMULO_HOME to the root of your Accumulo installation.\n01-11-2017 10:10:24 CST run_dataimport INFO - 17/11/01 10:10:24 INFO sqoop.Sqoop: Running Sqoop version: 1.4.6-cdh5.8.0\n01-11-2017 10:10:24 CST run_dataimport INFO - 17/11/01 10:10:24 WARN sqoop.ConnFactory: Parameter --driver is set to an explicit driver however appropriate connection manager is not being set (via --connection-manager). Sqoop is going to fall back to org.apache.sqoop.manager.GenericJdbcManager. Please specify explicitly which connection manager should be used next time.\n01-11-2017 10:10:24 CST run_dataimport INFO - 17/11/01 10:10:24 INFO manager.SqlManager: Using default fetchSize of 1000\n01-11-2017 10:10:24 CST run_dataimport INFO - 17/11/01 10:10:24 INFO tool.CodeGenTool: Beginning code generation\n01-11-2017 10:10:24 CST run_dataimport INFO - 17/11/01 10:10:24 INFO manager.SqlManager: Executing SQL statement: select top 50000 * from jzj.dbo.jzj_test where uid&gt;(select uid from (select top 100000 uid from jzj.dbo.jzj_test where uid&gt;31612700 and uid&lt;=32309400) a except select uid from (select top 99999 uid from jzj.dbo.jzj_test where uid&gt;31612700 and uid&lt;=32309400) a) and uid &lt;=32309400 and  (1 = 0) \n01-11-2017 10:10:24 CST run_dataimport INFO - 17/11/01 10:10:24 INFO manager.SqlManager: Executing SQL statement: select top 50000 * from jzj.dbo.jzj_test where uid&gt;(select uid from (select top 100000 uid from jzj.dbo.jzj_test where uid&gt;31612700 and uid&lt;=32309400) a except select uid from (select top 99999 uid from jzj.dbo.jzj_test where uid&gt;31612700 and uid&lt;=32309400) a) and uid &lt;=32309400 and  (1 = 0) \n01-11-2017 10:10:25 CST run_dataimport INFO - 17/11/01 10:10:25 INFO orm.CompilationManager: HADOOP_MAPRED_HOME is /opt/cloudera/parcels/CDH/lib/hadoop-mapreduce\n01-11-2017 10:10:27 CST run_dataimport INFO - Note: /tmp/sqoop-root/compile/9f82a203e0f4d154a66e65e3115fe8a9/QueryResult.java uses or overrides a deprecated API.\n01-11-2017 10:10:27 CST run_dataimport INFO - Note: Recompile with -Xlint:deprecation for details.\n01-11-2017 10:10:27 CST run_dataimport INFO - 17/11/01 10:10:27 INFO orm.CompilationManager: Writing jar file: /tmp/sqoop-root/compile/9f82a203e0f4d154a66e65e3115fe8a9/QueryResult.jar\n01-11-2017 10:10:28 CST run_dataimport INFO - 17/11/01 10:10:28 INFO tool.ImportTool: Maximal id query for free form incremental import: SELECT MAX(uid) FROM (select top 50000 * from jzj.dbo.jzj_test where uid&gt;(select uid from (select top 100000 uid from jzj.dbo.jzj_test where uid&gt;31612700 and uid&lt;=32309400) a except select uid from (select top 99999 uid from jzj.dbo.jzj_test where uid&gt;31612700 and uid&lt;=32309400) a) and uid &lt;=32309400 and (1 = 1)) sqoop_import_query_alias\n01-11-2017 10:20:33 CST run_dataimport INFO - 17/11/01 10:20:33 INFO tool.ImportTool: Incremental import based on column uid\n01-11-2017 10:20:33 CST run_dataimport INFO - 17/11/01 10:20:33 INFO tool.ImportTool: Upper bound value: 31762700\n01-11-2017 10:20:33 CST run_dataimport INFO - 17/11/01 10:20:33 INFO mapreduce.ImportJobBase: Beginning query import.\n01-11-2017 10:20:33 CST run_dataimport INFO - 17/11/01 10:20:33 INFO Configuration.deprecation: mapred.jar is deprecated. Instead, use mapreduce.job.jar\n01-11-2017 10:20:33 CST run_dataimport INFO - 17/11/01 10:20:33 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n01-11-2017 10:20:33 CST run_dataimport INFO - 17/11/01 10:20:33 INFO client.RMProxy: Connecting to ResourceManager at master/172.23.27.203:8032\n01-11-2017 10:20:40 CST run_dataimport INFO - 17/11/01 10:20:40 INFO db.DBInputFormat: Using read commited transaction isolation\n01-11-2017 10:20:40 CST run_dataimport INFO - 17/11/01 10:20:40 INFO db.DataDrivenDBInputFormat: BoundingValsQuery: SELECT MIN(uid), MAX(uid) FROM (select top 50000 * from jzj.dbo.jzj_test where uid&gt;(select uid from (select top 100000 uid from jzj.dbo.jzj_test where uid&gt;31612700 and uid&lt;=32309400) a except select uid from (select top 99999 uid from jzj.dbo.jzj_test where uid&gt;31612700 and uid&lt;=32309400) a) and uid &lt;=32309400 and uid &lt;= 31762700 AND  (1 = 1) ) AS t1\n01-11-2017 10:30:36 CST run_dataimport INFO - 17/11/01 10:30:36 INFO mapreduce.JobSubmitter: number of splits:4\n01-11-2017 10:30:37 CST run_dataimport INFO - 17/11/01 10:30:37 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1508671365448_1899\n01-11-2017 10:30:37 CST run_dataimport INFO - 17/11/01 10:30:37 INFO impl.YarnClientImpl: Submitted application application_1508671365448_1899\n01-11-2017 10:30:37 CST run_dataimport INFO - 17/11/01 10:30:37 INFO mapreduce.Job: The url to track the job: http://master:8088/proxy/application_1508671365448_1899/\n01-11-2017 10:30:37 CST run_dataimport INFO - 17/11/01 10:30:37 INFO mapreduce.Job: Running job: job_1508671365448_1899\n01-11-2017 10:30:44 CST run_dataimport INFO - 17/11/01 10:30:44 INFO mapreduce.Job: Job job_1508671365448_1899 running in uber mode : false\n01-11-2017 10:30:44 CST run_dataimport INFO - 17/11/01 10:30:44 INFO mapreduce.Job:  map 0% reduce 0%\n01-11-2017 10:40:52 CST run_dataimport INFO - 17/11/01 10:40:52 INFO mapreduce.Job:  map 25% reduce 0%\n01-11-2017 10:40:53 CST run_dataimport INFO - 17/11/01 10:40:53 INFO mapreduce.Job:  map 50% reduce 0%\n01-11-2017 10:41:00 CST run_dataimport INFO - 17/11/01 10:41:00 INFO mapreduce.Job:  map 75% reduce 0%\n01-11-2017 10:41:02 CST run_dataimport INFO - 17/11/01 10:41:02 INFO mapreduce.Job:  map 100% reduce 0%\n01-11-2017 10:41:03 CST run_dataimport INFO - 17/11/01 10:41:03 INFO mapreduce.Job: Job job_1508671365448_1899 completed successfully\n01-11-2017 10:41:03 CST run_dataimport INFO - 17/11/01 10:41:03 INFO mapreduce.Job: Counters: 30\n01-11-2017 10:41:03 CST run_dataimport INFO - \tFile System Counters\n01-11-2017 10:41:03 CST run_dataimport INFO - \t\tFILE: Number of bytes read=0\n01-11-2017 10:41:03 CST run_dataimport INFO - \t\tFILE: Number of bytes written=588428\n01-11-2017 10:41:03 CST run_dataimport INFO - \t\tFILE: Number of read operations=0\n01-11-2017 10:41:03 CST run_dataimport INFO - \t\tFILE: Number of large read operations=0\n01-11-2017 10:41:03 CST run_dataimport INFO - \t\tFILE: Number of write operations=0\n01-11-2017 10:41:03 CST run_dataimport INFO - \t\tHDFS: Number of bytes read=462\n01-11-2017 10:41:03 CST run_dataimport INFO - \t\tHDFS: Number of bytes written=23450000\n01-11-2017 10:41:03 CST run_dataimport INFO - \t\tHDFS: Number of read operations=16\n01-11-2017 10:41:03 CST run_dataimport INFO - \t\tHDFS: Number of large read operations=0\n01-11-2017 10:41:03 CST run_dataimport INFO - \t\tHDFS: Number of write operations=8\n01-11-2017 10:41:03 CST run_dataimport INFO - \tJob Counters \n01-11-2017 10:41:03 CST run_dataimport INFO - \t\tLaunched map tasks=4\n01-11-2017 10:41:03 CST run_dataimport INFO - \t\tOther local map tasks=4\n01-11-2017 10:41:03 CST run_dataimport INFO - \t\tTotal time spent by all maps in occupied slots (ms)=2440026\n01-11-2017 10:41:03 CST run_dataimport INFO - \t\tTotal time spent by all reduces in occupied slots (ms)=0\n01-11-2017 10:41:03 CST run_dataimport INFO - \t\tTotal time spent by all map tasks (ms)=2440026\n01-11-2017 10:41:03 CST run_dataimport INFO - \t\tTotal vcore-seconds taken by all map tasks=2440026\n01-11-2017 10:41:03 CST run_dataimport INFO - \t\tTotal megabyte-seconds taken by all map tasks=2498586624\n01-11-2017 10:41:03 CST run_dataimport INFO - \tMap-Reduce Framework\n01-11-2017 10:41:03 CST run_dataimport INFO - \t\tMap input records=50000\n01-11-2017 10:41:03 CST run_dataimport INFO - \t\tMap output records=50000\n01-11-2017 10:41:03 CST run_dataimport INFO - \t\tInput split bytes=462\n01-11-2017 10:41:03 CST run_dataimport INFO - \t\tSpilled Records=0\n01-11-2017 10:41:03 CST run_dataimport INFO - \t\tFailed Shuffles=0\n01-11-2017 10:41:03 CST run_dataimport INFO - \t\tMerged Map outputs=0\n01-11-2017 10:41:03 CST run_dataimport INFO - \t\tGC time elapsed (ms)=327\n01-11-2017 10:41:03 CST run_dataimport INFO - \t\tCPU time spent (ms)=33410\n01-11-2017 10:41:03 CST run_dataimport INFO - \t\tPhysical memory (bytes) snapshot=1476767744\n01-11-2017 10:41:03 CST run_dataimport INFO - \t\tVirtual memory (bytes) snapshot=6458535936\n01-11-2017 10:41:03 CST run_dataimport INFO - \t\tTotal committed heap usage (bytes)=3296722944\n01-11-2017 10:41:03 CST run_dataimport INFO - \tFile Input Format Counters \n01-11-2017 10:41:03 CST run_dataimport INFO - \t\tBytes Read=0\n01-11-2017 10:41:03 CST run_dataimport INFO - \tFile Output Format Counters \n01-11-2017 10:41:03 CST run_dataimport INFO - \t\tBytes Written=23450000\n01-11-2017 10:41:03 CST run_dataimport INFO - 17/11/01 10:41:03 INFO mapreduce.ImportJobBase: Transferred 22.3637 MB in 1,229.8978 seconds (18.6198 KB/sec)\n01-11-2017 10:41:03 CST run_dataimport INFO - 17/11/01 10:41:03 INFO mapreduce.ImportJobBase: Retrieved 50000 records.\n01-11-2017 10:41:03 CST run_dataimport INFO - 17/11/01 10:41:03 INFO util.AppendUtils: Appending to directory sql_to_hive\n01-11-2017 10:41:03 CST run_dataimport INFO - 17/11/01 10:41:03 INFO util.AppendUtils: Using found partition 2668\n01-11-2017 10:41:04 CST run_dataimport INFO - 17/11/01 10:41:04 INFO tool.ImportTool: Incremental import complete! To run another incremental import of all data following this import, supply the following arguments:\n01-11-2017 10:41:04 CST run_dataimport INFO - 17/11/01 10:41:04 INFO tool.ImportTool:  --incremental append\n01-11-2017 10:41:04 CST run_dataimport INFO - 17/11/01 10:41:04 INFO tool.ImportTool:   --check-column uid\n01-11-2017 10:41:04 CST run_dataimport INFO - 17/11/01 10:41:04 INFO tool.ImportTool:   --last-value 31762700\n01-11-2017 10:41:04 CST run_dataimport INFO - 17/11/01 10:41:04 INFO tool.ImportTool: (Consider saving this with 'sqoop job --create')\n01-11-2017 10:41:04 CST run_dataimport INFO - 11\n01-11-2017 10:41:04 CST run_dataimport INFO - Warning: /opt/cloudera/parcels/CDH-5.8.0-1.cdh5.8.0.p0.42/bin/../lib/sqoop/../accumulo does not exist! Accumulo imports will fail.\n01-11-2017 10:41:04 CST run_dataimport INFO - Please set $ACCUMULO_HOME to the root of your Accumulo installation.\n01-11-2017 10:41:06 CST run_dataimport INFO - 17/11/01 10:41:06 INFO sqoop.Sqoop: Running Sqoop version: 1.4.6-cdh5.8.0\n01-11-2017 10:41:06 CST run_dataimport INFO - 17/11/01 10:41:06 WARN sqoop.ConnFactory: Parameter --driver is set to an explicit driver however appropriate connection manager is not being set (via --connection-manager). Sqoop is going to fall back to org.apache.sqoop.manager.GenericJdbcManager. Please specify explicitly which connection manager should be used next time.\n01-11-2017 10:41:06 CST run_dataimport INFO - 17/11/01 10:41:06 INFO manager.SqlManager: Using default fetchSize of 1000\n01-11-2017 10:41:06 CST run_dataimport INFO - 17/11/01 10:41:06 INFO tool.CodeGenTool: Beginning code generation\n01-11-2017 10:41:07 CST run_dataimport INFO - 17/11/01 10:41:07 INFO manager.SqlManager: Executing SQL statement: select top 50000 * from jzj.dbo.jzj_test where uid&gt;(select uid from (select top 150000 uid from jzj.dbo.jzj_test where uid&gt;31612700 and uid&lt;=32309400) a except select uid from (select top 149999 uid from jzj.dbo.jzj_test where uid&gt;31612700 and uid&lt;=32309400) a) and uid &lt;=32309400 and  (1 = 0) \n01-11-2017 10:41:07 CST run_dataimport INFO - 17/11/01 10:41:07 INFO manager.SqlManager: Executing SQL statement: select top 50000 * from jzj.dbo.jzj_test where uid&gt;(select uid from (select top 150000 uid from jzj.dbo.jzj_test where uid&gt;31612700 and uid&lt;=32309400) a except select uid from (select top 149999 uid from jzj.dbo.jzj_test where uid&gt;31612700 and uid&lt;=32309400) a) and uid &lt;=32309400 and  (1 = 0) \n01-11-2017 10:41:07 CST run_dataimport INFO - 17/11/01 10:41:07 INFO orm.CompilationManager: HADOOP_MAPRED_HOME is /opt/cloudera/parcels/CDH/lib/hadoop-mapreduce\n01-11-2017 10:41:09 CST run_dataimport INFO - Note: /tmp/sqoop-root/compile/61fb0da5f39ceec0b287b16e9e75fd1b/QueryResult.java uses or overrides a deprecated API.\n01-11-2017 10:41:09 CST run_dataimport INFO - Note: Recompile with -Xlint:deprecation for details.\n01-11-2017 10:41:09 CST run_dataimport INFO - 17/11/01 10:41:09 INFO orm.CompilationManager: Writing jar file: /tmp/sqoop-root/compile/61fb0da5f39ceec0b287b16e9e75fd1b/QueryResult.jar\n01-11-2017 10:41:10 CST run_dataimport INFO - 17/11/01 10:41:10 INFO tool.ImportTool: Maximal id query for free form incremental import: SELECT MAX(uid) FROM (select top 50000 * from jzj.dbo.jzj_test where uid&gt;(select uid from (select top 150000 uid from jzj.dbo.jzj_test where uid&gt;31612700 and uid&lt;=32309400) a except select uid from (select top 149999 uid from jzj.dbo.jzj_test where uid&gt;31612700 and uid&lt;=32309400) a) and uid &lt;=32309400 and (1 = 1)) sqoop_import_query_alias\n", 'length': 34468}
[2017-11-01 10:54:10,080] app.hive_mysql.AzkabanMonitor [INFO] URL: https://172.23.27.203:8443
[2017-11-01 10:54:10,251] app.hive_mysql.AzkabanMonitor [INFO] Success: Login https://172.23.27.203:8443 with user azkaban
[2017-11-01 10:54:10,251] app.hive_mysql.AzkabanMonitor [DEBUG] {'session.id': '5d81f097-37ef-499c-b87f-a295103b043f', 'status': 'success'}
[2017-11-01 10:54:10,273] app.hive_mysql.AzkabanMonitor [INFO] Success: Fetch Executions of a Flow - run_dataimport
[2017-11-01 10:54:10,273] app.hive_mysql.AzkabanMonitor [DEBUG] {'total': 6, 'executions': [{'submitTime': 1509501650049, 'submitUser': 'azkaban', 'startTime': 1509501650215, 'endTime': -1, 'flowId': 'run_dataimport', 'projectId': 68, 'execId': 1658, 'status': 'RUNNING'}, {'submitTime': 1509494450033, 'submitUser': 'azkaban', 'startTime': 1509494450241, 'endTime': 1509496877667, 'flowId': 'run_dataimport', 'projectId': 68, 'execId': 1657, 'status': 'SUCCEEDED'}, {'submitTime': 1509458449965, 'submitUser': 'azkaban', 'startTime': 1509458450181, 'endTime': 1509489787442, 'flowId': 'run_dataimport', 'projectId': 68, 'execId': 1656, 'status': 'SUCCEEDED'}, {'submitTime': 1509444049936, 'submitUser': 'azkaban', 'startTime': 1509444050155, 'endTime': 1509457451629, 'flowId': 'run_dataimport', 'projectId': 68, 'execId': 1655, 'status': 'SUCCEEDED'}, {'submitTime': 1509438862311, 'submitUser': 'azkaban', 'startTime': 1509438862468, 'endTime': 1509438932606, 'flowId': 'run_dataimport', 'projectId': 68, 'execId': 1654, 'status': 'KILLED'}, {'submitTime': 1509438455131, 'submitUser': 'azkaban', 'startTime': 1509438455403, 'endTime': 1509438536090, 'flowId': 'run_dataimport', 'projectId': 68, 'execId': 1653, 'status': 'KILLED'}], 'length': 10, 'project': 'sql_to_hive', 'from': 0, 'projectId': 68, 'flow': 'run_dataimport'}
[2017-11-01 10:54:10,651] app.hive_mysql.AzkabanMonitor [INFO] Success: Fetch Execution Job Logs - 1658-run_dataimport
[2017-11-01 10:54:10,652] app.hive_mysql.AzkabanMonitor [DEBUG] {'offset': 0, 'data': "01-11-2017 10:00:50 CST run_dataimport INFO - Starting job run_dataimport at 1509501650218\n01-11-2017 10:00:50 CST run_dataimport INFO - azkaban.webserver.url property was not set\n01-11-2017 10:00:50 CST run_dataimport INFO - job JVM args: -Dazkaban.flowid=run_dataimport -Dazkaban.execid=1658 -Dazkaban.jobid=run_dataimport\n01-11-2017 10:00:50 CST run_dataimport INFO - Building command job executor. \n01-11-2017 10:00:50 CST run_dataimport INFO - Memory granted for job run_dataimport\n01-11-2017 10:00:50 CST run_dataimport INFO - 1 commands to execute.\n01-11-2017 10:00:50 CST run_dataimport INFO - cwd=/home/azkaban/azkaban_bk/azkaban-exec-server-3.35.0-22-g5517d50/executions/1658\n01-11-2017 10:00:50 CST run_dataimport INFO - effective user is: azkaban\n01-11-2017 10:00:50 CST run_dataimport INFO - Command: sh /home/sql_to_hive/dataimport.sh\n01-11-2017 10:00:50 CST run_dataimport INFO - Environment variables: {JOB_OUTPUT_PROP_FILE=/home/azkaban/azkaban_bk/azkaban-exec-server-3.35.0-22-g5517d50/executions/1658/run_dataimport_output_2861162274243800284_tmp, JOB_PROP_FILE=/home/azkaban/azkaban_bk/azkaban-exec-server-3.35.0-22-g5517d50/executions/1658/run_dataimport_props_4685474620571842989_tmp, KRB5CCNAME=/tmp/krb5cc__sql_to_hive__run_dataimport__run_dataimport__1658__azkaban, JOB_NAME=run_dataimport}\n01-11-2017 10:00:50 CST run_dataimport INFO - Working directory: /home/azkaban/azkaban_bk/azkaban-exec-server-3.35.0-22-g5517d50/executions/1658\n01-11-2017 10:00:50 CST run_dataimport INFO - log4j:WARN No appenders could be found for logger (org.apache.hive.jdbc.Utils).\n01-11-2017 10:00:50 CST run_dataimport INFO - log4j:WARN Please initialize the log4j system properly.\n01-11-2017 10:00:50 CST run_dataimport INFO - log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.\n01-11-2017 10:00:50 CST run_dataimport INFO - SLF4J: Failed to load class &quot;org.slf4j.impl.StaticLoggerBinder&quot;.\n01-11-2017 10:00:50 CST run_dataimport INFO - SLF4J: Defaulting to no-operation (NOP) logger implementation\n01-11-2017 10:00:50 CST run_dataimport INFO - SLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.\n01-11-2017 10:01:33 CST run_dataimport INFO - 1,32309400,31612700,696700sqlmax:32309400\n01-11-2017 10:01:33 CST run_dataimport INFO - hivemax:31612700\n01-11-2017 10:01:33 CST run_dataimport INFO - counts:696700\n01-11-2017 10:01:33 CST run_dataimport INFO - 14,46700\n01-11-2017 10:01:33 CST run_dataimport INFO - 14\n01-11-2017 10:01:33 CST run_dataimport INFO - Warning: /opt/cloudera/parcels/CDH-5.8.0-1.cdh5.8.0.p0.42/bin/../lib/sqoop/../accumulo does not exist! Accumulo imports will fail.\n01-11-2017 10:01:33 CST run_dataimport INFO - Please set $ACCUMULO_HOME to the root of your Accumulo installation.\n01-11-2017 10:01:35 CST run_dataimport INFO - 17/11/01 10:01:35 INFO sqoop.Sqoop: Running Sqoop version: 1.4.6-cdh5.8.0\n01-11-2017 10:01:35 CST run_dataimport INFO - 17/11/01 10:01:35 WARN sqoop.ConnFactory: Parameter --driver is set to an explicit driver however appropriate connection manager is not being set (via --connection-manager). Sqoop is going to fall back to org.apache.sqoop.manager.GenericJdbcManager. Please specify explicitly which connection manager should be used next time.\n01-11-2017 10:01:35 CST run_dataimport INFO - 17/11/01 10:01:35 INFO manager.SqlManager: Using default fetchSize of 1000\n01-11-2017 10:01:35 CST run_dataimport INFO - 17/11/01 10:01:35 INFO tool.CodeGenTool: Beginning code generation\n01-11-2017 10:01:36 CST run_dataimport INFO - 17/11/01 10:01:36 INFO manager.SqlManager: Executing SQL statement: select top 50000 * from jzj.dbo.jzj_test where uid&gt;=(select uid from (select top 1 uid from jzj.dbo.jzj_test where uid&gt;31612700 and uid&lt;=32309400) a) and uid &lt;=32309400 and  (1 = 0) \n01-11-2017 10:01:36 CST run_dataimport INFO - 17/11/01 10:01:36 INFO manager.SqlManager: Executing SQL statement: select top 50000 * from jzj.dbo.jzj_test where uid&gt;=(select uid from (select top 1 uid from jzj.dbo.jzj_test where uid&gt;31612700 and uid&lt;=32309400) a) and uid &lt;=32309400 and  (1 = 0) \n01-11-2017 10:01:36 CST run_dataimport INFO - 17/11/01 10:01:36 INFO orm.CompilationManager: HADOOP_MAPRED_HOME is /opt/cloudera/parcels/CDH/lib/hadoop-mapreduce\n01-11-2017 10:01:38 CST run_dataimport INFO - Note: /tmp/sqoop-root/compile/ff02930c93108caafd8dbc9f2c064c43/QueryResult.java uses or overrides a deprecated API.\n01-11-2017 10:01:38 CST run_dataimport INFO - Note: Recompile with -Xlint:deprecation for details.\n01-11-2017 10:01:38 CST run_dataimport INFO - 17/11/01 10:01:38 INFO orm.CompilationManager: Writing jar file: /tmp/sqoop-root/compile/ff02930c93108caafd8dbc9f2c064c43/QueryResult.jar\n01-11-2017 10:01:39 CST run_dataimport INFO - 17/11/01 10:01:39 INFO tool.ImportTool: Maximal id query for free form incremental import: SELECT MAX(uid) FROM (select top 50000 * from jzj.dbo.jzj_test where uid&gt;=(select uid from (select top 1 uid from jzj.dbo.jzj_test where uid&gt;31612700 and uid&lt;=32309400) a) and uid &lt;=32309400 and (1 = 1)) sqoop_import_query_alias\n01-11-2017 10:01:39 CST run_dataimport INFO - 17/11/01 10:01:39 INFO tool.ImportTool: Incremental import based on column uid\n01-11-2017 10:01:39 CST run_dataimport INFO - 17/11/01 10:01:39 INFO tool.ImportTool: Upper bound value: 31662700\n01-11-2017 10:01:39 CST run_dataimport INFO - 17/11/01 10:01:39 INFO mapreduce.ImportJobBase: Beginning query import.\n01-11-2017 10:01:39 CST run_dataimport INFO - 17/11/01 10:01:39 INFO Configuration.deprecation: mapred.jar is deprecated. Instead, use mapreduce.job.jar\n01-11-2017 10:01:39 CST run_dataimport INFO - 17/11/01 10:01:39 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n01-11-2017 10:01:39 CST run_dataimport INFO - 17/11/01 10:01:39 INFO client.RMProxy: Connecting to ResourceManager at master/172.23.27.203:8032\n01-11-2017 10:01:45 CST run_dataimport INFO - 17/11/01 10:01:45 INFO db.DBInputFormat: Using read commited transaction isolation\n01-11-2017 10:01:45 CST run_dataimport INFO - 17/11/01 10:01:45 INFO db.DataDrivenDBInputFormat: BoundingValsQuery: SELECT MIN(uid), MAX(uid) FROM (select top 50000 * from jzj.dbo.jzj_test where uid&gt;=(select uid from (select top 1 uid from jzj.dbo.jzj_test where uid&gt;31612700 and uid&lt;=32309400) a) and uid &lt;=32309400 and uid &lt;= 31662700 AND  (1 = 1) ) AS t1\n01-11-2017 10:01:46 CST run_dataimport INFO - 17/11/01 10:01:46 INFO mapreduce.JobSubmitter: number of splits:4\n01-11-2017 10:01:46 CST run_dataimport INFO - 17/11/01 10:01:46 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1508671365448_1897\n01-11-2017 10:01:47 CST run_dataimport INFO - 17/11/01 10:01:47 INFO impl.YarnClientImpl: Submitted application application_1508671365448_1897\n01-11-2017 10:01:47 CST run_dataimport INFO - 17/11/01 10:01:47 INFO mapreduce.Job: The url to track the job: http://master:8088/proxy/application_1508671365448_1897/\n01-11-2017 10:01:47 CST run_dataimport INFO - 17/11/01 10:01:47 INFO mapreduce.Job: Running job: job_1508671365448_1897\n01-11-2017 10:01:54 CST run_dataimport INFO - 17/11/01 10:01:54 INFO mapreduce.Job: Job job_1508671365448_1897 running in uber mode : false\n01-11-2017 10:01:54 CST run_dataimport INFO - 17/11/01 10:01:54 INFO mapreduce.Job:  map 0% reduce 0%\n01-11-2017 10:02:02 CST run_dataimport INFO - 17/11/01 10:02:02 INFO mapreduce.Job:  map 25% reduce 0%\n01-11-2017 10:02:03 CST run_dataimport INFO - 17/11/01 10:02:03 INFO mapreduce.Job:  map 100% reduce 0%\n01-11-2017 10:02:05 CST run_dataimport INFO - 17/11/01 10:02:05 INFO mapreduce.Job: Job job_1508671365448_1897 completed successfully\n01-11-2017 10:02:05 CST run_dataimport INFO - 17/11/01 10:02:05 INFO mapreduce.Job: Counters: 30\n01-11-2017 10:02:05 CST run_dataimport INFO - \tFile System Counters\n01-11-2017 10:02:05 CST run_dataimport INFO - \t\tFILE: Number of bytes read=0\n01-11-2017 10:02:05 CST run_dataimport INFO - \t\tFILE: Number of bytes written=587012\n01-11-2017 10:02:05 CST run_dataimport INFO - \t\tFILE: Number of read operations=0\n01-11-2017 10:02:05 CST run_dataimport INFO - \t\tFILE: Number of large read operations=0\n01-11-2017 10:02:05 CST run_dataimport INFO - \t\tFILE: Number of write operations=0\n01-11-2017 10:02:05 CST run_dataimport INFO - \t\tHDFS: Number of bytes read=462\n01-11-2017 10:02:05 CST run_dataimport INFO - \t\tHDFS: Number of bytes written=23450000\n01-11-2017 10:02:05 CST run_dataimport INFO - \t\tHDFS: Number of read operations=16\n01-11-2017 10:02:05 CST run_dataimport INFO - \t\tHDFS: Number of large read operations=0\n01-11-2017 10:02:05 CST run_dataimport INFO - \t\tHDFS: Number of write operations=8\n01-11-2017 10:02:05 CST run_dataimport INFO - \tJob Counters \n01-11-2017 10:02:05 CST run_dataimport INFO - \t\tLaunched map tasks=4\n01-11-2017 10:02:05 CST run_dataimport INFO - \t\tOther local map tasks=4\n01-11-2017 10:02:05 CST run_dataimport INFO - \t\tTotal time spent by all maps in occupied slots (ms)=26571\n01-11-2017 10:02:05 CST run_dataimport INFO - \t\tTotal time spent by all reduces in occupied slots (ms)=0\n01-11-2017 10:02:05 CST run_dataimport INFO - \t\tTotal time spent by all map tasks (ms)=26571\n01-11-2017 10:02:05 CST run_dataimport INFO - \t\tTotal vcore-seconds taken by all map tasks=26571\n01-11-2017 10:02:05 CST run_dataimport INFO - \t\tTotal megabyte-seconds taken by all map tasks=27208704\n01-11-2017 10:02:05 CST run_dataimport INFO - \tMap-Reduce Framework\n01-11-2017 10:02:05 CST run_dataimport INFO - \t\tMap input records=50000\n01-11-2017 10:02:05 CST run_dataimport INFO - \t\tMap output records=50000\n01-11-2017 10:02:05 CST run_dataimport INFO - \t\tInput split bytes=462\n01-11-2017 10:02:05 CST run_dataimport INFO - \t\tSpilled Records=0\n01-11-2017 10:02:05 CST run_dataimport INFO - \t\tFailed Shuffles=0\n01-11-2017 10:02:05 CST run_dataimport INFO - \t\tMerged Map outputs=0\n01-11-2017 10:02:05 CST run_dataimport INFO - \t\tGC time elapsed (ms)=211\n01-11-2017 10:02:05 CST run_dataimport INFO - \t\tCPU time spent (ms)=24550\n01-11-2017 10:02:05 CST run_dataimport INFO - \t\tPhysical memory (bytes) snapshot=1433231360\n01-11-2017 10:02:05 CST run_dataimport INFO - \t\tVirtual memory (bytes) snapshot=6511280128\n01-11-2017 10:02:05 CST run_dataimport INFO - \t\tTotal committed heap usage (bytes)=3296722944\n01-11-2017 10:02:05 CST run_dataimport INFO - \tFile Input Format Counters \n01-11-2017 10:02:05 CST run_dataimport INFO - \t\tBytes Read=0\n01-11-2017 10:02:05 CST run_dataimport INFO - \tFile Output Format Counters \n01-11-2017 10:02:05 CST run_dataimport INFO - \t\tBytes Written=23450000\n01-11-2017 10:02:05 CST run_dataimport INFO - 17/11/01 10:02:05 INFO mapreduce.ImportJobBase: Transferred 22.3637 MB in 26.2622 seconds (871.9906 KB/sec)\n01-11-2017 10:02:05 CST run_dataimport INFO - 17/11/01 10:02:05 INFO mapreduce.ImportJobBase: Retrieved 50000 records.\n01-11-2017 10:02:05 CST run_dataimport INFO - 17/11/01 10:02:05 INFO util.AppendUtils: Appending to directory sql_to_hive\n01-11-2017 10:02:06 CST run_dataimport INFO - 17/11/01 10:02:06 INFO util.AppendUtils: Using found partition 2660\n01-11-2017 10:02:06 CST run_dataimport INFO - 17/11/01 10:02:06 INFO tool.ImportTool: Incremental import complete! To run another incremental import of all data following this import, supply the following arguments:\n01-11-2017 10:02:06 CST run_dataimport INFO - 17/11/01 10:02:06 INFO tool.ImportTool:  --incremental append\n01-11-2017 10:02:06 CST run_dataimport INFO - 17/11/01 10:02:06 INFO tool.ImportTool:   --check-column uid\n01-11-2017 10:02:06 CST run_dataimport INFO - 17/11/01 10:02:06 INFO tool.ImportTool:   --last-value 31662700\n01-11-2017 10:02:06 CST run_dataimport INFO - 17/11/01 10:02:06 INFO tool.ImportTool: (Consider saving this with 'sqoop job --create')\n01-11-2017 10:02:06 CST run_dataimport INFO - 13\n01-11-2017 10:02:06 CST run_dataimport INFO - Warning: /opt/cloudera/parcels/CDH-5.8.0-1.cdh5.8.0.p0.42/bin/../lib/sqoop/../accumulo does not exist! Accumulo imports will fail.\n01-11-2017 10:02:06 CST run_dataimport INFO - Please set $ACCUMULO_HOME to the root of your Accumulo installation.\n01-11-2017 10:02:08 CST run_dataimport INFO - 17/11/01 10:02:08 INFO sqoop.Sqoop: Running Sqoop version: 1.4.6-cdh5.8.0\n01-11-2017 10:02:08 CST run_dataimport INFO - 17/11/01 10:02:08 WARN sqoop.ConnFactory: Parameter --driver is set to an explicit driver however appropriate connection manager is not being set (via --connection-manager). Sqoop is going to fall back to org.apache.sqoop.manager.GenericJdbcManager. Please specify explicitly which connection manager should be used next time.\n01-11-2017 10:02:08 CST run_dataimport INFO - 17/11/01 10:02:08 INFO manager.SqlManager: Using default fetchSize of 1000\n01-11-2017 10:02:08 CST run_dataimport INFO - 17/11/01 10:02:08 INFO tool.CodeGenTool: Beginning code generation\n01-11-2017 10:02:09 CST run_dataimport INFO - 17/11/01 10:02:09 INFO manager.SqlManager: Executing SQL statement: select top 50000 * from jzj.dbo.jzj_test where uid&gt;(select uid from (select top 50000 uid from jzj.dbo.jzj_test where uid&gt;31612700 and uid&lt;=32309400) a except select uid from (select top 49999 uid from jzj.dbo.jzj_test where uid&gt;31612700 and uid&lt;=32309400) a) and uid &lt;=32309400 and  (1 = 0) \n01-11-2017 10:02:09 CST run_dataimport INFO - 17/11/01 10:02:09 INFO manager.SqlManager: Executing SQL statement: select top 50000 * from jzj.dbo.jzj_test where uid&gt;(select uid from (select top 50000 uid from jzj.dbo.jzj_test where uid&gt;31612700 and uid&lt;=32309400) a except select uid from (select top 49999 uid from jzj.dbo.jzj_test where uid&gt;31612700 and uid&lt;=32309400) a) and uid &lt;=32309400 and  (1 = 0) \n01-11-2017 10:02:09 CST run_dataimport INFO - 17/11/01 10:02:09 INFO orm.CompilationManager: HADOOP_MAPRED_HOME is /opt/cloudera/parcels/CDH/lib/hadoop-mapreduce\n01-11-2017 10:02:11 CST run_dataimport INFO - Note: /tmp/sqoop-root/compile/9de72518c89c5280b88b044ac6fd6c80/QueryResult.java uses or overrides a deprecated API.\n01-11-2017 10:02:11 CST run_dataimport INFO - Note: Recompile with -Xlint:deprecation for details.\n01-11-2017 10:02:11 CST run_dataimport INFO - 17/11/01 10:02:11 INFO orm.CompilationManager: Writing jar file: /tmp/sqoop-root/compile/9de72518c89c5280b88b044ac6fd6c80/QueryResult.jar\n01-11-2017 10:02:12 CST run_dataimport INFO - 17/11/01 10:02:12 INFO tool.ImportTool: Maximal id query for free form incremental import: SELECT MAX(uid) FROM (select top 50000 * from jzj.dbo.jzj_test where uid&gt;(select uid from (select top 50000 uid from jzj.dbo.jzj_test where uid&gt;31612700 and uid&lt;=32309400) a except select uid from (select top 49999 uid from jzj.dbo.jzj_test where uid&gt;31612700 and uid&lt;=32309400) a) and uid &lt;=32309400 and (1 = 1)) sqoop_import_query_alias\n01-11-2017 10:04:47 CST run_dataimport INFO - 17/11/01 10:04:47 INFO tool.ImportTool: Incremental import based on column uid\n01-11-2017 10:04:47 CST run_dataimport INFO - 17/11/01 10:04:47 INFO tool.ImportTool: Upper bound value: 31712700\n01-11-2017 10:04:47 CST run_dataimport INFO - 17/11/01 10:04:47 INFO mapreduce.ImportJobBase: Beginning query import.\n01-11-2017 10:04:47 CST run_dataimport INFO - 17/11/01 10:04:47 INFO Configuration.deprecation: mapred.jar is deprecated. Instead, use mapreduce.job.jar\n01-11-2017 10:04:47 CST run_dataimport INFO - 17/11/01 10:04:47 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n01-11-2017 10:04:47 CST run_dataimport INFO - 17/11/01 10:04:47 INFO client.RMProxy: Connecting to ResourceManager at master/172.23.27.203:8032\n01-11-2017 10:04:52 CST run_dataimport INFO - 17/11/01 10:04:52 INFO db.DBInputFormat: Using read commited transaction isolation\n01-11-2017 10:04:52 CST run_dataimport INFO - 17/11/01 10:04:52 INFO db.DataDrivenDBInputFormat: BoundingValsQuery: SELECT MIN(uid), MAX(uid) FROM (select top 50000 * from jzj.dbo.jzj_test where uid&gt;(select uid from (select top 50000 uid from jzj.dbo.jzj_test where uid&gt;31612700 and uid&lt;=32309400) a except select uid from (select top 49999 uid from jzj.dbo.jzj_test where uid&gt;31612700 and uid&lt;=32309400) a) and uid &lt;=32309400 and uid &lt;= 31712700 AND  (1 = 1) ) AS t1\n01-11-2017 10:07:26 CST run_dataimport INFO - 17/11/01 10:07:26 INFO mapreduce.JobSubmitter: number of splits:4\n01-11-2017 10:07:27 CST run_dataimport INFO - 17/11/01 10:07:27 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1508671365448_1898\n01-11-2017 10:07:27 CST run_dataimport INFO - 17/11/01 10:07:27 INFO impl.YarnClientImpl: Submitted application application_1508671365448_1898\n01-11-2017 10:07:27 CST run_dataimport INFO - 17/11/01 10:07:27 INFO mapreduce.Job: The url to track the job: http://master:8088/proxy/application_1508671365448_1898/\n01-11-2017 10:07:27 CST run_dataimport INFO - 17/11/01 10:07:27 INFO mapreduce.Job: Running job: job_1508671365448_1898\n01-11-2017 10:07:34 CST run_dataimport INFO - 17/11/01 10:07:34 INFO mapreduce.Job: Job job_1508671365448_1898 running in uber mode : false\n01-11-2017 10:07:34 CST run_dataimport INFO - 17/11/01 10:07:34 INFO mapreduce.Job:  map 0% reduce 0%\n01-11-2017 10:10:17 CST run_dataimport INFO - 17/11/01 10:10:17 INFO mapreduce.Job:  map 25% reduce 0%\n01-11-2017 10:10:19 CST run_dataimport INFO - 17/11/01 10:10:19 INFO mapreduce.Job:  map 75% reduce 0%\n01-11-2017 10:10:20 CST run_dataimport INFO - 17/11/01 10:10:20 INFO mapreduce.Job:  map 100% reduce 0%\n01-11-2017 10:10:21 CST run_dataimport INFO - 17/11/01 10:10:21 INFO mapreduce.Job: Job job_1508671365448_1898 completed successfully\n01-11-2017 10:10:21 CST run_dataimport INFO - 17/11/01 10:10:21 INFO mapreduce.Job: Counters: 30\n01-11-2017 10:10:21 CST run_dataimport INFO - \tFile System Counters\n01-11-2017 10:10:21 CST run_dataimport INFO - \t\tFILE: Number of bytes read=0\n01-11-2017 10:10:21 CST run_dataimport INFO - \t\tFILE: Number of bytes written=588412\n01-11-2017 10:10:21 CST run_dataimport INFO - \t\tFILE: Number of read operations=0\n01-11-2017 10:10:21 CST run_dataimport INFO - \t\tFILE: Number of large read operations=0\n01-11-2017 10:10:21 CST run_dataimport INFO - \t\tFILE: Number of write operations=0\n01-11-2017 10:10:21 CST run_dataimport INFO - \t\tHDFS: Number of bytes read=462\n01-11-2017 10:10:21 CST run_dataimport INFO - \t\tHDFS: Number of bytes written=23450000\n01-11-2017 10:10:21 CST run_dataimport INFO - \t\tHDFS: Number of read operations=16\n01-11-2017 10:10:21 CST run_dataimport INFO - \t\tHDFS: Number of large read operations=0\n01-11-2017 10:10:21 CST run_dataimport INFO - \t\tHDFS: Number of write operations=8\n01-11-2017 10:10:21 CST run_dataimport INFO - \tJob Counters \n01-11-2017 10:10:21 CST run_dataimport INFO - \t\tLaunched map tasks=4\n01-11-2017 10:10:21 CST run_dataimport INFO - \t\tOther local map tasks=4\n01-11-2017 10:10:21 CST run_dataimport INFO - \t\tTotal time spent by all maps in occupied slots (ms)=645979\n01-11-2017 10:10:21 CST run_dataimport INFO - \t\tTotal time spent by all reduces in occupied slots (ms)=0\n01-11-2017 10:10:21 CST run_dataimport INFO - \t\tTotal time spent by all map tasks (ms)=645979\n01-11-2017 10:10:21 CST run_dataimport INFO - \t\tTotal vcore-seconds taken by all map tasks=645979\n01-11-2017 10:10:21 CST run_dataimport INFO - \t\tTotal megabyte-seconds taken by all map tasks=661482496\n01-11-2017 10:10:21 CST run_dataimport INFO - \tMap-Reduce Framework\n01-11-2017 10:10:21 CST run_dataimport INFO - \t\tMap input records=50000\n01-11-2017 10:10:21 CST run_dataimport INFO - \t\tMap output records=50000\n01-11-2017 10:10:21 CST run_dataimport INFO - \t\tInput split bytes=462\n01-11-2017 10:10:21 CST run_dataimport INFO - \t\tSpilled Records=0\n01-11-2017 10:10:21 CST run_dataimport INFO - \t\tFailed Shuffles=0\n01-11-2017 10:10:21 CST run_dataimport INFO - \t\tMerged Map outputs=0\n01-11-2017 10:10:21 CST run_dataimport INFO - \t\tGC time elapsed (ms)=299\n01-11-2017 10:10:21 CST run_dataimport INFO - \t\tCPU time spent (ms)=26430\n01-11-2017 10:10:21 CST run_dataimport INFO - \t\tPhysical memory (bytes) snapshot=1455120384\n01-11-2017 10:10:21 CST run_dataimport INFO - \t\tVirtual memory (bytes) snapshot=6527934464\n01-11-2017 10:10:21 CST run_dataimport INFO - \t\tTotal committed heap usage (bytes)=3296722944\n01-11-2017 10:10:21 CST run_dataimport INFO - \tFile Input Format Counters \n01-11-2017 10:10:21 CST run_dataimport INFO - \t\tBytes Read=0\n01-11-2017 10:10:21 CST run_dataimport INFO - \tFile Output Format Counters \n01-11-2017 10:10:21 CST run_dataimport INFO - \t\tBytes Written=23450000\n01-11-2017 10:10:21 CST run_dataimport INFO - 17/11/01 10:10:21 INFO mapreduce.ImportJobBase: Transferred 22.3637 MB in 333.7909 seconds (68.607 KB/sec)\n01-11-2017 10:10:21 CST run_dataimport INFO - 17/11/01 10:10:21 INFO mapreduce.ImportJobBase: Retrieved 50000 records.\n01-11-2017 10:10:21 CST run_dataimport INFO - 17/11/01 10:10:21 INFO util.AppendUtils: Appending to directory sql_to_hive\n01-11-2017 10:10:21 CST run_dataimport INFO - 17/11/01 10:10:21 INFO util.AppendUtils: Using found partition 2664\n01-11-2017 10:10:21 CST run_dataimport INFO - 17/11/01 10:10:21 INFO tool.ImportTool: Incremental import complete! To run another incremental import of all data following this import, supply the following arguments:\n01-11-2017 10:10:21 CST run_dataimport INFO - 17/11/01 10:10:21 INFO tool.ImportTool:  --incremental append\n01-11-2017 10:10:21 CST run_dataimport INFO - 17/11/01 10:10:21 INFO tool.ImportTool:   --check-column uid\n01-11-2017 10:10:21 CST run_dataimport INFO - 17/11/01 10:10:21 INFO tool.ImportTool:   --last-value 31712700\n01-11-2017 10:10:21 CST run_dataimport INFO - 17/11/01 10:10:21 INFO tool.ImportTool: (Consider saving this with 'sqoop job --create')\n01-11-2017 10:10:22 CST run_dataimport INFO - 12\n01-11-2017 10:10:22 CST run_dataimport INFO - Warning: /opt/cloudera/parcels/CDH-5.8.0-1.cdh5.8.0.p0.42/bin/../lib/sqoop/../accumulo does not exist! Accumulo imports will fail.\n01-11-2017 10:10:22 CST run_dataimport INFO - Please set $ACCUMULO_HOME to the root of your Accumulo installation.\n01-11-2017 10:10:24 CST run_dataimport INFO - 17/11/01 10:10:24 INFO sqoop.Sqoop: Running Sqoop version: 1.4.6-cdh5.8.0\n01-11-2017 10:10:24 CST run_dataimport INFO - 17/11/01 10:10:24 WARN sqoop.ConnFactory: Parameter --driver is set to an explicit driver however appropriate connection manager is not being set (via --connection-manager). Sqoop is going to fall back to org.apache.sqoop.manager.GenericJdbcManager. Please specify explicitly which connection manager should be used next time.\n01-11-2017 10:10:24 CST run_dataimport INFO - 17/11/01 10:10:24 INFO manager.SqlManager: Using default fetchSize of 1000\n01-11-2017 10:10:24 CST run_dataimport INFO - 17/11/01 10:10:24 INFO tool.CodeGenTool: Beginning code generation\n01-11-2017 10:10:24 CST run_dataimport INFO - 17/11/01 10:10:24 INFO manager.SqlManager: Executing SQL statement: select top 50000 * from jzj.dbo.jzj_test where uid&gt;(select uid from (select top 100000 uid from jzj.dbo.jzj_test where uid&gt;31612700 and uid&lt;=32309400) a except select uid from (select top 99999 uid from jzj.dbo.jzj_test where uid&gt;31612700 and uid&lt;=32309400) a) and uid &lt;=32309400 and  (1 = 0) \n01-11-2017 10:10:24 CST run_dataimport INFO - 17/11/01 10:10:24 INFO manager.SqlManager: Executing SQL statement: select top 50000 * from jzj.dbo.jzj_test where uid&gt;(select uid from (select top 100000 uid from jzj.dbo.jzj_test where uid&gt;31612700 and uid&lt;=32309400) a except select uid from (select top 99999 uid from jzj.dbo.jzj_test where uid&gt;31612700 and uid&lt;=32309400) a) and uid &lt;=32309400 and  (1 = 0) \n01-11-2017 10:10:25 CST run_dataimport INFO - 17/11/01 10:10:25 INFO orm.CompilationManager: HADOOP_MAPRED_HOME is /opt/cloudera/parcels/CDH/lib/hadoop-mapreduce\n01-11-2017 10:10:27 CST run_dataimport INFO - Note: /tmp/sqoop-root/compile/9f82a203e0f4d154a66e65e3115fe8a9/QueryResult.java uses or overrides a deprecated API.\n01-11-2017 10:10:27 CST run_dataimport INFO - Note: Recompile with -Xlint:deprecation for details.\n01-11-2017 10:10:27 CST run_dataimport INFO - 17/11/01 10:10:27 INFO orm.CompilationManager: Writing jar file: /tmp/sqoop-root/compile/9f82a203e0f4d154a66e65e3115fe8a9/QueryResult.jar\n01-11-2017 10:10:28 CST run_dataimport INFO - 17/11/01 10:10:28 INFO tool.ImportTool: Maximal id query for free form incremental import: SELECT MAX(uid) FROM (select top 50000 * from jzj.dbo.jzj_test where uid&gt;(select uid from (select top 100000 uid from jzj.dbo.jzj_test where uid&gt;31612700 and uid&lt;=32309400) a except select uid from (select top 99999 uid from jzj.dbo.jzj_test where uid&gt;31612700 and uid&lt;=32309400) a) and uid &lt;=32309400 and (1 = 1)) sqoop_import_query_alias\n01-11-2017 10:20:33 CST run_dataimport INFO - 17/11/01 10:20:33 INFO tool.ImportTool: Incremental import based on column uid\n01-11-2017 10:20:33 CST run_dataimport INFO - 17/11/01 10:20:33 INFO tool.ImportTool: Upper bound value: 31762700\n01-11-2017 10:20:33 CST run_dataimport INFO - 17/11/01 10:20:33 INFO mapreduce.ImportJobBase: Beginning query import.\n01-11-2017 10:20:33 CST run_dataimport INFO - 17/11/01 10:20:33 INFO Configuration.deprecation: mapred.jar is deprecated. Instead, use mapreduce.job.jar\n01-11-2017 10:20:33 CST run_dataimport INFO - 17/11/01 10:20:33 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n01-11-2017 10:20:33 CST run_dataimport INFO - 17/11/01 10:20:33 INFO client.RMProxy: Connecting to ResourceManager at master/172.23.27.203:8032\n01-11-2017 10:20:40 CST run_dataimport INFO - 17/11/01 10:20:40 INFO db.DBInputFormat: Using read commited transaction isolation\n01-11-2017 10:20:40 CST run_dataimport INFO - 17/11/01 10:20:40 INFO db.DataDrivenDBInputFormat: BoundingValsQuery: SELECT MIN(uid), MAX(uid) FROM (select top 50000 * from jzj.dbo.jzj_test where uid&gt;(select uid from (select top 100000 uid from jzj.dbo.jzj_test where uid&gt;31612700 and uid&lt;=32309400) a except select uid from (select top 99999 uid from jzj.dbo.jzj_test where uid&gt;31612700 and uid&lt;=32309400) a) and uid &lt;=32309400 and uid &lt;= 31762700 AND  (1 = 1) ) AS t1\n01-11-2017 10:30:36 CST run_dataimport INFO - 17/11/01 10:30:36 INFO mapreduce.JobSubmitter: number of splits:4\n01-11-2017 10:30:37 CST run_dataimport INFO - 17/11/01 10:30:37 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1508671365448_1899\n01-11-2017 10:30:37 CST run_dataimport INFO - 17/11/01 10:30:37 INFO impl.YarnClientImpl: Submitted application application_1508671365448_1899\n01-11-2017 10:30:37 CST run_dataimport INFO - 17/11/01 10:30:37 INFO mapreduce.Job: The url to track the job: http://master:8088/proxy/application_1508671365448_1899/\n01-11-2017 10:30:37 CST run_dataimport INFO - 17/11/01 10:30:37 INFO mapreduce.Job: Running job: job_1508671365448_1899\n01-11-2017 10:30:44 CST run_dataimport INFO - 17/11/01 10:30:44 INFO mapreduce.Job: Job job_1508671365448_1899 running in uber mode : false\n01-11-2017 10:30:44 CST run_dataimport INFO - 17/11/01 10:30:44 INFO mapreduce.Job:  map 0% reduce 0%\n01-11-2017 10:40:52 CST run_dataimport INFO - 17/11/01 10:40:52 INFO mapreduce.Job:  map 25% reduce 0%\n01-11-2017 10:40:53 CST run_dataimport INFO - 17/11/01 10:40:53 INFO mapreduce.Job:  map 50% reduce 0%\n01-11-2017 10:41:00 CST run_dataimport INFO - 17/11/01 10:41:00 INFO mapreduce.Job:  map 75% reduce 0%\n01-11-2017 10:41:02 CST run_dataimport INFO - 17/11/01 10:41:02 INFO mapreduce.Job:  map 100% reduce 0%\n01-11-2017 10:41:03 CST run_dataimport INFO - 17/11/01 10:41:03 INFO mapreduce.Job: Job job_1508671365448_1899 completed successfully\n01-11-2017 10:41:03 CST run_dataimport INFO - 17/11/01 10:41:03 INFO mapreduce.Job: Counters: 30\n01-11-2017 10:41:03 CST run_dataimport INFO - \tFile System Counters\n01-11-2017 10:41:03 CST run_dataimport INFO - \t\tFILE: Number of bytes read=0\n01-11-2017 10:41:03 CST run_dataimport INFO - \t\tFILE: Number of bytes written=588428\n01-11-2017 10:41:03 CST run_dataimport INFO - \t\tFILE: Number of read operations=0\n01-11-2017 10:41:03 CST run_dataimport INFO - \t\tFILE: Number of large read operations=0\n01-11-2017 10:41:03 CST run_dataimport INFO - \t\tFILE: Number of write operations=0\n01-11-2017 10:41:03 CST run_dataimport INFO - \t\tHDFS: Number of bytes read=462\n01-11-2017 10:41:03 CST run_dataimport INFO - \t\tHDFS: Number of bytes written=23450000\n01-11-2017 10:41:03 CST run_dataimport INFO - \t\tHDFS: Number of read operations=16\n01-11-2017 10:41:03 CST run_dataimport INFO - \t\tHDFS: Number of large read operations=0\n01-11-2017 10:41:03 CST run_dataimport INFO - \t\tHDFS: Number of write operations=8\n01-11-2017 10:41:03 CST run_dataimport INFO - \tJob Counters \n01-11-2017 10:41:03 CST run_dataimport INFO - \t\tLaunched map tasks=4\n01-11-2017 10:41:03 CST run_dataimport INFO - \t\tOther local map tasks=4\n01-11-2017 10:41:03 CST run_dataimport INFO - \t\tTotal time spent by all maps in occupied slots (ms)=2440026\n01-11-2017 10:41:03 CST run_dataimport INFO - \t\tTotal time spent by all reduces in occupied slots (ms)=0\n01-11-2017 10:41:03 CST run_dataimport INFO - \t\tTotal time spent by all map tasks (ms)=2440026\n01-11-2017 10:41:03 CST run_dataimport INFO - \t\tTotal vcore-seconds taken by all map tasks=2440026\n01-11-2017 10:41:03 CST run_dataimport INFO - \t\tTotal megabyte-seconds taken by all map tasks=2498586624\n01-11-2017 10:41:03 CST run_dataimport INFO - \tMap-Reduce Framework\n01-11-2017 10:41:03 CST run_dataimport INFO - \t\tMap input records=50000\n01-11-2017 10:41:03 CST run_dataimport INFO - \t\tMap output records=50000\n01-11-2017 10:41:03 CST run_dataimport INFO - \t\tInput split bytes=462\n01-11-2017 10:41:03 CST run_dataimport INFO - \t\tSpilled Records=0\n01-11-2017 10:41:03 CST run_dataimport INFO - \t\tFailed Shuffles=0\n01-11-2017 10:41:03 CST run_dataimport INFO - \t\tMerged Map outputs=0\n01-11-2017 10:41:03 CST run_dataimport INFO - \t\tGC time elapsed (ms)=327\n01-11-2017 10:41:03 CST run_dataimport INFO - \t\tCPU time spent (ms)=33410\n01-11-2017 10:41:03 CST run_dataimport INFO - \t\tPhysical memory (bytes) snapshot=1476767744\n01-11-2017 10:41:03 CST run_dataimport INFO - \t\tVirtual memory (bytes) snapshot=6458535936\n01-11-2017 10:41:03 CST run_dataimport INFO - \t\tTotal committed heap usage (bytes)=3296722944\n01-11-2017 10:41:03 CST run_dataimport INFO - \tFile Input Format Counters \n01-11-2017 10:41:03 CST run_dataimport INFO - \t\tBytes Read=0\n01-11-2017 10:41:03 CST run_dataimport INFO - \tFile Output Format Counters \n01-11-2017 10:41:03 CST run_dataimport INFO - \t\tBytes Written=23450000\n01-11-2017 10:41:03 CST run_dataimport INFO - 17/11/01 10:41:03 INFO mapreduce.ImportJobBase: Transferred 22.3637 MB in 1,229.8978 seconds (18.6198 KB/sec)\n01-11-2017 10:41:03 CST run_dataimport INFO - 17/11/01 10:41:03 INFO mapreduce.ImportJobBase: Retrieved 50000 records.\n01-11-2017 10:41:03 CST run_dataimport INFO - 17/11/01 10:41:03 INFO util.AppendUtils: Appending to directory sql_to_hive\n01-11-2017 10:41:03 CST run_dataimport INFO - 17/11/01 10:41:03 INFO util.AppendUtils: Using found partition 2668\n01-11-2017 10:41:04 CST run_dataimport INFO - 17/11/01 10:41:04 INFO tool.ImportTool: Incremental import complete! To run another incremental import of all data following this import, supply the following arguments:\n01-11-2017 10:41:04 CST run_dataimport INFO - 17/11/01 10:41:04 INFO tool.ImportTool:  --incremental append\n01-11-2017 10:41:04 CST run_dataimport INFO - 17/11/01 10:41:04 INFO tool.ImportTool:   --check-column uid\n01-11-2017 10:41:04 CST run_dataimport INFO - 17/11/01 10:41:04 INFO tool.ImportTool:   --last-value 31762700\n01-11-2017 10:41:04 CST run_dataimport INFO - 17/11/01 10:41:04 INFO tool.ImportTool: (Consider saving this with 'sqoop job --create')\n01-11-2017 10:41:04 CST run_dataimport INFO - 11\n01-11-2017 10:41:04 CST run_dataimport INFO - Warning: /opt/cloudera/parcels/CDH-5.8.0-1.cdh5.8.0.p0.42/bin/../lib/sqoop/../accumulo does not exist! Accumulo imports will fail.\n01-11-2017 10:41:04 CST run_dataimport INFO - Please set $ACCUMULO_HOME to the root of your Accumulo installation.\n01-11-2017 10:41:06 CST run_dataimport INFO - 17/11/01 10:41:06 INFO sqoop.Sqoop: Running Sqoop version: 1.4.6-cdh5.8.0\n01-11-2017 10:41:06 CST run_dataimport INFO - 17/11/01 10:41:06 WARN sqoop.ConnFactory: Parameter --driver is set to an explicit driver however appropriate connection manager is not being set (via --connection-manager). Sqoop is going to fall back to org.apache.sqoop.manager.GenericJdbcManager. Please specify explicitly which connection manager should be used next time.\n01-11-2017 10:41:06 CST run_dataimport INFO - 17/11/01 10:41:06 INFO manager.SqlManager: Using default fetchSize of 1000\n01-11-2017 10:41:06 CST run_dataimport INFO - 17/11/01 10:41:06 INFO tool.CodeGenTool: Beginning code generation\n01-11-2017 10:41:07 CST run_dataimport INFO - 17/11/01 10:41:07 INFO manager.SqlManager: Executing SQL statement: select top 50000 * from jzj.dbo.jzj_test where uid&gt;(select uid from (select top 150000 uid from jzj.dbo.jzj_test where uid&gt;31612700 and uid&lt;=32309400) a except select uid from (select top 149999 uid from jzj.dbo.jzj_test where uid&gt;31612700 and uid&lt;=32309400) a) and uid &lt;=32309400 and  (1 = 0) \n01-11-2017 10:41:07 CST run_dataimport INFO - 17/11/01 10:41:07 INFO manager.SqlManager: Executing SQL statement: select top 50000 * from jzj.dbo.jzj_test where uid&gt;(select uid from (select top 150000 uid from jzj.dbo.jzj_test where uid&gt;31612700 and uid&lt;=32309400) a except select uid from (select top 149999 uid from jzj.dbo.jzj_test where uid&gt;31612700 and uid&lt;=32309400) a) and uid &lt;=32309400 and  (1 = 0) \n01-11-2017 10:41:07 CST run_dataimport INFO - 17/11/01 10:41:07 INFO orm.CompilationManager: HADOOP_MAPRED_HOME is /opt/cloudera/parcels/CDH/lib/hadoop-mapreduce\n01-11-2017 10:41:09 CST run_dataimport INFO - Note: /tmp/sqoop-root/compile/61fb0da5f39ceec0b287b16e9e75fd1b/QueryResult.java uses or overrides a deprecated API.\n01-11-2017 10:41:09 CST run_dataimport INFO - Note: Recompile with -Xlint:deprecation for details.\n01-11-2017 10:41:09 CST run_dataimport INFO - 17/11/01 10:41:09 INFO orm.CompilationManager: Writing jar file: /tmp/sqoop-root/compile/61fb0da5f39ceec0b287b16e9e75fd1b/QueryResult.jar\n01-11-2017 10:41:10 CST run_dataimport INFO - 17/11/01 10:41:10 INFO tool.ImportTool: Maximal id query for free form incremental import: SELECT MAX(uid) FROM (select top 50000 * from jzj.dbo.jzj_test where uid&gt;(select uid from (select top 150000 uid from jzj.dbo.jzj_test where uid&gt;31612700 and uid&lt;=32309400) a except select uid from (select top 149999 uid from jzj.dbo.jzj_test where uid&gt;31612700 and uid&lt;=32309400) a) and uid &lt;=32309400 and (1 = 1)) sqoop_import_query_alias\n", 'length': 34468}
[2017-11-01 10:59:06,964] app.hive_mysql.AzkabanMonitor [INFO] URL: https://172.23.27.203:8443
[2017-11-01 10:59:07,127] app.hive_mysql.AzkabanMonitor [INFO] Success: Login https://172.23.27.203:8443 with user azkaban
[2017-11-01 10:59:07,127] app.hive_mysql.AzkabanMonitor [DEBUG] {'session.id': '76a01482-0379-4e3f-8769-e1f2ca9a8c01', 'status': 'success'}
[2017-11-01 10:59:07,149] app.hive_mysql.AzkabanMonitor [INFO] Success: Fetch Executions of a Flow - run_dataimport
[2017-11-01 10:59:07,149] app.hive_mysql.AzkabanMonitor [DEBUG] {'total': 6, 'executions': [{'submitTime': 1509501650049, 'submitUser': 'azkaban', 'startTime': 1509501650215, 'endTime': -1, 'flowId': 'run_dataimport', 'projectId': 68, 'execId': 1658, 'status': 'RUNNING'}, {'submitTime': 1509494450033, 'submitUser': 'azkaban', 'startTime': 1509494450241, 'endTime': 1509496877667, 'flowId': 'run_dataimport', 'projectId': 68, 'execId': 1657, 'status': 'SUCCEEDED'}, {'submitTime': 1509458449965, 'submitUser': 'azkaban', 'startTime': 1509458450181, 'endTime': 1509489787442, 'flowId': 'run_dataimport', 'projectId': 68, 'execId': 1656, 'status': 'SUCCEEDED'}, {'submitTime': 1509444049936, 'submitUser': 'azkaban', 'startTime': 1509444050155, 'endTime': 1509457451629, 'flowId': 'run_dataimport', 'projectId': 68, 'execId': 1655, 'status': 'SUCCEEDED'}, {'submitTime': 1509438862311, 'submitUser': 'azkaban', 'startTime': 1509438862468, 'endTime': 1509438932606, 'flowId': 'run_dataimport', 'projectId': 68, 'execId': 1654, 'status': 'KILLED'}, {'submitTime': 1509438455131, 'submitUser': 'azkaban', 'startTime': 1509438455403, 'endTime': 1509438536090, 'flowId': 'run_dataimport', 'projectId': 68, 'execId': 1653, 'status': 'KILLED'}], 'length': 10, 'project': 'sql_to_hive', 'from': 0, 'projectId': 68, 'flow': 'run_dataimport'}
[2017-11-01 10:59:07,521] app.hive_mysql.AzkabanMonitor [INFO] Success: Fetch Execution Job Logs - 1658-run_dataimport
[2017-11-01 10:59:07,521] app.hive_mysql.AzkabanMonitor [DEBUG] {'offset': 0, 'data': "01-11-2017 10:00:50 CST run_dataimport INFO - Starting job run_dataimport at 1509501650218\n01-11-2017 10:00:50 CST run_dataimport INFO - azkaban.webserver.url property was not set\n01-11-2017 10:00:50 CST run_dataimport INFO - job JVM args: -Dazkaban.flowid=run_dataimport -Dazkaban.execid=1658 -Dazkaban.jobid=run_dataimport\n01-11-2017 10:00:50 CST run_dataimport INFO - Building command job executor. \n01-11-2017 10:00:50 CST run_dataimport INFO - Memory granted for job run_dataimport\n01-11-2017 10:00:50 CST run_dataimport INFO - 1 commands to execute.\n01-11-2017 10:00:50 CST run_dataimport INFO - cwd=/home/azkaban/azkaban_bk/azkaban-exec-server-3.35.0-22-g5517d50/executions/1658\n01-11-2017 10:00:50 CST run_dataimport INFO - effective user is: azkaban\n01-11-2017 10:00:50 CST run_dataimport INFO - Command: sh /home/sql_to_hive/dataimport.sh\n01-11-2017 10:00:50 CST run_dataimport INFO - Environment variables: {JOB_OUTPUT_PROP_FILE=/home/azkaban/azkaban_bk/azkaban-exec-server-3.35.0-22-g5517d50/executions/1658/run_dataimport_output_2861162274243800284_tmp, JOB_PROP_FILE=/home/azkaban/azkaban_bk/azkaban-exec-server-3.35.0-22-g5517d50/executions/1658/run_dataimport_props_4685474620571842989_tmp, KRB5CCNAME=/tmp/krb5cc__sql_to_hive__run_dataimport__run_dataimport__1658__azkaban, JOB_NAME=run_dataimport}\n01-11-2017 10:00:50 CST run_dataimport INFO - Working directory: /home/azkaban/azkaban_bk/azkaban-exec-server-3.35.0-22-g5517d50/executions/1658\n01-11-2017 10:00:50 CST run_dataimport INFO - log4j:WARN No appenders could be found for logger (org.apache.hive.jdbc.Utils).\n01-11-2017 10:00:50 CST run_dataimport INFO - log4j:WARN Please initialize the log4j system properly.\n01-11-2017 10:00:50 CST run_dataimport INFO - log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.\n01-11-2017 10:00:50 CST run_dataimport INFO - SLF4J: Failed to load class &quot;org.slf4j.impl.StaticLoggerBinder&quot;.\n01-11-2017 10:00:50 CST run_dataimport INFO - SLF4J: Defaulting to no-operation (NOP) logger implementation\n01-11-2017 10:00:50 CST run_dataimport INFO - SLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.\n01-11-2017 10:01:33 CST run_dataimport INFO - 1,32309400,31612700,696700sqlmax:32309400\n01-11-2017 10:01:33 CST run_dataimport INFO - hivemax:31612700\n01-11-2017 10:01:33 CST run_dataimport INFO - counts:696700\n01-11-2017 10:01:33 CST run_dataimport INFO - 14,46700\n01-11-2017 10:01:33 CST run_dataimport INFO - 14\n01-11-2017 10:01:33 CST run_dataimport INFO - Warning: /opt/cloudera/parcels/CDH-5.8.0-1.cdh5.8.0.p0.42/bin/../lib/sqoop/../accumulo does not exist! Accumulo imports will fail.\n01-11-2017 10:01:33 CST run_dataimport INFO - Please set $ACCUMULO_HOME to the root of your Accumulo installation.\n01-11-2017 10:01:35 CST run_dataimport INFO - 17/11/01 10:01:35 INFO sqoop.Sqoop: Running Sqoop version: 1.4.6-cdh5.8.0\n01-11-2017 10:01:35 CST run_dataimport INFO - 17/11/01 10:01:35 WARN sqoop.ConnFactory: Parameter --driver is set to an explicit driver however appropriate connection manager is not being set (via --connection-manager). Sqoop is going to fall back to org.apache.sqoop.manager.GenericJdbcManager. Please specify explicitly which connection manager should be used next time.\n01-11-2017 10:01:35 CST run_dataimport INFO - 17/11/01 10:01:35 INFO manager.SqlManager: Using default fetchSize of 1000\n01-11-2017 10:01:35 CST run_dataimport INFO - 17/11/01 10:01:35 INFO tool.CodeGenTool: Beginning code generation\n01-11-2017 10:01:36 CST run_dataimport INFO - 17/11/01 10:01:36 INFO manager.SqlManager: Executing SQL statement: select top 50000 * from jzj.dbo.jzj_test where uid&gt;=(select uid from (select top 1 uid from jzj.dbo.jzj_test where uid&gt;31612700 and uid&lt;=32309400) a) and uid &lt;=32309400 and  (1 = 0) \n01-11-2017 10:01:36 CST run_dataimport INFO - 17/11/01 10:01:36 INFO manager.SqlManager: Executing SQL statement: select top 50000 * from jzj.dbo.jzj_test where uid&gt;=(select uid from (select top 1 uid from jzj.dbo.jzj_test where uid&gt;31612700 and uid&lt;=32309400) a) and uid &lt;=32309400 and  (1 = 0) \n01-11-2017 10:01:36 CST run_dataimport INFO - 17/11/01 10:01:36 INFO orm.CompilationManager: HADOOP_MAPRED_HOME is /opt/cloudera/parcels/CDH/lib/hadoop-mapreduce\n01-11-2017 10:01:38 CST run_dataimport INFO - Note: /tmp/sqoop-root/compile/ff02930c93108caafd8dbc9f2c064c43/QueryResult.java uses or overrides a deprecated API.\n01-11-2017 10:01:38 CST run_dataimport INFO - Note: Recompile with -Xlint:deprecation for details.\n01-11-2017 10:01:38 CST run_dataimport INFO - 17/11/01 10:01:38 INFO orm.CompilationManager: Writing jar file: /tmp/sqoop-root/compile/ff02930c93108caafd8dbc9f2c064c43/QueryResult.jar\n01-11-2017 10:01:39 CST run_dataimport INFO - 17/11/01 10:01:39 INFO tool.ImportTool: Maximal id query for free form incremental import: SELECT MAX(uid) FROM (select top 50000 * from jzj.dbo.jzj_test where uid&gt;=(select uid from (select top 1 uid from jzj.dbo.jzj_test where uid&gt;31612700 and uid&lt;=32309400) a) and uid &lt;=32309400 and (1 = 1)) sqoop_import_query_alias\n01-11-2017 10:01:39 CST run_dataimport INFO - 17/11/01 10:01:39 INFO tool.ImportTool: Incremental import based on column uid\n01-11-2017 10:01:39 CST run_dataimport INFO - 17/11/01 10:01:39 INFO tool.ImportTool: Upper bound value: 31662700\n01-11-2017 10:01:39 CST run_dataimport INFO - 17/11/01 10:01:39 INFO mapreduce.ImportJobBase: Beginning query import.\n01-11-2017 10:01:39 CST run_dataimport INFO - 17/11/01 10:01:39 INFO Configuration.deprecation: mapred.jar is deprecated. Instead, use mapreduce.job.jar\n01-11-2017 10:01:39 CST run_dataimport INFO - 17/11/01 10:01:39 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n01-11-2017 10:01:39 CST run_dataimport INFO - 17/11/01 10:01:39 INFO client.RMProxy: Connecting to ResourceManager at master/172.23.27.203:8032\n01-11-2017 10:01:45 CST run_dataimport INFO - 17/11/01 10:01:45 INFO db.DBInputFormat: Using read commited transaction isolation\n01-11-2017 10:01:45 CST run_dataimport INFO - 17/11/01 10:01:45 INFO db.DataDrivenDBInputFormat: BoundingValsQuery: SELECT MIN(uid), MAX(uid) FROM (select top 50000 * from jzj.dbo.jzj_test where uid&gt;=(select uid from (select top 1 uid from jzj.dbo.jzj_test where uid&gt;31612700 and uid&lt;=32309400) a) and uid &lt;=32309400 and uid &lt;= 31662700 AND  (1 = 1) ) AS t1\n01-11-2017 10:01:46 CST run_dataimport INFO - 17/11/01 10:01:46 INFO mapreduce.JobSubmitter: number of splits:4\n01-11-2017 10:01:46 CST run_dataimport INFO - 17/11/01 10:01:46 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1508671365448_1897\n01-11-2017 10:01:47 CST run_dataimport INFO - 17/11/01 10:01:47 INFO impl.YarnClientImpl: Submitted application application_1508671365448_1897\n01-11-2017 10:01:47 CST run_dataimport INFO - 17/11/01 10:01:47 INFO mapreduce.Job: The url to track the job: http://master:8088/proxy/application_1508671365448_1897/\n01-11-2017 10:01:47 CST run_dataimport INFO - 17/11/01 10:01:47 INFO mapreduce.Job: Running job: job_1508671365448_1897\n01-11-2017 10:01:54 CST run_dataimport INFO - 17/11/01 10:01:54 INFO mapreduce.Job: Job job_1508671365448_1897 running in uber mode : false\n01-11-2017 10:01:54 CST run_dataimport INFO - 17/11/01 10:01:54 INFO mapreduce.Job:  map 0% reduce 0%\n01-11-2017 10:02:02 CST run_dataimport INFO - 17/11/01 10:02:02 INFO mapreduce.Job:  map 25% reduce 0%\n01-11-2017 10:02:03 CST run_dataimport INFO - 17/11/01 10:02:03 INFO mapreduce.Job:  map 100% reduce 0%\n01-11-2017 10:02:05 CST run_dataimport INFO - 17/11/01 10:02:05 INFO mapreduce.Job: Job job_1508671365448_1897 completed successfully\n01-11-2017 10:02:05 CST run_dataimport INFO - 17/11/01 10:02:05 INFO mapreduce.Job: Counters: 30\n01-11-2017 10:02:05 CST run_dataimport INFO - \tFile System Counters\n01-11-2017 10:02:05 CST run_dataimport INFO - \t\tFILE: Number of bytes read=0\n01-11-2017 10:02:05 CST run_dataimport INFO - \t\tFILE: Number of bytes written=587012\n01-11-2017 10:02:05 CST run_dataimport INFO - \t\tFILE: Number of read operations=0\n01-11-2017 10:02:05 CST run_dataimport INFO - \t\tFILE: Number of large read operations=0\n01-11-2017 10:02:05 CST run_dataimport INFO - \t\tFILE: Number of write operations=0\n01-11-2017 10:02:05 CST run_dataimport INFO - \t\tHDFS: Number of bytes read=462\n01-11-2017 10:02:05 CST run_dataimport INFO - \t\tHDFS: Number of bytes written=23450000\n01-11-2017 10:02:05 CST run_dataimport INFO - \t\tHDFS: Number of read operations=16\n01-11-2017 10:02:05 CST run_dataimport INFO - \t\tHDFS: Number of large read operations=0\n01-11-2017 10:02:05 CST run_dataimport INFO - \t\tHDFS: Number of write operations=8\n01-11-2017 10:02:05 CST run_dataimport INFO - \tJob Counters \n01-11-2017 10:02:05 CST run_dataimport INFO - \t\tLaunched map tasks=4\n01-11-2017 10:02:05 CST run_dataimport INFO - \t\tOther local map tasks=4\n01-11-2017 10:02:05 CST run_dataimport INFO - \t\tTotal time spent by all maps in occupied slots (ms)=26571\n01-11-2017 10:02:05 CST run_dataimport INFO - \t\tTotal time spent by all reduces in occupied slots (ms)=0\n01-11-2017 10:02:05 CST run_dataimport INFO - \t\tTotal time spent by all map tasks (ms)=26571\n01-11-2017 10:02:05 CST run_dataimport INFO - \t\tTotal vcore-seconds taken by all map tasks=26571\n01-11-2017 10:02:05 CST run_dataimport INFO - \t\tTotal megabyte-seconds taken by all map tasks=27208704\n01-11-2017 10:02:05 CST run_dataimport INFO - \tMap-Reduce Framework\n01-11-2017 10:02:05 CST run_dataimport INFO - \t\tMap input records=50000\n01-11-2017 10:02:05 CST run_dataimport INFO - \t\tMap output records=50000\n01-11-2017 10:02:05 CST run_dataimport INFO - \t\tInput split bytes=462\n01-11-2017 10:02:05 CST run_dataimport INFO - \t\tSpilled Records=0\n01-11-2017 10:02:05 CST run_dataimport INFO - \t\tFailed Shuffles=0\n01-11-2017 10:02:05 CST run_dataimport INFO - \t\tMerged Map outputs=0\n01-11-2017 10:02:05 CST run_dataimport INFO - \t\tGC time elapsed (ms)=211\n01-11-2017 10:02:05 CST run_dataimport INFO - \t\tCPU time spent (ms)=24550\n01-11-2017 10:02:05 CST run_dataimport INFO - \t\tPhysical memory (bytes) snapshot=1433231360\n01-11-2017 10:02:05 CST run_dataimport INFO - \t\tVirtual memory (bytes) snapshot=6511280128\n01-11-2017 10:02:05 CST run_dataimport INFO - \t\tTotal committed heap usage (bytes)=3296722944\n01-11-2017 10:02:05 CST run_dataimport INFO - \tFile Input Format Counters \n01-11-2017 10:02:05 CST run_dataimport INFO - \t\tBytes Read=0\n01-11-2017 10:02:05 CST run_dataimport INFO - \tFile Output Format Counters \n01-11-2017 10:02:05 CST run_dataimport INFO - \t\tBytes Written=23450000\n01-11-2017 10:02:05 CST run_dataimport INFO - 17/11/01 10:02:05 INFO mapreduce.ImportJobBase: Transferred 22.3637 MB in 26.2622 seconds (871.9906 KB/sec)\n01-11-2017 10:02:05 CST run_dataimport INFO - 17/11/01 10:02:05 INFO mapreduce.ImportJobBase: Retrieved 50000 records.\n01-11-2017 10:02:05 CST run_dataimport INFO - 17/11/01 10:02:05 INFO util.AppendUtils: Appending to directory sql_to_hive\n01-11-2017 10:02:06 CST run_dataimport INFO - 17/11/01 10:02:06 INFO util.AppendUtils: Using found partition 2660\n01-11-2017 10:02:06 CST run_dataimport INFO - 17/11/01 10:02:06 INFO tool.ImportTool: Incremental import complete! To run another incremental import of all data following this import, supply the following arguments:\n01-11-2017 10:02:06 CST run_dataimport INFO - 17/11/01 10:02:06 INFO tool.ImportTool:  --incremental append\n01-11-2017 10:02:06 CST run_dataimport INFO - 17/11/01 10:02:06 INFO tool.ImportTool:   --check-column uid\n01-11-2017 10:02:06 CST run_dataimport INFO - 17/11/01 10:02:06 INFO tool.ImportTool:   --last-value 31662700\n01-11-2017 10:02:06 CST run_dataimport INFO - 17/11/01 10:02:06 INFO tool.ImportTool: (Consider saving this with 'sqoop job --create')\n01-11-2017 10:02:06 CST run_dataimport INFO - 13\n01-11-2017 10:02:06 CST run_dataimport INFO - Warning: /opt/cloudera/parcels/CDH-5.8.0-1.cdh5.8.0.p0.42/bin/../lib/sqoop/../accumulo does not exist! Accumulo imports will fail.\n01-11-2017 10:02:06 CST run_dataimport INFO - Please set $ACCUMULO_HOME to the root of your Accumulo installation.\n01-11-2017 10:02:08 CST run_dataimport INFO - 17/11/01 10:02:08 INFO sqoop.Sqoop: Running Sqoop version: 1.4.6-cdh5.8.0\n01-11-2017 10:02:08 CST run_dataimport INFO - 17/11/01 10:02:08 WARN sqoop.ConnFactory: Parameter --driver is set to an explicit driver however appropriate connection manager is not being set (via --connection-manager). Sqoop is going to fall back to org.apache.sqoop.manager.GenericJdbcManager. Please specify explicitly which connection manager should be used next time.\n01-11-2017 10:02:08 CST run_dataimport INFO - 17/11/01 10:02:08 INFO manager.SqlManager: Using default fetchSize of 1000\n01-11-2017 10:02:08 CST run_dataimport INFO - 17/11/01 10:02:08 INFO tool.CodeGenTool: Beginning code generation\n01-11-2017 10:02:09 CST run_dataimport INFO - 17/11/01 10:02:09 INFO manager.SqlManager: Executing SQL statement: select top 50000 * from jzj.dbo.jzj_test where uid&gt;(select uid from (select top 50000 uid from jzj.dbo.jzj_test where uid&gt;31612700 and uid&lt;=32309400) a except select uid from (select top 49999 uid from jzj.dbo.jzj_test where uid&gt;31612700 and uid&lt;=32309400) a) and uid &lt;=32309400 and  (1 = 0) \n01-11-2017 10:02:09 CST run_dataimport INFO - 17/11/01 10:02:09 INFO manager.SqlManager: Executing SQL statement: select top 50000 * from jzj.dbo.jzj_test where uid&gt;(select uid from (select top 50000 uid from jzj.dbo.jzj_test where uid&gt;31612700 and uid&lt;=32309400) a except select uid from (select top 49999 uid from jzj.dbo.jzj_test where uid&gt;31612700 and uid&lt;=32309400) a) and uid &lt;=32309400 and  (1 = 0) \n01-11-2017 10:02:09 CST run_dataimport INFO - 17/11/01 10:02:09 INFO orm.CompilationManager: HADOOP_MAPRED_HOME is /opt/cloudera/parcels/CDH/lib/hadoop-mapreduce\n01-11-2017 10:02:11 CST run_dataimport INFO - Note: /tmp/sqoop-root/compile/9de72518c89c5280b88b044ac6fd6c80/QueryResult.java uses or overrides a deprecated API.\n01-11-2017 10:02:11 CST run_dataimport INFO - Note: Recompile with -Xlint:deprecation for details.\n01-11-2017 10:02:11 CST run_dataimport INFO - 17/11/01 10:02:11 INFO orm.CompilationManager: Writing jar file: /tmp/sqoop-root/compile/9de72518c89c5280b88b044ac6fd6c80/QueryResult.jar\n01-11-2017 10:02:12 CST run_dataimport INFO - 17/11/01 10:02:12 INFO tool.ImportTool: Maximal id query for free form incremental import: SELECT MAX(uid) FROM (select top 50000 * from jzj.dbo.jzj_test where uid&gt;(select uid from (select top 50000 uid from jzj.dbo.jzj_test where uid&gt;31612700 and uid&lt;=32309400) a except select uid from (select top 49999 uid from jzj.dbo.jzj_test where uid&gt;31612700 and uid&lt;=32309400) a) and uid &lt;=32309400 and (1 = 1)) sqoop_import_query_alias\n01-11-2017 10:04:47 CST run_dataimport INFO - 17/11/01 10:04:47 INFO tool.ImportTool: Incremental import based on column uid\n01-11-2017 10:04:47 CST run_dataimport INFO - 17/11/01 10:04:47 INFO tool.ImportTool: Upper bound value: 31712700\n01-11-2017 10:04:47 CST run_dataimport INFO - 17/11/01 10:04:47 INFO mapreduce.ImportJobBase: Beginning query import.\n01-11-2017 10:04:47 CST run_dataimport INFO - 17/11/01 10:04:47 INFO Configuration.deprecation: mapred.jar is deprecated. Instead, use mapreduce.job.jar\n01-11-2017 10:04:47 CST run_dataimport INFO - 17/11/01 10:04:47 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n01-11-2017 10:04:47 CST run_dataimport INFO - 17/11/01 10:04:47 INFO client.RMProxy: Connecting to ResourceManager at master/172.23.27.203:8032\n01-11-2017 10:04:52 CST run_dataimport INFO - 17/11/01 10:04:52 INFO db.DBInputFormat: Using read commited transaction isolation\n01-11-2017 10:04:52 CST run_dataimport INFO - 17/11/01 10:04:52 INFO db.DataDrivenDBInputFormat: BoundingValsQuery: SELECT MIN(uid), MAX(uid) FROM (select top 50000 * from jzj.dbo.jzj_test where uid&gt;(select uid from (select top 50000 uid from jzj.dbo.jzj_test where uid&gt;31612700 and uid&lt;=32309400) a except select uid from (select top 49999 uid from jzj.dbo.jzj_test where uid&gt;31612700 and uid&lt;=32309400) a) and uid &lt;=32309400 and uid &lt;= 31712700 AND  (1 = 1) ) AS t1\n01-11-2017 10:07:26 CST run_dataimport INFO - 17/11/01 10:07:26 INFO mapreduce.JobSubmitter: number of splits:4\n01-11-2017 10:07:27 CST run_dataimport INFO - 17/11/01 10:07:27 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1508671365448_1898\n01-11-2017 10:07:27 CST run_dataimport INFO - 17/11/01 10:07:27 INFO impl.YarnClientImpl: Submitted application application_1508671365448_1898\n01-11-2017 10:07:27 CST run_dataimport INFO - 17/11/01 10:07:27 INFO mapreduce.Job: The url to track the job: http://master:8088/proxy/application_1508671365448_1898/\n01-11-2017 10:07:27 CST run_dataimport INFO - 17/11/01 10:07:27 INFO mapreduce.Job: Running job: job_1508671365448_1898\n01-11-2017 10:07:34 CST run_dataimport INFO - 17/11/01 10:07:34 INFO mapreduce.Job: Job job_1508671365448_1898 running in uber mode : false\n01-11-2017 10:07:34 CST run_dataimport INFO - 17/11/01 10:07:34 INFO mapreduce.Job:  map 0% reduce 0%\n01-11-2017 10:10:17 CST run_dataimport INFO - 17/11/01 10:10:17 INFO mapreduce.Job:  map 25% reduce 0%\n01-11-2017 10:10:19 CST run_dataimport INFO - 17/11/01 10:10:19 INFO mapreduce.Job:  map 75% reduce 0%\n01-11-2017 10:10:20 CST run_dataimport INFO - 17/11/01 10:10:20 INFO mapreduce.Job:  map 100% reduce 0%\n01-11-2017 10:10:21 CST run_dataimport INFO - 17/11/01 10:10:21 INFO mapreduce.Job: Job job_1508671365448_1898 completed successfully\n01-11-2017 10:10:21 CST run_dataimport INFO - 17/11/01 10:10:21 INFO mapreduce.Job: Counters: 30\n01-11-2017 10:10:21 CST run_dataimport INFO - \tFile System Counters\n01-11-2017 10:10:21 CST run_dataimport INFO - \t\tFILE: Number of bytes read=0\n01-11-2017 10:10:21 CST run_dataimport INFO - \t\tFILE: Number of bytes written=588412\n01-11-2017 10:10:21 CST run_dataimport INFO - \t\tFILE: Number of read operations=0\n01-11-2017 10:10:21 CST run_dataimport INFO - \t\tFILE: Number of large read operations=0\n01-11-2017 10:10:21 CST run_dataimport INFO - \t\tFILE: Number of write operations=0\n01-11-2017 10:10:21 CST run_dataimport INFO - \t\tHDFS: Number of bytes read=462\n01-11-2017 10:10:21 CST run_dataimport INFO - \t\tHDFS: Number of bytes written=23450000\n01-11-2017 10:10:21 CST run_dataimport INFO - \t\tHDFS: Number of read operations=16\n01-11-2017 10:10:21 CST run_dataimport INFO - \t\tHDFS: Number of large read operations=0\n01-11-2017 10:10:21 CST run_dataimport INFO - \t\tHDFS: Number of write operations=8\n01-11-2017 10:10:21 CST run_dataimport INFO - \tJob Counters \n01-11-2017 10:10:21 CST run_dataimport INFO - \t\tLaunched map tasks=4\n01-11-2017 10:10:21 CST run_dataimport INFO - \t\tOther local map tasks=4\n01-11-2017 10:10:21 CST run_dataimport INFO - \t\tTotal time spent by all maps in occupied slots (ms)=645979\n01-11-2017 10:10:21 CST run_dataimport INFO - \t\tTotal time spent by all reduces in occupied slots (ms)=0\n01-11-2017 10:10:21 CST run_dataimport INFO - \t\tTotal time spent by all map tasks (ms)=645979\n01-11-2017 10:10:21 CST run_dataimport INFO - \t\tTotal vcore-seconds taken by all map tasks=645979\n01-11-2017 10:10:21 CST run_dataimport INFO - \t\tTotal megabyte-seconds taken by all map tasks=661482496\n01-11-2017 10:10:21 CST run_dataimport INFO - \tMap-Reduce Framework\n01-11-2017 10:10:21 CST run_dataimport INFO - \t\tMap input records=50000\n01-11-2017 10:10:21 CST run_dataimport INFO - \t\tMap output records=50000\n01-11-2017 10:10:21 CST run_dataimport INFO - \t\tInput split bytes=462\n01-11-2017 10:10:21 CST run_dataimport INFO - \t\tSpilled Records=0\n01-11-2017 10:10:21 CST run_dataimport INFO - \t\tFailed Shuffles=0\n01-11-2017 10:10:21 CST run_dataimport INFO - \t\tMerged Map outputs=0\n01-11-2017 10:10:21 CST run_dataimport INFO - \t\tGC time elapsed (ms)=299\n01-11-2017 10:10:21 CST run_dataimport INFO - \t\tCPU time spent (ms)=26430\n01-11-2017 10:10:21 CST run_dataimport INFO - \t\tPhysical memory (bytes) snapshot=1455120384\n01-11-2017 10:10:21 CST run_dataimport INFO - \t\tVirtual memory (bytes) snapshot=6527934464\n01-11-2017 10:10:21 CST run_dataimport INFO - \t\tTotal committed heap usage (bytes)=3296722944\n01-11-2017 10:10:21 CST run_dataimport INFO - \tFile Input Format Counters \n01-11-2017 10:10:21 CST run_dataimport INFO - \t\tBytes Read=0\n01-11-2017 10:10:21 CST run_dataimport INFO - \tFile Output Format Counters \n01-11-2017 10:10:21 CST run_dataimport INFO - \t\tBytes Written=23450000\n01-11-2017 10:10:21 CST run_dataimport INFO - 17/11/01 10:10:21 INFO mapreduce.ImportJobBase: Transferred 22.3637 MB in 333.7909 seconds (68.607 KB/sec)\n01-11-2017 10:10:21 CST run_dataimport INFO - 17/11/01 10:10:21 INFO mapreduce.ImportJobBase: Retrieved 50000 records.\n01-11-2017 10:10:21 CST run_dataimport INFO - 17/11/01 10:10:21 INFO util.AppendUtils: Appending to directory sql_to_hive\n01-11-2017 10:10:21 CST run_dataimport INFO - 17/11/01 10:10:21 INFO util.AppendUtils: Using found partition 2664\n01-11-2017 10:10:21 CST run_dataimport INFO - 17/11/01 10:10:21 INFO tool.ImportTool: Incremental import complete! To run another incremental import of all data following this import, supply the following arguments:\n01-11-2017 10:10:21 CST run_dataimport INFO - 17/11/01 10:10:21 INFO tool.ImportTool:  --incremental append\n01-11-2017 10:10:21 CST run_dataimport INFO - 17/11/01 10:10:21 INFO tool.ImportTool:   --check-column uid\n01-11-2017 10:10:21 CST run_dataimport INFO - 17/11/01 10:10:21 INFO tool.ImportTool:   --last-value 31712700\n01-11-2017 10:10:21 CST run_dataimport INFO - 17/11/01 10:10:21 INFO tool.ImportTool: (Consider saving this with 'sqoop job --create')\n01-11-2017 10:10:22 CST run_dataimport INFO - 12\n01-11-2017 10:10:22 CST run_dataimport INFO - Warning: /opt/cloudera/parcels/CDH-5.8.0-1.cdh5.8.0.p0.42/bin/../lib/sqoop/../accumulo does not exist! Accumulo imports will fail.\n01-11-2017 10:10:22 CST run_dataimport INFO - Please set $ACCUMULO_HOME to the root of your Accumulo installation.\n01-11-2017 10:10:24 CST run_dataimport INFO - 17/11/01 10:10:24 INFO sqoop.Sqoop: Running Sqoop version: 1.4.6-cdh5.8.0\n01-11-2017 10:10:24 CST run_dataimport INFO - 17/11/01 10:10:24 WARN sqoop.ConnFactory: Parameter --driver is set to an explicit driver however appropriate connection manager is not being set (via --connection-manager). Sqoop is going to fall back to org.apache.sqoop.manager.GenericJdbcManager. Please specify explicitly which connection manager should be used next time.\n01-11-2017 10:10:24 CST run_dataimport INFO - 17/11/01 10:10:24 INFO manager.SqlManager: Using default fetchSize of 1000\n01-11-2017 10:10:24 CST run_dataimport INFO - 17/11/01 10:10:24 INFO tool.CodeGenTool: Beginning code generation\n01-11-2017 10:10:24 CST run_dataimport INFO - 17/11/01 10:10:24 INFO manager.SqlManager: Executing SQL statement: select top 50000 * from jzj.dbo.jzj_test where uid&gt;(select uid from (select top 100000 uid from jzj.dbo.jzj_test where uid&gt;31612700 and uid&lt;=32309400) a except select uid from (select top 99999 uid from jzj.dbo.jzj_test where uid&gt;31612700 and uid&lt;=32309400) a) and uid &lt;=32309400 and  (1 = 0) \n01-11-2017 10:10:24 CST run_dataimport INFO - 17/11/01 10:10:24 INFO manager.SqlManager: Executing SQL statement: select top 50000 * from jzj.dbo.jzj_test where uid&gt;(select uid from (select top 100000 uid from jzj.dbo.jzj_test where uid&gt;31612700 and uid&lt;=32309400) a except select uid from (select top 99999 uid from jzj.dbo.jzj_test where uid&gt;31612700 and uid&lt;=32309400) a) and uid &lt;=32309400 and  (1 = 0) \n01-11-2017 10:10:25 CST run_dataimport INFO - 17/11/01 10:10:25 INFO orm.CompilationManager: HADOOP_MAPRED_HOME is /opt/cloudera/parcels/CDH/lib/hadoop-mapreduce\n01-11-2017 10:10:27 CST run_dataimport INFO - Note: /tmp/sqoop-root/compile/9f82a203e0f4d154a66e65e3115fe8a9/QueryResult.java uses or overrides a deprecated API.\n01-11-2017 10:10:27 CST run_dataimport INFO - Note: Recompile with -Xlint:deprecation for details.\n01-11-2017 10:10:27 CST run_dataimport INFO - 17/11/01 10:10:27 INFO orm.CompilationManager: Writing jar file: /tmp/sqoop-root/compile/9f82a203e0f4d154a66e65e3115fe8a9/QueryResult.jar\n01-11-2017 10:10:28 CST run_dataimport INFO - 17/11/01 10:10:28 INFO tool.ImportTool: Maximal id query for free form incremental import: SELECT MAX(uid) FROM (select top 50000 * from jzj.dbo.jzj_test where uid&gt;(select uid from (select top 100000 uid from jzj.dbo.jzj_test where uid&gt;31612700 and uid&lt;=32309400) a except select uid from (select top 99999 uid from jzj.dbo.jzj_test where uid&gt;31612700 and uid&lt;=32309400) a) and uid &lt;=32309400 and (1 = 1)) sqoop_import_query_alias\n01-11-2017 10:20:33 CST run_dataimport INFO - 17/11/01 10:20:33 INFO tool.ImportTool: Incremental import based on column uid\n01-11-2017 10:20:33 CST run_dataimport INFO - 17/11/01 10:20:33 INFO tool.ImportTool: Upper bound value: 31762700\n01-11-2017 10:20:33 CST run_dataimport INFO - 17/11/01 10:20:33 INFO mapreduce.ImportJobBase: Beginning query import.\n01-11-2017 10:20:33 CST run_dataimport INFO - 17/11/01 10:20:33 INFO Configuration.deprecation: mapred.jar is deprecated. Instead, use mapreduce.job.jar\n01-11-2017 10:20:33 CST run_dataimport INFO - 17/11/01 10:20:33 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n01-11-2017 10:20:33 CST run_dataimport INFO - 17/11/01 10:20:33 INFO client.RMProxy: Connecting to ResourceManager at master/172.23.27.203:8032\n01-11-2017 10:20:40 CST run_dataimport INFO - 17/11/01 10:20:40 INFO db.DBInputFormat: Using read commited transaction isolation\n01-11-2017 10:20:40 CST run_dataimport INFO - 17/11/01 10:20:40 INFO db.DataDrivenDBInputFormat: BoundingValsQuery: SELECT MIN(uid), MAX(uid) FROM (select top 50000 * from jzj.dbo.jzj_test where uid&gt;(select uid from (select top 100000 uid from jzj.dbo.jzj_test where uid&gt;31612700 and uid&lt;=32309400) a except select uid from (select top 99999 uid from jzj.dbo.jzj_test where uid&gt;31612700 and uid&lt;=32309400) a) and uid &lt;=32309400 and uid &lt;= 31762700 AND  (1 = 1) ) AS t1\n01-11-2017 10:30:36 CST run_dataimport INFO - 17/11/01 10:30:36 INFO mapreduce.JobSubmitter: number of splits:4\n01-11-2017 10:30:37 CST run_dataimport INFO - 17/11/01 10:30:37 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1508671365448_1899\n01-11-2017 10:30:37 CST run_dataimport INFO - 17/11/01 10:30:37 INFO impl.YarnClientImpl: Submitted application application_1508671365448_1899\n01-11-2017 10:30:37 CST run_dataimport INFO - 17/11/01 10:30:37 INFO mapreduce.Job: The url to track the job: http://master:8088/proxy/application_1508671365448_1899/\n01-11-2017 10:30:37 CST run_dataimport INFO - 17/11/01 10:30:37 INFO mapreduce.Job: Running job: job_1508671365448_1899\n01-11-2017 10:30:44 CST run_dataimport INFO - 17/11/01 10:30:44 INFO mapreduce.Job: Job job_1508671365448_1899 running in uber mode : false\n01-11-2017 10:30:44 CST run_dataimport INFO - 17/11/01 10:30:44 INFO mapreduce.Job:  map 0% reduce 0%\n01-11-2017 10:40:52 CST run_dataimport INFO - 17/11/01 10:40:52 INFO mapreduce.Job:  map 25% reduce 0%\n01-11-2017 10:40:53 CST run_dataimport INFO - 17/11/01 10:40:53 INFO mapreduce.Job:  map 50% reduce 0%\n01-11-2017 10:41:00 CST run_dataimport INFO - 17/11/01 10:41:00 INFO mapreduce.Job:  map 75% reduce 0%\n01-11-2017 10:41:02 CST run_dataimport INFO - 17/11/01 10:41:02 INFO mapreduce.Job:  map 100% reduce 0%\n01-11-2017 10:41:03 CST run_dataimport INFO - 17/11/01 10:41:03 INFO mapreduce.Job: Job job_1508671365448_1899 completed successfully\n01-11-2017 10:41:03 CST run_dataimport INFO - 17/11/01 10:41:03 INFO mapreduce.Job: Counters: 30\n01-11-2017 10:41:03 CST run_dataimport INFO - \tFile System Counters\n01-11-2017 10:41:03 CST run_dataimport INFO - \t\tFILE: Number of bytes read=0\n01-11-2017 10:41:03 CST run_dataimport INFO - \t\tFILE: Number of bytes written=588428\n01-11-2017 10:41:03 CST run_dataimport INFO - \t\tFILE: Number of read operations=0\n01-11-2017 10:41:03 CST run_dataimport INFO - \t\tFILE: Number of large read operations=0\n01-11-2017 10:41:03 CST run_dataimport INFO - \t\tFILE: Number of write operations=0\n01-11-2017 10:41:03 CST run_dataimport INFO - \t\tHDFS: Number of bytes read=462\n01-11-2017 10:41:03 CST run_dataimport INFO - \t\tHDFS: Number of bytes written=23450000\n01-11-2017 10:41:03 CST run_dataimport INFO - \t\tHDFS: Number of read operations=16\n01-11-2017 10:41:03 CST run_dataimport INFO - \t\tHDFS: Number of large read operations=0\n01-11-2017 10:41:03 CST run_dataimport INFO - \t\tHDFS: Number of write operations=8\n01-11-2017 10:41:03 CST run_dataimport INFO - \tJob Counters \n01-11-2017 10:41:03 CST run_dataimport INFO - \t\tLaunched map tasks=4\n01-11-2017 10:41:03 CST run_dataimport INFO - \t\tOther local map tasks=4\n01-11-2017 10:41:03 CST run_dataimport INFO - \t\tTotal time spent by all maps in occupied slots (ms)=2440026\n01-11-2017 10:41:03 CST run_dataimport INFO - \t\tTotal time spent by all reduces in occupied slots (ms)=0\n01-11-2017 10:41:03 CST run_dataimport INFO - \t\tTotal time spent by all map tasks (ms)=2440026\n01-11-2017 10:41:03 CST run_dataimport INFO - \t\tTotal vcore-seconds taken by all map tasks=2440026\n01-11-2017 10:41:03 CST run_dataimport INFO - \t\tTotal megabyte-seconds taken by all map tasks=2498586624\n01-11-2017 10:41:03 CST run_dataimport INFO - \tMap-Reduce Framework\n01-11-2017 10:41:03 CST run_dataimport INFO - \t\tMap input records=50000\n01-11-2017 10:41:03 CST run_dataimport INFO - \t\tMap output records=50000\n01-11-2017 10:41:03 CST run_dataimport INFO - \t\tInput split bytes=462\n01-11-2017 10:41:03 CST run_dataimport INFO - \t\tSpilled Records=0\n01-11-2017 10:41:03 CST run_dataimport INFO - \t\tFailed Shuffles=0\n01-11-2017 10:41:03 CST run_dataimport INFO - \t\tMerged Map outputs=0\n01-11-2017 10:41:03 CST run_dataimport INFO - \t\tGC time elapsed (ms)=327\n01-11-2017 10:41:03 CST run_dataimport INFO - \t\tCPU time spent (ms)=33410\n01-11-2017 10:41:03 CST run_dataimport INFO - \t\tPhysical memory (bytes) snapshot=1476767744\n01-11-2017 10:41:03 CST run_dataimport INFO - \t\tVirtual memory (bytes) snapshot=6458535936\n01-11-2017 10:41:03 CST run_dataimport INFO - \t\tTotal committed heap usage (bytes)=3296722944\n01-11-2017 10:41:03 CST run_dataimport INFO - \tFile Input Format Counters \n01-11-2017 10:41:03 CST run_dataimport INFO - \t\tBytes Read=0\n01-11-2017 10:41:03 CST run_dataimport INFO - \tFile Output Format Counters \n01-11-2017 10:41:03 CST run_dataimport INFO - \t\tBytes Written=23450000\n01-11-2017 10:41:03 CST run_dataimport INFO - 17/11/01 10:41:03 INFO mapreduce.ImportJobBase: Transferred 22.3637 MB in 1,229.8978 seconds (18.6198 KB/sec)\n01-11-2017 10:41:03 CST run_dataimport INFO - 17/11/01 10:41:03 INFO mapreduce.ImportJobBase: Retrieved 50000 records.\n01-11-2017 10:41:03 CST run_dataimport INFO - 17/11/01 10:41:03 INFO util.AppendUtils: Appending to directory sql_to_hive\n01-11-2017 10:41:03 CST run_dataimport INFO - 17/11/01 10:41:03 INFO util.AppendUtils: Using found partition 2668\n01-11-2017 10:41:04 CST run_dataimport INFO - 17/11/01 10:41:04 INFO tool.ImportTool: Incremental import complete! To run another incremental import of all data following this import, supply the following arguments:\n01-11-2017 10:41:04 CST run_dataimport INFO - 17/11/01 10:41:04 INFO tool.ImportTool:  --incremental append\n01-11-2017 10:41:04 CST run_dataimport INFO - 17/11/01 10:41:04 INFO tool.ImportTool:   --check-column uid\n01-11-2017 10:41:04 CST run_dataimport INFO - 17/11/01 10:41:04 INFO tool.ImportTool:   --last-value 31762700\n01-11-2017 10:41:04 CST run_dataimport INFO - 17/11/01 10:41:04 INFO tool.ImportTool: (Consider saving this with 'sqoop job --create')\n01-11-2017 10:41:04 CST run_dataimport INFO - 11\n01-11-2017 10:41:04 CST run_dataimport INFO - Warning: /opt/cloudera/parcels/CDH-5.8.0-1.cdh5.8.0.p0.42/bin/../lib/sqoop/../accumulo does not exist! Accumulo imports will fail.\n01-11-2017 10:41:04 CST run_dataimport INFO - Please set $ACCUMULO_HOME to the root of your Accumulo installation.\n01-11-2017 10:41:06 CST run_dataimport INFO - 17/11/01 10:41:06 INFO sqoop.Sqoop: Running Sqoop version: 1.4.6-cdh5.8.0\n01-11-2017 10:41:06 CST run_dataimport INFO - 17/11/01 10:41:06 WARN sqoop.ConnFactory: Parameter --driver is set to an explicit driver however appropriate connection manager is not being set (via --connection-manager). Sqoop is going to fall back to org.apache.sqoop.manager.GenericJdbcManager. Please specify explicitly which connection manager should be used next time.\n01-11-2017 10:41:06 CST run_dataimport INFO - 17/11/01 10:41:06 INFO manager.SqlManager: Using default fetchSize of 1000\n01-11-2017 10:41:06 CST run_dataimport INFO - 17/11/01 10:41:06 INFO tool.CodeGenTool: Beginning code generation\n01-11-2017 10:41:07 CST run_dataimport INFO - 17/11/01 10:41:07 INFO manager.SqlManager: Executing SQL statement: select top 50000 * from jzj.dbo.jzj_test where uid&gt;(select uid from (select top 150000 uid from jzj.dbo.jzj_test where uid&gt;31612700 and uid&lt;=32309400) a except select uid from (select top 149999 uid from jzj.dbo.jzj_test where uid&gt;31612700 and uid&lt;=32309400) a) and uid &lt;=32309400 and  (1 = 0) \n01-11-2017 10:41:07 CST run_dataimport INFO - 17/11/01 10:41:07 INFO manager.SqlManager: Executing SQL statement: select top 50000 * from jzj.dbo.jzj_test where uid&gt;(select uid from (select top 150000 uid from jzj.dbo.jzj_test where uid&gt;31612700 and uid&lt;=32309400) a except select uid from (select top 149999 uid from jzj.dbo.jzj_test where uid&gt;31612700 and uid&lt;=32309400) a) and uid &lt;=32309400 and  (1 = 0) \n01-11-2017 10:41:07 CST run_dataimport INFO - 17/11/01 10:41:07 INFO orm.CompilationManager: HADOOP_MAPRED_HOME is /opt/cloudera/parcels/CDH/lib/hadoop-mapreduce\n01-11-2017 10:41:09 CST run_dataimport INFO - Note: /tmp/sqoop-root/compile/61fb0da5f39ceec0b287b16e9e75fd1b/QueryResult.java uses or overrides a deprecated API.\n01-11-2017 10:41:09 CST run_dataimport INFO - Note: Recompile with -Xlint:deprecation for details.\n01-11-2017 10:41:09 CST run_dataimport INFO - 17/11/01 10:41:09 INFO orm.CompilationManager: Writing jar file: /tmp/sqoop-root/compile/61fb0da5f39ceec0b287b16e9e75fd1b/QueryResult.jar\n01-11-2017 10:41:10 CST run_dataimport INFO - 17/11/01 10:41:10 INFO tool.ImportTool: Maximal id query for free form incremental import: SELECT MAX(uid) FROM (select top 50000 * from jzj.dbo.jzj_test where uid&gt;(select uid from (select top 150000 uid from jzj.dbo.jzj_test where uid&gt;31612700 and uid&lt;=32309400) a except select uid from (select top 149999 uid from jzj.dbo.jzj_test where uid&gt;31612700 and uid&lt;=32309400) a) and uid &lt;=32309400 and (1 = 1)) sqoop_import_query_alias\n", 'length': 34468}
[2017-11-01 11:00:39,139] app.hive_mysql.AzkabanMonitor [INFO] URL: https://172.23.27.203:8443
[2017-11-01 11:00:39,312] app.hive_mysql.AzkabanMonitor [INFO] Success: Login https://172.23.27.203:8443 with user azkaban
[2017-11-01 11:00:39,312] app.hive_mysql.AzkabanMonitor [DEBUG] {'session.id': '2db2c5e4-b2fc-4b1f-bfc8-035d0bbe5a05', 'status': 'success'}
[2017-11-01 11:00:39,332] app.hive_mysql.AzkabanMonitor [INFO] Success: Fetch Executions of a Flow - run_dataimport
[2017-11-01 11:00:39,333] app.hive_mysql.AzkabanMonitor [DEBUG] {'total': 6, 'executions': [{'submitTime': 1509501650049, 'submitUser': 'azkaban', 'startTime': 1509501650215, 'endTime': -1, 'flowId': 'run_dataimport', 'projectId': 68, 'execId': 1658, 'status': 'RUNNING'}, {'submitTime': 1509494450033, 'submitUser': 'azkaban', 'startTime': 1509494450241, 'endTime': 1509496877667, 'flowId': 'run_dataimport', 'projectId': 68, 'execId': 1657, 'status': 'SUCCEEDED'}, {'submitTime': 1509458449965, 'submitUser': 'azkaban', 'startTime': 1509458450181, 'endTime': 1509489787442, 'flowId': 'run_dataimport', 'projectId': 68, 'execId': 1656, 'status': 'SUCCEEDED'}, {'submitTime': 1509444049936, 'submitUser': 'azkaban', 'startTime': 1509444050155, 'endTime': 1509457451629, 'flowId': 'run_dataimport', 'projectId': 68, 'execId': 1655, 'status': 'SUCCEEDED'}, {'submitTime': 1509438862311, 'submitUser': 'azkaban', 'startTime': 1509438862468, 'endTime': 1509438932606, 'flowId': 'run_dataimport', 'projectId': 68, 'execId': 1654, 'status': 'KILLED'}, {'submitTime': 1509438455131, 'submitUser': 'azkaban', 'startTime': 1509438455403, 'endTime': 1509438536090, 'flowId': 'run_dataimport', 'projectId': 68, 'execId': 1653, 'status': 'KILLED'}], 'length': 10, 'project': 'sql_to_hive', 'from': 0, 'projectId': 68, 'flow': 'run_dataimport'}
[2017-11-01 11:00:39,641] app.hive_mysql.AzkabanMonitor [INFO] Success: Fetch Execution Job Logs - 1658-run_dataimport
[2017-11-01 11:00:39,642] app.hive_mysql.AzkabanMonitor [DEBUG] {'offset': 0, 'data': "01-11-2017 10:00:50 CST run_dataimport INFO - Starting job run_dataimport at 1509501650218\n01-11-2017 10:00:50 CST run_dataimport INFO - azkaban.webserver.url property was not set\n01-11-2017 10:00:50 CST run_dataimport INFO - job JVM args: -Dazkaban.flowid=run_dataimport -Dazkaban.execid=1658 -Dazkaban.jobid=run_dataimport\n01-11-2017 10:00:50 CST run_dataimport INFO - Building command job executor. \n01-11-2017 10:00:50 CST run_dataimport INFO - Memory granted for job run_dataimport\n01-11-2017 10:00:50 CST run_dataimport INFO - 1 commands to execute.\n01-11-2017 10:00:50 CST run_dataimport INFO - cwd=/home/azkaban/azkaban_bk/azkaban-exec-server-3.35.0-22-g5517d50/executions/1658\n01-11-2017 10:00:50 CST run_dataimport INFO - effective user is: azkaban\n01-11-2017 10:00:50 CST run_dataimport INFO - Command: sh /home/sql_to_hive/dataimport.sh\n01-11-2017 10:00:50 CST run_dataimport INFO - Environment variables: {JOB_OUTPUT_PROP_FILE=/home/azkaban/azkaban_bk/azkaban-exec-server-3.35.0-22-g5517d50/executions/1658/run_dataimport_output_2861162274243800284_tmp, JOB_PROP_FILE=/home/azkaban/azkaban_bk/azkaban-exec-server-3.35.0-22-g5517d50/executions/1658/run_dataimport_props_4685474620571842989_tmp, KRB5CCNAME=/tmp/krb5cc__sql_to_hive__run_dataimport__run_dataimport__1658__azkaban, JOB_NAME=run_dataimport}\n01-11-2017 10:00:50 CST run_dataimport INFO - Working directory: /home/azkaban/azkaban_bk/azkaban-exec-server-3.35.0-22-g5517d50/executions/1658\n01-11-2017 10:00:50 CST run_dataimport INFO - log4j:WARN No appenders could be found for logger (org.apache.hive.jdbc.Utils).\n01-11-2017 10:00:50 CST run_dataimport INFO - log4j:WARN Please initialize the log4j system properly.\n01-11-2017 10:00:50 CST run_dataimport INFO - log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.\n01-11-2017 10:00:50 CST run_dataimport INFO - SLF4J: Failed to load class &quot;org.slf4j.impl.StaticLoggerBinder&quot;.\n01-11-2017 10:00:50 CST run_dataimport INFO - SLF4J: Defaulting to no-operation (NOP) logger implementation\n01-11-2017 10:00:50 CST run_dataimport INFO - SLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.\n01-11-2017 10:01:33 CST run_dataimport INFO - 1,32309400,31612700,696700sqlmax:32309400\n01-11-2017 10:01:33 CST run_dataimport INFO - hivemax:31612700\n01-11-2017 10:01:33 CST run_dataimport INFO - counts:696700\n01-11-2017 10:01:33 CST run_dataimport INFO - 14,46700\n01-11-2017 10:01:33 CST run_dataimport INFO - 14\n01-11-2017 10:01:33 CST run_dataimport INFO - Warning: /opt/cloudera/parcels/CDH-5.8.0-1.cdh5.8.0.p0.42/bin/../lib/sqoop/../accumulo does not exist! Accumulo imports will fail.\n01-11-2017 10:01:33 CST run_dataimport INFO - Please set $ACCUMULO_HOME to the root of your Accumulo installation.\n01-11-2017 10:01:35 CST run_dataimport INFO - 17/11/01 10:01:35 INFO sqoop.Sqoop: Running Sqoop version: 1.4.6-cdh5.8.0\n01-11-2017 10:01:35 CST run_dataimport INFO - 17/11/01 10:01:35 WARN sqoop.ConnFactory: Parameter --driver is set to an explicit driver however appropriate connection manager is not being set (via --connection-manager). Sqoop is going to fall back to org.apache.sqoop.manager.GenericJdbcManager. Please specify explicitly which connection manager should be used next time.\n01-11-2017 10:01:35 CST run_dataimport INFO - 17/11/01 10:01:35 INFO manager.SqlManager: Using default fetchSize of 1000\n01-11-2017 10:01:35 CST run_dataimport INFO - 17/11/01 10:01:35 INFO tool.CodeGenTool: Beginning code generation\n01-11-2017 10:01:36 CST run_dataimport INFO - 17/11/01 10:01:36 INFO manager.SqlManager: Executing SQL statement: select top 50000 * from jzj.dbo.jzj_test where uid&gt;=(select uid from (select top 1 uid from jzj.dbo.jzj_test where uid&gt;31612700 and uid&lt;=32309400) a) and uid &lt;=32309400 and  (1 = 0) \n01-11-2017 10:01:36 CST run_dataimport INFO - 17/11/01 10:01:36 INFO manager.SqlManager: Executing SQL statement: select top 50000 * from jzj.dbo.jzj_test where uid&gt;=(select uid from (select top 1 uid from jzj.dbo.jzj_test where uid&gt;31612700 and uid&lt;=32309400) a) and uid &lt;=32309400 and  (1 = 0) \n01-11-2017 10:01:36 CST run_dataimport INFO - 17/11/01 10:01:36 INFO orm.CompilationManager: HADOOP_MAPRED_HOME is /opt/cloudera/parcels/CDH/lib/hadoop-mapreduce\n01-11-2017 10:01:38 CST run_dataimport INFO - Note: /tmp/sqoop-root/compile/ff02930c93108caafd8dbc9f2c064c43/QueryResult.java uses or overrides a deprecated API.\n01-11-2017 10:01:38 CST run_dataimport INFO - Note: Recompile with -Xlint:deprecation for details.\n01-11-2017 10:01:38 CST run_dataimport INFO - 17/11/01 10:01:38 INFO orm.CompilationManager: Writing jar file: /tmp/sqoop-root/compile/ff02930c93108caafd8dbc9f2c064c43/QueryResult.jar\n01-11-2017 10:01:39 CST run_dataimport INFO - 17/11/01 10:01:39 INFO tool.ImportTool: Maximal id query for free form incremental import: SELECT MAX(uid) FROM (select top 50000 * from jzj.dbo.jzj_test where uid&gt;=(select uid from (select top 1 uid from jzj.dbo.jzj_test where uid&gt;31612700 and uid&lt;=32309400) a) and uid &lt;=32309400 and (1 = 1)) sqoop_import_query_alias\n01-11-2017 10:01:39 CST run_dataimport INFO - 17/11/01 10:01:39 INFO tool.ImportTool: Incremental import based on column uid\n01-11-2017 10:01:39 CST run_dataimport INFO - 17/11/01 10:01:39 INFO tool.ImportTool: Upper bound value: 31662700\n01-11-2017 10:01:39 CST run_dataimport INFO - 17/11/01 10:01:39 INFO mapreduce.ImportJobBase: Beginning query import.\n01-11-2017 10:01:39 CST run_dataimport INFO - 17/11/01 10:01:39 INFO Configuration.deprecation: mapred.jar is deprecated. Instead, use mapreduce.job.jar\n01-11-2017 10:01:39 CST run_dataimport INFO - 17/11/01 10:01:39 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n01-11-2017 10:01:39 CST run_dataimport INFO - 17/11/01 10:01:39 INFO client.RMProxy: Connecting to ResourceManager at master/172.23.27.203:8032\n01-11-2017 10:01:45 CST run_dataimport INFO - 17/11/01 10:01:45 INFO db.DBInputFormat: Using read commited transaction isolation\n01-11-2017 10:01:45 CST run_dataimport INFO - 17/11/01 10:01:45 INFO db.DataDrivenDBInputFormat: BoundingValsQuery: SELECT MIN(uid), MAX(uid) FROM (select top 50000 * from jzj.dbo.jzj_test where uid&gt;=(select uid from (select top 1 uid from jzj.dbo.jzj_test where uid&gt;31612700 and uid&lt;=32309400) a) and uid &lt;=32309400 and uid &lt;= 31662700 AND  (1 = 1) ) AS t1\n01-11-2017 10:01:46 CST run_dataimport INFO - 17/11/01 10:01:46 INFO mapreduce.JobSubmitter: number of splits:4\n01-11-2017 10:01:46 CST run_dataimport INFO - 17/11/01 10:01:46 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1508671365448_1897\n01-11-2017 10:01:47 CST run_dataimport INFO - 17/11/01 10:01:47 INFO impl.YarnClientImpl: Submitted application application_1508671365448_1897\n01-11-2017 10:01:47 CST run_dataimport INFO - 17/11/01 10:01:47 INFO mapreduce.Job: The url to track the job: http://master:8088/proxy/application_1508671365448_1897/\n01-11-2017 10:01:47 CST run_dataimport INFO - 17/11/01 10:01:47 INFO mapreduce.Job: Running job: job_1508671365448_1897\n01-11-2017 10:01:54 CST run_dataimport INFO - 17/11/01 10:01:54 INFO mapreduce.Job: Job job_1508671365448_1897 running in uber mode : false\n01-11-2017 10:01:54 CST run_dataimport INFO - 17/11/01 10:01:54 INFO mapreduce.Job:  map 0% reduce 0%\n01-11-2017 10:02:02 CST run_dataimport INFO - 17/11/01 10:02:02 INFO mapreduce.Job:  map 25% reduce 0%\n01-11-2017 10:02:03 CST run_dataimport INFO - 17/11/01 10:02:03 INFO mapreduce.Job:  map 100% reduce 0%\n01-11-2017 10:02:05 CST run_dataimport INFO - 17/11/01 10:02:05 INFO mapreduce.Job: Job job_1508671365448_1897 completed successfully\n01-11-2017 10:02:05 CST run_dataimport INFO - 17/11/01 10:02:05 INFO mapreduce.Job: Counters: 30\n01-11-2017 10:02:05 CST run_dataimport INFO - \tFile System Counters\n01-11-2017 10:02:05 CST run_dataimport INFO - \t\tFILE: Number of bytes read=0\n01-11-2017 10:02:05 CST run_dataimport INFO - \t\tFILE: Number of bytes written=587012\n01-11-2017 10:02:05 CST run_dataimport INFO - \t\tFILE: Number of read operations=0\n01-11-2017 10:02:05 CST run_dataimport INFO - \t\tFILE: Number of large read operations=0\n01-11-2017 10:02:05 CST run_dataimport INFO - \t\tFILE: Number of write operations=0\n01-11-2017 10:02:05 CST run_dataimport INFO - \t\tHDFS: Number of bytes read=462\n01-11-2017 10:02:05 CST run_dataimport INFO - \t\tHDFS: Number of bytes written=23450000\n01-11-2017 10:02:05 CST run_dataimport INFO - \t\tHDFS: Number of read operations=16\n01-11-2017 10:02:05 CST run_dataimport INFO - \t\tHDFS: Number of large read operations=0\n01-11-2017 10:02:05 CST run_dataimport INFO - \t\tHDFS: Number of write operations=8\n01-11-2017 10:02:05 CST run_dataimport INFO - \tJob Counters \n01-11-2017 10:02:05 CST run_dataimport INFO - \t\tLaunched map tasks=4\n01-11-2017 10:02:05 CST run_dataimport INFO - \t\tOther local map tasks=4\n01-11-2017 10:02:05 CST run_dataimport INFO - \t\tTotal time spent by all maps in occupied slots (ms)=26571\n01-11-2017 10:02:05 CST run_dataimport INFO - \t\tTotal time spent by all reduces in occupied slots (ms)=0\n01-11-2017 10:02:05 CST run_dataimport INFO - \t\tTotal time spent by all map tasks (ms)=26571\n01-11-2017 10:02:05 CST run_dataimport INFO - \t\tTotal vcore-seconds taken by all map tasks=26571\n01-11-2017 10:02:05 CST run_dataimport INFO - \t\tTotal megabyte-seconds taken by all map tasks=27208704\n01-11-2017 10:02:05 CST run_dataimport INFO - \tMap-Reduce Framework\n01-11-2017 10:02:05 CST run_dataimport INFO - \t\tMap input records=50000\n01-11-2017 10:02:05 CST run_dataimport INFO - \t\tMap output records=50000\n01-11-2017 10:02:05 CST run_dataimport INFO - \t\tInput split bytes=462\n01-11-2017 10:02:05 CST run_dataimport INFO - \t\tSpilled Records=0\n01-11-2017 10:02:05 CST run_dataimport INFO - \t\tFailed Shuffles=0\n01-11-2017 10:02:05 CST run_dataimport INFO - \t\tMerged Map outputs=0\n01-11-2017 10:02:05 CST run_dataimport INFO - \t\tGC time elapsed (ms)=211\n01-11-2017 10:02:05 CST run_dataimport INFO - \t\tCPU time spent (ms)=24550\n01-11-2017 10:02:05 CST run_dataimport INFO - \t\tPhysical memory (bytes) snapshot=1433231360\n01-11-2017 10:02:05 CST run_dataimport INFO - \t\tVirtual memory (bytes) snapshot=6511280128\n01-11-2017 10:02:05 CST run_dataimport INFO - \t\tTotal committed heap usage (bytes)=3296722944\n01-11-2017 10:02:05 CST run_dataimport INFO - \tFile Input Format Counters \n01-11-2017 10:02:05 CST run_dataimport INFO - \t\tBytes Read=0\n01-11-2017 10:02:05 CST run_dataimport INFO - \tFile Output Format Counters \n01-11-2017 10:02:05 CST run_dataimport INFO - \t\tBytes Written=23450000\n01-11-2017 10:02:05 CST run_dataimport INFO - 17/11/01 10:02:05 INFO mapreduce.ImportJobBase: Transferred 22.3637 MB in 26.2622 seconds (871.9906 KB/sec)\n01-11-2017 10:02:05 CST run_dataimport INFO - 17/11/01 10:02:05 INFO mapreduce.ImportJobBase: Retrieved 50000 records.\n01-11-2017 10:02:05 CST run_dataimport INFO - 17/11/01 10:02:05 INFO util.AppendUtils: Appending to directory sql_to_hive\n01-11-2017 10:02:06 CST run_dataimport INFO - 17/11/01 10:02:06 INFO util.AppendUtils: Using found partition 2660\n01-11-2017 10:02:06 CST run_dataimport INFO - 17/11/01 10:02:06 INFO tool.ImportTool: Incremental import complete! To run another incremental import of all data following this import, supply the following arguments:\n01-11-2017 10:02:06 CST run_dataimport INFO - 17/11/01 10:02:06 INFO tool.ImportTool:  --incremental append\n01-11-2017 10:02:06 CST run_dataimport INFO - 17/11/01 10:02:06 INFO tool.ImportTool:   --check-column uid\n01-11-2017 10:02:06 CST run_dataimport INFO - 17/11/01 10:02:06 INFO tool.ImportTool:   --last-value 31662700\n01-11-2017 10:02:06 CST run_dataimport INFO - 17/11/01 10:02:06 INFO tool.ImportTool: (Consider saving this with 'sqoop job --create')\n01-11-2017 10:02:06 CST run_dataimport INFO - 13\n01-11-2017 10:02:06 CST run_dataimport INFO - Warning: /opt/cloudera/parcels/CDH-5.8.0-1.cdh5.8.0.p0.42/bin/../lib/sqoop/../accumulo does not exist! Accumulo imports will fail.\n01-11-2017 10:02:06 CST run_dataimport INFO - Please set $ACCUMULO_HOME to the root of your Accumulo installation.\n01-11-2017 10:02:08 CST run_dataimport INFO - 17/11/01 10:02:08 INFO sqoop.Sqoop: Running Sqoop version: 1.4.6-cdh5.8.0\n01-11-2017 10:02:08 CST run_dataimport INFO - 17/11/01 10:02:08 WARN sqoop.ConnFactory: Parameter --driver is set to an explicit driver however appropriate connection manager is not being set (via --connection-manager). Sqoop is going to fall back to org.apache.sqoop.manager.GenericJdbcManager. Please specify explicitly which connection manager should be used next time.\n01-11-2017 10:02:08 CST run_dataimport INFO - 17/11/01 10:02:08 INFO manager.SqlManager: Using default fetchSize of 1000\n01-11-2017 10:02:08 CST run_dataimport INFO - 17/11/01 10:02:08 INFO tool.CodeGenTool: Beginning code generation\n01-11-2017 10:02:09 CST run_dataimport INFO - 17/11/01 10:02:09 INFO manager.SqlManager: Executing SQL statement: select top 50000 * from jzj.dbo.jzj_test where uid&gt;(select uid from (select top 50000 uid from jzj.dbo.jzj_test where uid&gt;31612700 and uid&lt;=32309400) a except select uid from (select top 49999 uid from jzj.dbo.jzj_test where uid&gt;31612700 and uid&lt;=32309400) a) and uid &lt;=32309400 and  (1 = 0) \n01-11-2017 10:02:09 CST run_dataimport INFO - 17/11/01 10:02:09 INFO manager.SqlManager: Executing SQL statement: select top 50000 * from jzj.dbo.jzj_test where uid&gt;(select uid from (select top 50000 uid from jzj.dbo.jzj_test where uid&gt;31612700 and uid&lt;=32309400) a except select uid from (select top 49999 uid from jzj.dbo.jzj_test where uid&gt;31612700 and uid&lt;=32309400) a) and uid &lt;=32309400 and  (1 = 0) \n01-11-2017 10:02:09 CST run_dataimport INFO - 17/11/01 10:02:09 INFO orm.CompilationManager: HADOOP_MAPRED_HOME is /opt/cloudera/parcels/CDH/lib/hadoop-mapreduce\n01-11-2017 10:02:11 CST run_dataimport INFO - Note: /tmp/sqoop-root/compile/9de72518c89c5280b88b044ac6fd6c80/QueryResult.java uses or overrides a deprecated API.\n01-11-2017 10:02:11 CST run_dataimport INFO - Note: Recompile with -Xlint:deprecation for details.\n01-11-2017 10:02:11 CST run_dataimport INFO - 17/11/01 10:02:11 INFO orm.CompilationManager: Writing jar file: /tmp/sqoop-root/compile/9de72518c89c5280b88b044ac6fd6c80/QueryResult.jar\n01-11-2017 10:02:12 CST run_dataimport INFO - 17/11/01 10:02:12 INFO tool.ImportTool: Maximal id query for free form incremental import: SELECT MAX(uid) FROM (select top 50000 * from jzj.dbo.jzj_test where uid&gt;(select uid from (select top 50000 uid from jzj.dbo.jzj_test where uid&gt;31612700 and uid&lt;=32309400) a except select uid from (select top 49999 uid from jzj.dbo.jzj_test where uid&gt;31612700 and uid&lt;=32309400) a) and uid &lt;=32309400 and (1 = 1)) sqoop_import_query_alias\n01-11-2017 10:04:47 CST run_dataimport INFO - 17/11/01 10:04:47 INFO tool.ImportTool: Incremental import based on column uid\n01-11-2017 10:04:47 CST run_dataimport INFO - 17/11/01 10:04:47 INFO tool.ImportTool: Upper bound value: 31712700\n01-11-2017 10:04:47 CST run_dataimport INFO - 17/11/01 10:04:47 INFO mapreduce.ImportJobBase: Beginning query import.\n01-11-2017 10:04:47 CST run_dataimport INFO - 17/11/01 10:04:47 INFO Configuration.deprecation: mapred.jar is deprecated. Instead, use mapreduce.job.jar\n01-11-2017 10:04:47 CST run_dataimport INFO - 17/11/01 10:04:47 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n01-11-2017 10:04:47 CST run_dataimport INFO - 17/11/01 10:04:47 INFO client.RMProxy: Connecting to ResourceManager at master/172.23.27.203:8032\n01-11-2017 10:04:52 CST run_dataimport INFO - 17/11/01 10:04:52 INFO db.DBInputFormat: Using read commited transaction isolation\n01-11-2017 10:04:52 CST run_dataimport INFO - 17/11/01 10:04:52 INFO db.DataDrivenDBInputFormat: BoundingValsQuery: SELECT MIN(uid), MAX(uid) FROM (select top 50000 * from jzj.dbo.jzj_test where uid&gt;(select uid from (select top 50000 uid from jzj.dbo.jzj_test where uid&gt;31612700 and uid&lt;=32309400) a except select uid from (select top 49999 uid from jzj.dbo.jzj_test where uid&gt;31612700 and uid&lt;=32309400) a) and uid &lt;=32309400 and uid &lt;= 31712700 AND  (1 = 1) ) AS t1\n01-11-2017 10:07:26 CST run_dataimport INFO - 17/11/01 10:07:26 INFO mapreduce.JobSubmitter: number of splits:4\n01-11-2017 10:07:27 CST run_dataimport INFO - 17/11/01 10:07:27 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1508671365448_1898\n01-11-2017 10:07:27 CST run_dataimport INFO - 17/11/01 10:07:27 INFO impl.YarnClientImpl: Submitted application application_1508671365448_1898\n01-11-2017 10:07:27 CST run_dataimport INFO - 17/11/01 10:07:27 INFO mapreduce.Job: The url to track the job: http://master:8088/proxy/application_1508671365448_1898/\n01-11-2017 10:07:27 CST run_dataimport INFO - 17/11/01 10:07:27 INFO mapreduce.Job: Running job: job_1508671365448_1898\n01-11-2017 10:07:34 CST run_dataimport INFO - 17/11/01 10:07:34 INFO mapreduce.Job: Job job_1508671365448_1898 running in uber mode : false\n01-11-2017 10:07:34 CST run_dataimport INFO - 17/11/01 10:07:34 INFO mapreduce.Job:  map 0% reduce 0%\n01-11-2017 10:10:17 CST run_dataimport INFO - 17/11/01 10:10:17 INFO mapreduce.Job:  map 25% reduce 0%\n01-11-2017 10:10:19 CST run_dataimport INFO - 17/11/01 10:10:19 INFO mapreduce.Job:  map 75% reduce 0%\n01-11-2017 10:10:20 CST run_dataimport INFO - 17/11/01 10:10:20 INFO mapreduce.Job:  map 100% reduce 0%\n01-11-2017 10:10:21 CST run_dataimport INFO - 17/11/01 10:10:21 INFO mapreduce.Job: Job job_1508671365448_1898 completed successfully\n01-11-2017 10:10:21 CST run_dataimport INFO - 17/11/01 10:10:21 INFO mapreduce.Job: Counters: 30\n01-11-2017 10:10:21 CST run_dataimport INFO - \tFile System Counters\n01-11-2017 10:10:21 CST run_dataimport INFO - \t\tFILE: Number of bytes read=0\n01-11-2017 10:10:21 CST run_dataimport INFO - \t\tFILE: Number of bytes written=588412\n01-11-2017 10:10:21 CST run_dataimport INFO - \t\tFILE: Number of read operations=0\n01-11-2017 10:10:21 CST run_dataimport INFO - \t\tFILE: Number of large read operations=0\n01-11-2017 10:10:21 CST run_dataimport INFO - \t\tFILE: Number of write operations=0\n01-11-2017 10:10:21 CST run_dataimport INFO - \t\tHDFS: Number of bytes read=462\n01-11-2017 10:10:21 CST run_dataimport INFO - \t\tHDFS: Number of bytes written=23450000\n01-11-2017 10:10:21 CST run_dataimport INFO - \t\tHDFS: Number of read operations=16\n01-11-2017 10:10:21 CST run_dataimport INFO - \t\tHDFS: Number of large read operations=0\n01-11-2017 10:10:21 CST run_dataimport INFO - \t\tHDFS: Number of write operations=8\n01-11-2017 10:10:21 CST run_dataimport INFO - \tJob Counters \n01-11-2017 10:10:21 CST run_dataimport INFO - \t\tLaunched map tasks=4\n01-11-2017 10:10:21 CST run_dataimport INFO - \t\tOther local map tasks=4\n01-11-2017 10:10:21 CST run_dataimport INFO - \t\tTotal time spent by all maps in occupied slots (ms)=645979\n01-11-2017 10:10:21 CST run_dataimport INFO - \t\tTotal time spent by all reduces in occupied slots (ms)=0\n01-11-2017 10:10:21 CST run_dataimport INFO - \t\tTotal time spent by all map tasks (ms)=645979\n01-11-2017 10:10:21 CST run_dataimport INFO - \t\tTotal vcore-seconds taken by all map tasks=645979\n01-11-2017 10:10:21 CST run_dataimport INFO - \t\tTotal megabyte-seconds taken by all map tasks=661482496\n01-11-2017 10:10:21 CST run_dataimport INFO - \tMap-Reduce Framework\n01-11-2017 10:10:21 CST run_dataimport INFO - \t\tMap input records=50000\n01-11-2017 10:10:21 CST run_dataimport INFO - \t\tMap output records=50000\n01-11-2017 10:10:21 CST run_dataimport INFO - \t\tInput split bytes=462\n01-11-2017 10:10:21 CST run_dataimport INFO - \t\tSpilled Records=0\n01-11-2017 10:10:21 CST run_dataimport INFO - \t\tFailed Shuffles=0\n01-11-2017 10:10:21 CST run_dataimport INFO - \t\tMerged Map outputs=0\n01-11-2017 10:10:21 CST run_dataimport INFO - \t\tGC time elapsed (ms)=299\n01-11-2017 10:10:21 CST run_dataimport INFO - \t\tCPU time spent (ms)=26430\n01-11-2017 10:10:21 CST run_dataimport INFO - \t\tPhysical memory (bytes) snapshot=1455120384\n01-11-2017 10:10:21 CST run_dataimport INFO - \t\tVirtual memory (bytes) snapshot=6527934464\n01-11-2017 10:10:21 CST run_dataimport INFO - \t\tTotal committed heap usage (bytes)=3296722944\n01-11-2017 10:10:21 CST run_dataimport INFO - \tFile Input Format Counters \n01-11-2017 10:10:21 CST run_dataimport INFO - \t\tBytes Read=0\n01-11-2017 10:10:21 CST run_dataimport INFO - \tFile Output Format Counters \n01-11-2017 10:10:21 CST run_dataimport INFO - \t\tBytes Written=23450000\n01-11-2017 10:10:21 CST run_dataimport INFO - 17/11/01 10:10:21 INFO mapreduce.ImportJobBase: Transferred 22.3637 MB in 333.7909 seconds (68.607 KB/sec)\n01-11-2017 10:10:21 CST run_dataimport INFO - 17/11/01 10:10:21 INFO mapreduce.ImportJobBase: Retrieved 50000 records.\n01-11-2017 10:10:21 CST run_dataimport INFO - 17/11/01 10:10:21 INFO util.AppendUtils: Appending to directory sql_to_hive\n01-11-2017 10:10:21 CST run_dataimport INFO - 17/11/01 10:10:21 INFO util.AppendUtils: Using found partition 2664\n01-11-2017 10:10:21 CST run_dataimport INFO - 17/11/01 10:10:21 INFO tool.ImportTool: Incremental import complete! To run another incremental import of all data following this import, supply the following arguments:\n01-11-2017 10:10:21 CST run_dataimport INFO - 17/11/01 10:10:21 INFO tool.ImportTool:  --incremental append\n01-11-2017 10:10:21 CST run_dataimport INFO - 17/11/01 10:10:21 INFO tool.ImportTool:   --check-column uid\n01-11-2017 10:10:21 CST run_dataimport INFO - 17/11/01 10:10:21 INFO tool.ImportTool:   --last-value 31712700\n01-11-2017 10:10:21 CST run_dataimport INFO - 17/11/01 10:10:21 INFO tool.ImportTool: (Consider saving this with 'sqoop job --create')\n01-11-2017 10:10:22 CST run_dataimport INFO - 12\n01-11-2017 10:10:22 CST run_dataimport INFO - Warning: /opt/cloudera/parcels/CDH-5.8.0-1.cdh5.8.0.p0.42/bin/../lib/sqoop/../accumulo does not exist! Accumulo imports will fail.\n01-11-2017 10:10:22 CST run_dataimport INFO - Please set $ACCUMULO_HOME to the root of your Accumulo installation.\n01-11-2017 10:10:24 CST run_dataimport INFO - 17/11/01 10:10:24 INFO sqoop.Sqoop: Running Sqoop version: 1.4.6-cdh5.8.0\n01-11-2017 10:10:24 CST run_dataimport INFO - 17/11/01 10:10:24 WARN sqoop.ConnFactory: Parameter --driver is set to an explicit driver however appropriate connection manager is not being set (via --connection-manager). Sqoop is going to fall back to org.apache.sqoop.manager.GenericJdbcManager. Please specify explicitly which connection manager should be used next time.\n01-11-2017 10:10:24 CST run_dataimport INFO - 17/11/01 10:10:24 INFO manager.SqlManager: Using default fetchSize of 1000\n01-11-2017 10:10:24 CST run_dataimport INFO - 17/11/01 10:10:24 INFO tool.CodeGenTool: Beginning code generation\n01-11-2017 10:10:24 CST run_dataimport INFO - 17/11/01 10:10:24 INFO manager.SqlManager: Executing SQL statement: select top 50000 * from jzj.dbo.jzj_test where uid&gt;(select uid from (select top 100000 uid from jzj.dbo.jzj_test where uid&gt;31612700 and uid&lt;=32309400) a except select uid from (select top 99999 uid from jzj.dbo.jzj_test where uid&gt;31612700 and uid&lt;=32309400) a) and uid &lt;=32309400 and  (1 = 0) \n01-11-2017 10:10:24 CST run_dataimport INFO - 17/11/01 10:10:24 INFO manager.SqlManager: Executing SQL statement: select top 50000 * from jzj.dbo.jzj_test where uid&gt;(select uid from (select top 100000 uid from jzj.dbo.jzj_test where uid&gt;31612700 and uid&lt;=32309400) a except select uid from (select top 99999 uid from jzj.dbo.jzj_test where uid&gt;31612700 and uid&lt;=32309400) a) and uid &lt;=32309400 and  (1 = 0) \n01-11-2017 10:10:25 CST run_dataimport INFO - 17/11/01 10:10:25 INFO orm.CompilationManager: HADOOP_MAPRED_HOME is /opt/cloudera/parcels/CDH/lib/hadoop-mapreduce\n01-11-2017 10:10:27 CST run_dataimport INFO - Note: /tmp/sqoop-root/compile/9f82a203e0f4d154a66e65e3115fe8a9/QueryResult.java uses or overrides a deprecated API.\n01-11-2017 10:10:27 CST run_dataimport INFO - Note: Recompile with -Xlint:deprecation for details.\n01-11-2017 10:10:27 CST run_dataimport INFO - 17/11/01 10:10:27 INFO orm.CompilationManager: Writing jar file: /tmp/sqoop-root/compile/9f82a203e0f4d154a66e65e3115fe8a9/QueryResult.jar\n01-11-2017 10:10:28 CST run_dataimport INFO - 17/11/01 10:10:28 INFO tool.ImportTool: Maximal id query for free form incremental import: SELECT MAX(uid) FROM (select top 50000 * from jzj.dbo.jzj_test where uid&gt;(select uid from (select top 100000 uid from jzj.dbo.jzj_test where uid&gt;31612700 and uid&lt;=32309400) a except select uid from (select top 99999 uid from jzj.dbo.jzj_test where uid&gt;31612700 and uid&lt;=32309400) a) and uid &lt;=32309400 and (1 = 1)) sqoop_import_query_alias\n01-11-2017 10:20:33 CST run_dataimport INFO - 17/11/01 10:20:33 INFO tool.ImportTool: Incremental import based on column uid\n01-11-2017 10:20:33 CST run_dataimport INFO - 17/11/01 10:20:33 INFO tool.ImportTool: Upper bound value: 31762700\n01-11-2017 10:20:33 CST run_dataimport INFO - 17/11/01 10:20:33 INFO mapreduce.ImportJobBase: Beginning query import.\n01-11-2017 10:20:33 CST run_dataimport INFO - 17/11/01 10:20:33 INFO Configuration.deprecation: mapred.jar is deprecated. Instead, use mapreduce.job.jar\n01-11-2017 10:20:33 CST run_dataimport INFO - 17/11/01 10:20:33 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n01-11-2017 10:20:33 CST run_dataimport INFO - 17/11/01 10:20:33 INFO client.RMProxy: Connecting to ResourceManager at master/172.23.27.203:8032\n01-11-2017 10:20:40 CST run_dataimport INFO - 17/11/01 10:20:40 INFO db.DBInputFormat: Using read commited transaction isolation\n01-11-2017 10:20:40 CST run_dataimport INFO - 17/11/01 10:20:40 INFO db.DataDrivenDBInputFormat: BoundingValsQuery: SELECT MIN(uid), MAX(uid) FROM (select top 50000 * from jzj.dbo.jzj_test where uid&gt;(select uid from (select top 100000 uid from jzj.dbo.jzj_test where uid&gt;31612700 and uid&lt;=32309400) a except select uid from (select top 99999 uid from jzj.dbo.jzj_test where uid&gt;31612700 and uid&lt;=32309400) a) and uid &lt;=32309400 and uid &lt;= 31762700 AND  (1 = 1) ) AS t1\n01-11-2017 10:30:36 CST run_dataimport INFO - 17/11/01 10:30:36 INFO mapreduce.JobSubmitter: number of splits:4\n01-11-2017 10:30:37 CST run_dataimport INFO - 17/11/01 10:30:37 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1508671365448_1899\n01-11-2017 10:30:37 CST run_dataimport INFO - 17/11/01 10:30:37 INFO impl.YarnClientImpl: Submitted application application_1508671365448_1899\n01-11-2017 10:30:37 CST run_dataimport INFO - 17/11/01 10:30:37 INFO mapreduce.Job: The url to track the job: http://master:8088/proxy/application_1508671365448_1899/\n01-11-2017 10:30:37 CST run_dataimport INFO - 17/11/01 10:30:37 INFO mapreduce.Job: Running job: job_1508671365448_1899\n01-11-2017 10:30:44 CST run_dataimport INFO - 17/11/01 10:30:44 INFO mapreduce.Job: Job job_1508671365448_1899 running in uber mode : false\n01-11-2017 10:30:44 CST run_dataimport INFO - 17/11/01 10:30:44 INFO mapreduce.Job:  map 0% reduce 0%\n01-11-2017 10:40:52 CST run_dataimport INFO - 17/11/01 10:40:52 INFO mapreduce.Job:  map 25% reduce 0%\n01-11-2017 10:40:53 CST run_dataimport INFO - 17/11/01 10:40:53 INFO mapreduce.Job:  map 50% reduce 0%\n01-11-2017 10:41:00 CST run_dataimport INFO - 17/11/01 10:41:00 INFO mapreduce.Job:  map 75% reduce 0%\n01-11-2017 10:41:02 CST run_dataimport INFO - 17/11/01 10:41:02 INFO mapreduce.Job:  map 100% reduce 0%\n01-11-2017 10:41:03 CST run_dataimport INFO - 17/11/01 10:41:03 INFO mapreduce.Job: Job job_1508671365448_1899 completed successfully\n01-11-2017 10:41:03 CST run_dataimport INFO - 17/11/01 10:41:03 INFO mapreduce.Job: Counters: 30\n01-11-2017 10:41:03 CST run_dataimport INFO - \tFile System Counters\n01-11-2017 10:41:03 CST run_dataimport INFO - \t\tFILE: Number of bytes read=0\n01-11-2017 10:41:03 CST run_dataimport INFO - \t\tFILE: Number of bytes written=588428\n01-11-2017 10:41:03 CST run_dataimport INFO - \t\tFILE: Number of read operations=0\n01-11-2017 10:41:03 CST run_dataimport INFO - \t\tFILE: Number of large read operations=0\n01-11-2017 10:41:03 CST run_dataimport INFO - \t\tFILE: Number of write operations=0\n01-11-2017 10:41:03 CST run_dataimport INFO - \t\tHDFS: Number of bytes read=462\n01-11-2017 10:41:03 CST run_dataimport INFO - \t\tHDFS: Number of bytes written=23450000\n01-11-2017 10:41:03 CST run_dataimport INFO - \t\tHDFS: Number of read operations=16\n01-11-2017 10:41:03 CST run_dataimport INFO - \t\tHDFS: Number of large read operations=0\n01-11-2017 10:41:03 CST run_dataimport INFO - \t\tHDFS: Number of write operations=8\n01-11-2017 10:41:03 CST run_dataimport INFO - \tJob Counters \n01-11-2017 10:41:03 CST run_dataimport INFO - \t\tLaunched map tasks=4\n01-11-2017 10:41:03 CST run_dataimport INFO - \t\tOther local map tasks=4\n01-11-2017 10:41:03 CST run_dataimport INFO - \t\tTotal time spent by all maps in occupied slots (ms)=2440026\n01-11-2017 10:41:03 CST run_dataimport INFO - \t\tTotal time spent by all reduces in occupied slots (ms)=0\n01-11-2017 10:41:03 CST run_dataimport INFO - \t\tTotal time spent by all map tasks (ms)=2440026\n01-11-2017 10:41:03 CST run_dataimport INFO - \t\tTotal vcore-seconds taken by all map tasks=2440026\n01-11-2017 10:41:03 CST run_dataimport INFO - \t\tTotal megabyte-seconds taken by all map tasks=2498586624\n01-11-2017 10:41:03 CST run_dataimport INFO - \tMap-Reduce Framework\n01-11-2017 10:41:03 CST run_dataimport INFO - \t\tMap input records=50000\n01-11-2017 10:41:03 CST run_dataimport INFO - \t\tMap output records=50000\n01-11-2017 10:41:03 CST run_dataimport INFO - \t\tInput split bytes=462\n01-11-2017 10:41:03 CST run_dataimport INFO - \t\tSpilled Records=0\n01-11-2017 10:41:03 CST run_dataimport INFO - \t\tFailed Shuffles=0\n01-11-2017 10:41:03 CST run_dataimport INFO - \t\tMerged Map outputs=0\n01-11-2017 10:41:03 CST run_dataimport INFO - \t\tGC time elapsed (ms)=327\n01-11-2017 10:41:03 CST run_dataimport INFO - \t\tCPU time spent (ms)=33410\n01-11-2017 10:41:03 CST run_dataimport INFO - \t\tPhysical memory (bytes) snapshot=1476767744\n01-11-2017 10:41:03 CST run_dataimport INFO - \t\tVirtual memory (bytes) snapshot=6458535936\n01-11-2017 10:41:03 CST run_dataimport INFO - \t\tTotal committed heap usage (bytes)=3296722944\n01-11-2017 10:41:03 CST run_dataimport INFO - \tFile Input Format Counters \n01-11-2017 10:41:03 CST run_dataimport INFO - \t\tBytes Read=0\n01-11-2017 10:41:03 CST run_dataimport INFO - \tFile Output Format Counters \n01-11-2017 10:41:03 CST run_dataimport INFO - \t\tBytes Written=23450000\n01-11-2017 10:41:03 CST run_dataimport INFO - 17/11/01 10:41:03 INFO mapreduce.ImportJobBase: Transferred 22.3637 MB in 1,229.8978 seconds (18.6198 KB/sec)\n01-11-2017 10:41:03 CST run_dataimport INFO - 17/11/01 10:41:03 INFO mapreduce.ImportJobBase: Retrieved 50000 records.\n01-11-2017 10:41:03 CST run_dataimport INFO - 17/11/01 10:41:03 INFO util.AppendUtils: Appending to directory sql_to_hive\n01-11-2017 10:41:03 CST run_dataimport INFO - 17/11/01 10:41:03 INFO util.AppendUtils: Using found partition 2668\n01-11-2017 10:41:04 CST run_dataimport INFO - 17/11/01 10:41:04 INFO tool.ImportTool: Incremental import complete! To run another incremental import of all data following this import, supply the following arguments:\n01-11-2017 10:41:04 CST run_dataimport INFO - 17/11/01 10:41:04 INFO tool.ImportTool:  --incremental append\n01-11-2017 10:41:04 CST run_dataimport INFO - 17/11/01 10:41:04 INFO tool.ImportTool:   --check-column uid\n01-11-2017 10:41:04 CST run_dataimport INFO - 17/11/01 10:41:04 INFO tool.ImportTool:   --last-value 31762700\n01-11-2017 10:41:04 CST run_dataimport INFO - 17/11/01 10:41:04 INFO tool.ImportTool: (Consider saving this with 'sqoop job --create')\n01-11-2017 10:41:04 CST run_dataimport INFO - 11\n01-11-2017 10:41:04 CST run_dataimport INFO - Warning: /opt/cloudera/parcels/CDH-5.8.0-1.cdh5.8.0.p0.42/bin/../lib/sqoop/../accumulo does not exist! Accumulo imports will fail.\n01-11-2017 10:41:04 CST run_dataimport INFO - Please set $ACCUMULO_HOME to the root of your Accumulo installation.\n01-11-2017 10:41:06 CST run_dataimport INFO - 17/11/01 10:41:06 INFO sqoop.Sqoop: Running Sqoop version: 1.4.6-cdh5.8.0\n01-11-2017 10:41:06 CST run_dataimport INFO - 17/11/01 10:41:06 WARN sqoop.ConnFactory: Parameter --driver is set to an explicit driver however appropriate connection manager is not being set (via --connection-manager). Sqoop is going to fall back to org.apache.sqoop.manager.GenericJdbcManager. Please specify explicitly which connection manager should be used next time.\n01-11-2017 10:41:06 CST run_dataimport INFO - 17/11/01 10:41:06 INFO manager.SqlManager: Using default fetchSize of 1000\n01-11-2017 10:41:06 CST run_dataimport INFO - 17/11/01 10:41:06 INFO tool.CodeGenTool: Beginning code generation\n01-11-2017 10:41:07 CST run_dataimport INFO - 17/11/01 10:41:07 INFO manager.SqlManager: Executing SQL statement: select top 50000 * from jzj.dbo.jzj_test where uid&gt;(select uid from (select top 150000 uid from jzj.dbo.jzj_test where uid&gt;31612700 and uid&lt;=32309400) a except select uid from (select top 149999 uid from jzj.dbo.jzj_test where uid&gt;31612700 and uid&lt;=32309400) a) and uid &lt;=32309400 and  (1 = 0) \n01-11-2017 10:41:07 CST run_dataimport INFO - 17/11/01 10:41:07 INFO manager.SqlManager: Executing SQL statement: select top 50000 * from jzj.dbo.jzj_test where uid&gt;(select uid from (select top 150000 uid from jzj.dbo.jzj_test where uid&gt;31612700 and uid&lt;=32309400) a except select uid from (select top 149999 uid from jzj.dbo.jzj_test where uid&gt;31612700 and uid&lt;=32309400) a) and uid &lt;=32309400 and  (1 = 0) \n01-11-2017 10:41:07 CST run_dataimport INFO - 17/11/01 10:41:07 INFO orm.CompilationManager: HADOOP_MAPRED_HOME is /opt/cloudera/parcels/CDH/lib/hadoop-mapreduce\n01-11-2017 10:41:09 CST run_dataimport INFO - Note: /tmp/sqoop-root/compile/61fb0da5f39ceec0b287b16e9e75fd1b/QueryResult.java uses or overrides a deprecated API.\n01-11-2017 10:41:09 CST run_dataimport INFO - Note: Recompile with -Xlint:deprecation for details.\n01-11-2017 10:41:09 CST run_dataimport INFO - 17/11/01 10:41:09 INFO orm.CompilationManager: Writing jar file: /tmp/sqoop-root/compile/61fb0da5f39ceec0b287b16e9e75fd1b/QueryResult.jar\n01-11-2017 10:41:10 CST run_dataimport INFO - 17/11/01 10:41:10 INFO tool.ImportTool: Maximal id query for free form incremental import: SELECT MAX(uid) FROM (select top 50000 * from jzj.dbo.jzj_test where uid&gt;(select uid from (select top 150000 uid from jzj.dbo.jzj_test where uid&gt;31612700 and uid&lt;=32309400) a except select uid from (select top 149999 uid from jzj.dbo.jzj_test where uid&gt;31612700 and uid&lt;=32309400) a) and uid &lt;=32309400 and (1 = 1)) sqoop_import_query_alias\n", 'length': 34468}
[2017-11-01 18:09:29,927] app.hive_mysql.AzkabanMonitor [INFO] URL: https://172.23.27.203:8443
[2017-11-01 18:09:30,105] app.hive_mysql.AzkabanMonitor [INFO] Success: Login https://172.23.27.203:8443 with user azkaban
[2017-11-01 18:09:30,105] app.hive_mysql.AzkabanMonitor [DEBUG] {'session.id': 'b037e356-0a41-4707-83a4-bf1123d888d1', 'status': 'success'}
[2017-11-01 18:09:30,116] app.hive_mysql.AzkabanMonitor [INFO] Success: Fetch Executions of a Flow - run_dataimport
[2017-11-01 18:09:30,117] app.hive_mysql.AzkabanMonitor [DEBUG] {'total': 6, 'executions': [{'submitTime': 1509501650049, 'submitUser': 'azkaban', 'startTime': 1509501650215, 'endTime': -1, 'flowId': 'run_dataimport', 'projectId': 68, 'execId': 1658, 'status': 'RUNNING'}, {'submitTime': 1509494450033, 'submitUser': 'azkaban', 'startTime': 1509494450241, 'endTime': 1509496877667, 'flowId': 'run_dataimport', 'projectId': 68, 'execId': 1657, 'status': 'SUCCEEDED'}, {'submitTime': 1509458449965, 'submitUser': 'azkaban', 'startTime': 1509458450181, 'endTime': 1509489787442, 'flowId': 'run_dataimport', 'projectId': 68, 'execId': 1656, 'status': 'SUCCEEDED'}, {'submitTime': 1509444049936, 'submitUser': 'azkaban', 'startTime': 1509444050155, 'endTime': 1509457451629, 'flowId': 'run_dataimport', 'projectId': 68, 'execId': 1655, 'status': 'SUCCEEDED'}, {'submitTime': 1509438862311, 'submitUser': 'azkaban', 'startTime': 1509438862468, 'endTime': 1509438932606, 'flowId': 'run_dataimport', 'projectId': 68, 'execId': 1654, 'status': 'KILLED'}, {'submitTime': 1509438455131, 'submitUser': 'azkaban', 'startTime': 1509438455403, 'endTime': 1509438536090, 'flowId': 'run_dataimport', 'projectId': 68, 'execId': 1653, 'status': 'KILLED'}], 'length': 10, 'project': 'sql_to_hive', 'from': 0, 'projectId': 68, 'flow': 'run_dataimport'}
[2017-11-01 18:09:30,408] app.hive_mysql.AzkabanMonitor [INFO] Success: Fetch Execution Job Logs - 1658-run_dataimport
[2017-11-01 18:09:30,409] app.hive_mysql.AzkabanMonitor [DEBUG] {'offset': 0, 'data': "01-11-2017 10:00:50 CST run_dataimport INFO - Starting job run_dataimport at 1509501650218\n01-11-2017 10:00:50 CST run_dataimport INFO - azkaban.webserver.url property was not set\n01-11-2017 10:00:50 CST run_dataimport INFO - job JVM args: -Dazkaban.flowid=run_dataimport -Dazkaban.execid=1658 -Dazkaban.jobid=run_dataimport\n01-11-2017 10:00:50 CST run_dataimport INFO - Building command job executor. \n01-11-2017 10:00:50 CST run_dataimport INFO - Memory granted for job run_dataimport\n01-11-2017 10:00:50 CST run_dataimport INFO - 1 commands to execute.\n01-11-2017 10:00:50 CST run_dataimport INFO - cwd=/home/azkaban/azkaban_bk/azkaban-exec-server-3.35.0-22-g5517d50/executions/1658\n01-11-2017 10:00:50 CST run_dataimport INFO - effective user is: azkaban\n01-11-2017 10:00:50 CST run_dataimport INFO - Command: sh /home/sql_to_hive/dataimport.sh\n01-11-2017 10:00:50 CST run_dataimport INFO - Environment variables: {JOB_OUTPUT_PROP_FILE=/home/azkaban/azkaban_bk/azkaban-exec-server-3.35.0-22-g5517d50/executions/1658/run_dataimport_output_2861162274243800284_tmp, JOB_PROP_FILE=/home/azkaban/azkaban_bk/azkaban-exec-server-3.35.0-22-g5517d50/executions/1658/run_dataimport_props_4685474620571842989_tmp, KRB5CCNAME=/tmp/krb5cc__sql_to_hive__run_dataimport__run_dataimport__1658__azkaban, JOB_NAME=run_dataimport}\n01-11-2017 10:00:50 CST run_dataimport INFO - Working directory: /home/azkaban/azkaban_bk/azkaban-exec-server-3.35.0-22-g5517d50/executions/1658\n01-11-2017 10:00:50 CST run_dataimport INFO - log4j:WARN No appenders could be found for logger (org.apache.hive.jdbc.Utils).\n01-11-2017 10:00:50 CST run_dataimport INFO - log4j:WARN Please initialize the log4j system properly.\n01-11-2017 10:00:50 CST run_dataimport INFO - log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.\n01-11-2017 10:00:50 CST run_dataimport INFO - SLF4J: Failed to load class &quot;org.slf4j.impl.StaticLoggerBinder&quot;.\n01-11-2017 10:00:50 CST run_dataimport INFO - SLF4J: Defaulting to no-operation (NOP) logger implementation\n01-11-2017 10:00:50 CST run_dataimport INFO - SLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.\n01-11-2017 10:01:33 CST run_dataimport INFO - 1,32309400,31612700,696700sqlmax:32309400\n01-11-2017 10:01:33 CST run_dataimport INFO - hivemax:31612700\n01-11-2017 10:01:33 CST run_dataimport INFO - counts:696700\n01-11-2017 10:01:33 CST run_dataimport INFO - 14,46700\n01-11-2017 10:01:33 CST run_dataimport INFO - 14\n01-11-2017 10:01:33 CST run_dataimport INFO - Warning: /opt/cloudera/parcels/CDH-5.8.0-1.cdh5.8.0.p0.42/bin/../lib/sqoop/../accumulo does not exist! Accumulo imports will fail.\n01-11-2017 10:01:33 CST run_dataimport INFO - Please set $ACCUMULO_HOME to the root of your Accumulo installation.\n01-11-2017 10:01:35 CST run_dataimport INFO - 17/11/01 10:01:35 INFO sqoop.Sqoop: Running Sqoop version: 1.4.6-cdh5.8.0\n01-11-2017 10:01:35 CST run_dataimport INFO - 17/11/01 10:01:35 WARN sqoop.ConnFactory: Parameter --driver is set to an explicit driver however appropriate connection manager is not being set (via --connection-manager). Sqoop is going to fall back to org.apache.sqoop.manager.GenericJdbcManager. Please specify explicitly which connection manager should be used next time.\n01-11-2017 10:01:35 CST run_dataimport INFO - 17/11/01 10:01:35 INFO manager.SqlManager: Using default fetchSize of 1000\n01-11-2017 10:01:35 CST run_dataimport INFO - 17/11/01 10:01:35 INFO tool.CodeGenTool: Beginning code generation\n01-11-2017 10:01:36 CST run_dataimport INFO - 17/11/01 10:01:36 INFO manager.SqlManager: Executing SQL statement: select top 50000 * from jzj.dbo.jzj_test where uid&gt;=(select uid from (select top 1 uid from jzj.dbo.jzj_test where uid&gt;31612700 and uid&lt;=32309400) a) and uid &lt;=32309400 and  (1 = 0) \n01-11-2017 10:01:36 CST run_dataimport INFO - 17/11/01 10:01:36 INFO manager.SqlManager: Executing SQL statement: select top 50000 * from jzj.dbo.jzj_test where uid&gt;=(select uid from (select top 1 uid from jzj.dbo.jzj_test where uid&gt;31612700 and uid&lt;=32309400) a) and uid &lt;=32309400 and  (1 = 0) \n01-11-2017 10:01:36 CST run_dataimport INFO - 17/11/01 10:01:36 INFO orm.CompilationManager: HADOOP_MAPRED_HOME is /opt/cloudera/parcels/CDH/lib/hadoop-mapreduce\n01-11-2017 10:01:38 CST run_dataimport INFO - Note: /tmp/sqoop-root/compile/ff02930c93108caafd8dbc9f2c064c43/QueryResult.java uses or overrides a deprecated API.\n01-11-2017 10:01:38 CST run_dataimport INFO - Note: Recompile with -Xlint:deprecation for details.\n01-11-2017 10:01:38 CST run_dataimport INFO - 17/11/01 10:01:38 INFO orm.CompilationManager: Writing jar file: /tmp/sqoop-root/compile/ff02930c93108caafd8dbc9f2c064c43/QueryResult.jar\n01-11-2017 10:01:39 CST run_dataimport INFO - 17/11/01 10:01:39 INFO tool.ImportTool: Maximal id query for free form incremental import: SELECT MAX(uid) FROM (select top 50000 * from jzj.dbo.jzj_test where uid&gt;=(select uid from (select top 1 uid from jzj.dbo.jzj_test where uid&gt;31612700 and uid&lt;=32309400) a) and uid &lt;=32309400 and (1 = 1)) sqoop_import_query_alias\n01-11-2017 10:01:39 CST run_dataimport INFO - 17/11/01 10:01:39 INFO tool.ImportTool: Incremental import based on column uid\n01-11-2017 10:01:39 CST run_dataimport INFO - 17/11/01 10:01:39 INFO tool.ImportTool: Upper bound value: 31662700\n01-11-2017 10:01:39 CST run_dataimport INFO - 17/11/01 10:01:39 INFO mapreduce.ImportJobBase: Beginning query import.\n01-11-2017 10:01:39 CST run_dataimport INFO - 17/11/01 10:01:39 INFO Configuration.deprecation: mapred.jar is deprecated. Instead, use mapreduce.job.jar\n01-11-2017 10:01:39 CST run_dataimport INFO - 17/11/01 10:01:39 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n01-11-2017 10:01:39 CST run_dataimport INFO - 17/11/01 10:01:39 INFO client.RMProxy: Connecting to ResourceManager at master/172.23.27.203:8032\n01-11-2017 10:01:45 CST run_dataimport INFO - 17/11/01 10:01:45 INFO db.DBInputFormat: Using read commited transaction isolation\n01-11-2017 10:01:45 CST run_dataimport INFO - 17/11/01 10:01:45 INFO db.DataDrivenDBInputFormat: BoundingValsQuery: SELECT MIN(uid), MAX(uid) FROM (select top 50000 * from jzj.dbo.jzj_test where uid&gt;=(select uid from (select top 1 uid from jzj.dbo.jzj_test where uid&gt;31612700 and uid&lt;=32309400) a) and uid &lt;=32309400 and uid &lt;= 31662700 AND  (1 = 1) ) AS t1\n01-11-2017 10:01:46 CST run_dataimport INFO - 17/11/01 10:01:46 INFO mapreduce.JobSubmitter: number of splits:4\n01-11-2017 10:01:46 CST run_dataimport INFO - 17/11/01 10:01:46 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1508671365448_1897\n01-11-2017 10:01:47 CST run_dataimport INFO - 17/11/01 10:01:47 INFO impl.YarnClientImpl: Submitted application application_1508671365448_1897\n01-11-2017 10:01:47 CST run_dataimport INFO - 17/11/01 10:01:47 INFO mapreduce.Job: The url to track the job: http://master:8088/proxy/application_1508671365448_1897/\n01-11-2017 10:01:47 CST run_dataimport INFO - 17/11/01 10:01:47 INFO mapreduce.Job: Running job: job_1508671365448_1897\n01-11-2017 10:01:54 CST run_dataimport INFO - 17/11/01 10:01:54 INFO mapreduce.Job: Job job_1508671365448_1897 running in uber mode : false\n01-11-2017 10:01:54 CST run_dataimport INFO - 17/11/01 10:01:54 INFO mapreduce.Job:  map 0% reduce 0%\n01-11-2017 10:02:02 CST run_dataimport INFO - 17/11/01 10:02:02 INFO mapreduce.Job:  map 25% reduce 0%\n01-11-2017 10:02:03 CST run_dataimport INFO - 17/11/01 10:02:03 INFO mapreduce.Job:  map 100% reduce 0%\n01-11-2017 10:02:05 CST run_dataimport INFO - 17/11/01 10:02:05 INFO mapreduce.Job: Job job_1508671365448_1897 completed successfully\n01-11-2017 10:02:05 CST run_dataimport INFO - 17/11/01 10:02:05 INFO mapreduce.Job: Counters: 30\n01-11-2017 10:02:05 CST run_dataimport INFO - \tFile System Counters\n01-11-2017 10:02:05 CST run_dataimport INFO - \t\tFILE: Number of bytes read=0\n01-11-2017 10:02:05 CST run_dataimport INFO - \t\tFILE: Number of bytes written=587012\n01-11-2017 10:02:05 CST run_dataimport INFO - \t\tFILE: Number of read operations=0\n01-11-2017 10:02:05 CST run_dataimport INFO - \t\tFILE: Number of large read operations=0\n01-11-2017 10:02:05 CST run_dataimport INFO - \t\tFILE: Number of write operations=0\n01-11-2017 10:02:05 CST run_dataimport INFO - \t\tHDFS: Number of bytes read=462\n01-11-2017 10:02:05 CST run_dataimport INFO - \t\tHDFS: Number of bytes written=23450000\n01-11-2017 10:02:05 CST run_dataimport INFO - \t\tHDFS: Number of read operations=16\n01-11-2017 10:02:05 CST run_dataimport INFO - \t\tHDFS: Number of large read operations=0\n01-11-2017 10:02:05 CST run_dataimport INFO - \t\tHDFS: Number of write operations=8\n01-11-2017 10:02:05 CST run_dataimport INFO - \tJob Counters \n01-11-2017 10:02:05 CST run_dataimport INFO - \t\tLaunched map tasks=4\n01-11-2017 10:02:05 CST run_dataimport INFO - \t\tOther local map tasks=4\n01-11-2017 10:02:05 CST run_dataimport INFO - \t\tTotal time spent by all maps in occupied slots (ms)=26571\n01-11-2017 10:02:05 CST run_dataimport INFO - \t\tTotal time spent by all reduces in occupied slots (ms)=0\n01-11-2017 10:02:05 CST run_dataimport INFO - \t\tTotal time spent by all map tasks (ms)=26571\n01-11-2017 10:02:05 CST run_dataimport INFO - \t\tTotal vcore-seconds taken by all map tasks=26571\n01-11-2017 10:02:05 CST run_dataimport INFO - \t\tTotal megabyte-seconds taken by all map tasks=27208704\n01-11-2017 10:02:05 CST run_dataimport INFO - \tMap-Reduce Framework\n01-11-2017 10:02:05 CST run_dataimport INFO - \t\tMap input records=50000\n01-11-2017 10:02:05 CST run_dataimport INFO - \t\tMap output records=50000\n01-11-2017 10:02:05 CST run_dataimport INFO - \t\tInput split bytes=462\n01-11-2017 10:02:05 CST run_dataimport INFO - \t\tSpilled Records=0\n01-11-2017 10:02:05 CST run_dataimport INFO - \t\tFailed Shuffles=0\n01-11-2017 10:02:05 CST run_dataimport INFO - \t\tMerged Map outputs=0\n01-11-2017 10:02:05 CST run_dataimport INFO - \t\tGC time elapsed (ms)=211\n01-11-2017 10:02:05 CST run_dataimport INFO - \t\tCPU time spent (ms)=24550\n01-11-2017 10:02:05 CST run_dataimport INFO - \t\tPhysical memory (bytes) snapshot=1433231360\n01-11-2017 10:02:05 CST run_dataimport INFO - \t\tVirtual memory (bytes) snapshot=6511280128\n01-11-2017 10:02:05 CST run_dataimport INFO - \t\tTotal committed heap usage (bytes)=3296722944\n01-11-2017 10:02:05 CST run_dataimport INFO - \tFile Input Format Counters \n01-11-2017 10:02:05 CST run_dataimport INFO - \t\tBytes Read=0\n01-11-2017 10:02:05 CST run_dataimport INFO - \tFile Output Format Counters \n01-11-2017 10:02:05 CST run_dataimport INFO - \t\tBytes Written=23450000\n01-11-2017 10:02:05 CST run_dataimport INFO - 17/11/01 10:02:05 INFO mapreduce.ImportJobBase: Transferred 22.3637 MB in 26.2622 seconds (871.9906 KB/sec)\n01-11-2017 10:02:05 CST run_dataimport INFO - 17/11/01 10:02:05 INFO mapreduce.ImportJobBase: Retrieved 50000 records.\n01-11-2017 10:02:05 CST run_dataimport INFO - 17/11/01 10:02:05 INFO util.AppendUtils: Appending to directory sql_to_hive\n01-11-2017 10:02:06 CST run_dataimport INFO - 17/11/01 10:02:06 INFO util.AppendUtils: Using found partition 2660\n01-11-2017 10:02:06 CST run_dataimport INFO - 17/11/01 10:02:06 INFO tool.ImportTool: Incremental import complete! To run another incremental import of all data following this import, supply the following arguments:\n01-11-2017 10:02:06 CST run_dataimport INFO - 17/11/01 10:02:06 INFO tool.ImportTool:  --incremental append\n01-11-2017 10:02:06 CST run_dataimport INFO - 17/11/01 10:02:06 INFO tool.ImportTool:   --check-column uid\n01-11-2017 10:02:06 CST run_dataimport INFO - 17/11/01 10:02:06 INFO tool.ImportTool:   --last-value 31662700\n01-11-2017 10:02:06 CST run_dataimport INFO - 17/11/01 10:02:06 INFO tool.ImportTool: (Consider saving this with 'sqoop job --create')\n01-11-2017 10:02:06 CST run_dataimport INFO - 13\n01-11-2017 10:02:06 CST run_dataimport INFO - Warning: /opt/cloudera/parcels/CDH-5.8.0-1.cdh5.8.0.p0.42/bin/../lib/sqoop/../accumulo does not exist! Accumulo imports will fail.\n01-11-2017 10:02:06 CST run_dataimport INFO - Please set $ACCUMULO_HOME to the root of your Accumulo installation.\n01-11-2017 10:02:08 CST run_dataimport INFO - 17/11/01 10:02:08 INFO sqoop.Sqoop: Running Sqoop version: 1.4.6-cdh5.8.0\n01-11-2017 10:02:08 CST run_dataimport INFO - 17/11/01 10:02:08 WARN sqoop.ConnFactory: Parameter --driver is set to an explicit driver however appropriate connection manager is not being set (via --connection-manager). Sqoop is going to fall back to org.apache.sqoop.manager.GenericJdbcManager. Please specify explicitly which connection manager should be used next time.\n01-11-2017 10:02:08 CST run_dataimport INFO - 17/11/01 10:02:08 INFO manager.SqlManager: Using default fetchSize of 1000\n01-11-2017 10:02:08 CST run_dataimport INFO - 17/11/01 10:02:08 INFO tool.CodeGenTool: Beginning code generation\n01-11-2017 10:02:09 CST run_dataimport INFO - 17/11/01 10:02:09 INFO manager.SqlManager: Executing SQL statement: select top 50000 * from jzj.dbo.jzj_test where uid&gt;(select uid from (select top 50000 uid from jzj.dbo.jzj_test where uid&gt;31612700 and uid&lt;=32309400) a except select uid from (select top 49999 uid from jzj.dbo.jzj_test where uid&gt;31612700 and uid&lt;=32309400) a) and uid &lt;=32309400 and  (1 = 0) \n01-11-2017 10:02:09 CST run_dataimport INFO - 17/11/01 10:02:09 INFO manager.SqlManager: Executing SQL statement: select top 50000 * from jzj.dbo.jzj_test where uid&gt;(select uid from (select top 50000 uid from jzj.dbo.jzj_test where uid&gt;31612700 and uid&lt;=32309400) a except select uid from (select top 49999 uid from jzj.dbo.jzj_test where uid&gt;31612700 and uid&lt;=32309400) a) and uid &lt;=32309400 and  (1 = 0) \n01-11-2017 10:02:09 CST run_dataimport INFO - 17/11/01 10:02:09 INFO orm.CompilationManager: HADOOP_MAPRED_HOME is /opt/cloudera/parcels/CDH/lib/hadoop-mapreduce\n01-11-2017 10:02:11 CST run_dataimport INFO - Note: /tmp/sqoop-root/compile/9de72518c89c5280b88b044ac6fd6c80/QueryResult.java uses or overrides a deprecated API.\n01-11-2017 10:02:11 CST run_dataimport INFO - Note: Recompile with -Xlint:deprecation for details.\n01-11-2017 10:02:11 CST run_dataimport INFO - 17/11/01 10:02:11 INFO orm.CompilationManager: Writing jar file: /tmp/sqoop-root/compile/9de72518c89c5280b88b044ac6fd6c80/QueryResult.jar\n01-11-2017 10:02:12 CST run_dataimport INFO - 17/11/01 10:02:12 INFO tool.ImportTool: Maximal id query for free form incremental import: SELECT MAX(uid) FROM (select top 50000 * from jzj.dbo.jzj_test where uid&gt;(select uid from (select top 50000 uid from jzj.dbo.jzj_test where uid&gt;31612700 and uid&lt;=32309400) a except select uid from (select top 49999 uid from jzj.dbo.jzj_test where uid&gt;31612700 and uid&lt;=32309400) a) and uid &lt;=32309400 and (1 = 1)) sqoop_import_query_alias\n01-11-2017 10:04:47 CST run_dataimport INFO - 17/11/01 10:04:47 INFO tool.ImportTool: Incremental import based on column uid\n01-11-2017 10:04:47 CST run_dataimport INFO - 17/11/01 10:04:47 INFO tool.ImportTool: Upper bound value: 31712700\n01-11-2017 10:04:47 CST run_dataimport INFO - 17/11/01 10:04:47 INFO mapreduce.ImportJobBase: Beginning query import.\n01-11-2017 10:04:47 CST run_dataimport INFO - 17/11/01 10:04:47 INFO Configuration.deprecation: mapred.jar is deprecated. Instead, use mapreduce.job.jar\n01-11-2017 10:04:47 CST run_dataimport INFO - 17/11/01 10:04:47 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n01-11-2017 10:04:47 CST run_dataimport INFO - 17/11/01 10:04:47 INFO client.RMProxy: Connecting to ResourceManager at master/172.23.27.203:8032\n01-11-2017 10:04:52 CST run_dataimport INFO - 17/11/01 10:04:52 INFO db.DBInputFormat: Using read commited transaction isolation\n01-11-2017 10:04:52 CST run_dataimport INFO - 17/11/01 10:04:52 INFO db.DataDrivenDBInputFormat: BoundingValsQuery: SELECT MIN(uid), MAX(uid) FROM (select top 50000 * from jzj.dbo.jzj_test where uid&gt;(select uid from (select top 50000 uid from jzj.dbo.jzj_test where uid&gt;31612700 and uid&lt;=32309400) a except select uid from (select top 49999 uid from jzj.dbo.jzj_test where uid&gt;31612700 and uid&lt;=32309400) a) and uid &lt;=32309400 and uid &lt;= 31712700 AND  (1 = 1) ) AS t1\n01-11-2017 10:07:26 CST run_dataimport INFO - 17/11/01 10:07:26 INFO mapreduce.JobSubmitter: number of splits:4\n01-11-2017 10:07:27 CST run_dataimport INFO - 17/11/01 10:07:27 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1508671365448_1898\n01-11-2017 10:07:27 CST run_dataimport INFO - 17/11/01 10:07:27 INFO impl.YarnClientImpl: Submitted application application_1508671365448_1898\n01-11-2017 10:07:27 CST run_dataimport INFO - 17/11/01 10:07:27 INFO mapreduce.Job: The url to track the job: http://master:8088/proxy/application_1508671365448_1898/\n01-11-2017 10:07:27 CST run_dataimport INFO - 17/11/01 10:07:27 INFO mapreduce.Job: Running job: job_1508671365448_1898\n01-11-2017 10:07:34 CST run_dataimport INFO - 17/11/01 10:07:34 INFO mapreduce.Job: Job job_1508671365448_1898 running in uber mode : false\n01-11-2017 10:07:34 CST run_dataimport INFO - 17/11/01 10:07:34 INFO mapreduce.Job:  map 0% reduce 0%\n01-11-2017 10:10:17 CST run_dataimport INFO - 17/11/01 10:10:17 INFO mapreduce.Job:  map 25% reduce 0%\n01-11-2017 10:10:19 CST run_dataimport INFO - 17/11/01 10:10:19 INFO mapreduce.Job:  map 75% reduce 0%\n01-11-2017 10:10:20 CST run_dataimport INFO - 17/11/01 10:10:20 INFO mapreduce.Job:  map 100% reduce 0%\n01-11-2017 10:10:21 CST run_dataimport INFO - 17/11/01 10:10:21 INFO mapreduce.Job: Job job_1508671365448_1898 completed successfully\n01-11-2017 10:10:21 CST run_dataimport INFO - 17/11/01 10:10:21 INFO mapreduce.Job: Counters: 30\n01-11-2017 10:10:21 CST run_dataimport INFO - \tFile System Counters\n01-11-2017 10:10:21 CST run_dataimport INFO - \t\tFILE: Number of bytes read=0\n01-11-2017 10:10:21 CST run_dataimport INFO - \t\tFILE: Number of bytes written=588412\n01-11-2017 10:10:21 CST run_dataimport INFO - \t\tFILE: Number of read operations=0\n01-11-2017 10:10:21 CST run_dataimport INFO - \t\tFILE: Number of large read operations=0\n01-11-2017 10:10:21 CST run_dataimport INFO - \t\tFILE: Number of write operations=0\n01-11-2017 10:10:21 CST run_dataimport INFO - \t\tHDFS: Number of bytes read=462\n01-11-2017 10:10:21 CST run_dataimport INFO - \t\tHDFS: Number of bytes written=23450000\n01-11-2017 10:10:21 CST run_dataimport INFO - \t\tHDFS: Number of read operations=16\n01-11-2017 10:10:21 CST run_dataimport INFO - \t\tHDFS: Number of large read operations=0\n01-11-2017 10:10:21 CST run_dataimport INFO - \t\tHDFS: Number of write operations=8\n01-11-2017 10:10:21 CST run_dataimport INFO - \tJob Counters \n01-11-2017 10:10:21 CST run_dataimport INFO - \t\tLaunched map tasks=4\n01-11-2017 10:10:21 CST run_dataimport INFO - \t\tOther local map tasks=4\n01-11-2017 10:10:21 CST run_dataimport INFO - \t\tTotal time spent by all maps in occupied slots (ms)=645979\n01-11-2017 10:10:21 CST run_dataimport INFO - \t\tTotal time spent by all reduces in occupied slots (ms)=0\n01-11-2017 10:10:21 CST run_dataimport INFO - \t\tTotal time spent by all map tasks (ms)=645979\n01-11-2017 10:10:21 CST run_dataimport INFO - \t\tTotal vcore-seconds taken by all map tasks=645979\n01-11-2017 10:10:21 CST run_dataimport INFO - \t\tTotal megabyte-seconds taken by all map tasks=661482496\n01-11-2017 10:10:21 CST run_dataimport INFO - \tMap-Reduce Framework\n01-11-2017 10:10:21 CST run_dataimport INFO - \t\tMap input records=50000\n01-11-2017 10:10:21 CST run_dataimport INFO - \t\tMap output records=50000\n01-11-2017 10:10:21 CST run_dataimport INFO - \t\tInput split bytes=462\n01-11-2017 10:10:21 CST run_dataimport INFO - \t\tSpilled Records=0\n01-11-2017 10:10:21 CST run_dataimport INFO - \t\tFailed Shuffles=0\n01-11-2017 10:10:21 CST run_dataimport INFO - \t\tMerged Map outputs=0\n01-11-2017 10:10:21 CST run_dataimport INFO - \t\tGC time elapsed (ms)=299\n01-11-2017 10:10:21 CST run_dataimport INFO - \t\tCPU time spent (ms)=26430\n01-11-2017 10:10:21 CST run_dataimport INFO - \t\tPhysical memory (bytes) snapshot=1455120384\n01-11-2017 10:10:21 CST run_dataimport INFO - \t\tVirtual memory (bytes) snapshot=6527934464\n01-11-2017 10:10:21 CST run_dataimport INFO - \t\tTotal committed heap usage (bytes)=3296722944\n01-11-2017 10:10:21 CST run_dataimport INFO - \tFile Input Format Counters \n01-11-2017 10:10:21 CST run_dataimport INFO - \t\tBytes Read=0\n01-11-2017 10:10:21 CST run_dataimport INFO - \tFile Output Format Counters \n01-11-2017 10:10:21 CST run_dataimport INFO - \t\tBytes Written=23450000\n01-11-2017 10:10:21 CST run_dataimport INFO - 17/11/01 10:10:21 INFO mapreduce.ImportJobBase: Transferred 22.3637 MB in 333.7909 seconds (68.607 KB/sec)\n01-11-2017 10:10:21 CST run_dataimport INFO - 17/11/01 10:10:21 INFO mapreduce.ImportJobBase: Retrieved 50000 records.\n01-11-2017 10:10:21 CST run_dataimport INFO - 17/11/01 10:10:21 INFO util.AppendUtils: Appending to directory sql_to_hive\n01-11-2017 10:10:21 CST run_dataimport INFO - 17/11/01 10:10:21 INFO util.AppendUtils: Using found partition 2664\n01-11-2017 10:10:21 CST run_dataimport INFO - 17/11/01 10:10:21 INFO tool.ImportTool: Incremental import complete! To run another incremental import of all data following this import, supply the following arguments:\n01-11-2017 10:10:21 CST run_dataimport INFO - 17/11/01 10:10:21 INFO tool.ImportTool:  --incremental append\n01-11-2017 10:10:21 CST run_dataimport INFO - 17/11/01 10:10:21 INFO tool.ImportTool:   --check-column uid\n01-11-2017 10:10:21 CST run_dataimport INFO - 17/11/01 10:10:21 INFO tool.ImportTool:   --last-value 31712700\n01-11-2017 10:10:21 CST run_dataimport INFO - 17/11/01 10:10:21 INFO tool.ImportTool: (Consider saving this with 'sqoop job --create')\n01-11-2017 10:10:22 CST run_dataimport INFO - 12\n01-11-2017 10:10:22 CST run_dataimport INFO - Warning: /opt/cloudera/parcels/CDH-5.8.0-1.cdh5.8.0.p0.42/bin/../lib/sqoop/../accumulo does not exist! Accumulo imports will fail.\n01-11-2017 10:10:22 CST run_dataimport INFO - Please set $ACCUMULO_HOME to the root of your Accumulo installation.\n01-11-2017 10:10:24 CST run_dataimport INFO - 17/11/01 10:10:24 INFO sqoop.Sqoop: Running Sqoop version: 1.4.6-cdh5.8.0\n01-11-2017 10:10:24 CST run_dataimport INFO - 17/11/01 10:10:24 WARN sqoop.ConnFactory: Parameter --driver is set to an explicit driver however appropriate connection manager is not being set (via --connection-manager). Sqoop is going to fall back to org.apache.sqoop.manager.GenericJdbcManager. Please specify explicitly which connection manager should be used next time.\n01-11-2017 10:10:24 CST run_dataimport INFO - 17/11/01 10:10:24 INFO manager.SqlManager: Using default fetchSize of 1000\n01-11-2017 10:10:24 CST run_dataimport INFO - 17/11/01 10:10:24 INFO tool.CodeGenTool: Beginning code generation\n01-11-2017 10:10:24 CST run_dataimport INFO - 17/11/01 10:10:24 INFO manager.SqlManager: Executing SQL statement: select top 50000 * from jzj.dbo.jzj_test where uid&gt;(select uid from (select top 100000 uid from jzj.dbo.jzj_test where uid&gt;31612700 and uid&lt;=32309400) a except select uid from (select top 99999 uid from jzj.dbo.jzj_test where uid&gt;31612700 and uid&lt;=32309400) a) and uid &lt;=32309400 and  (1 = 0) \n01-11-2017 10:10:24 CST run_dataimport INFO - 17/11/01 10:10:24 INFO manager.SqlManager: Executing SQL statement: select top 50000 * from jzj.dbo.jzj_test where uid&gt;(select uid from (select top 100000 uid from jzj.dbo.jzj_test where uid&gt;31612700 and uid&lt;=32309400) a except select uid from (select top 99999 uid from jzj.dbo.jzj_test where uid&gt;31612700 and uid&lt;=32309400) a) and uid &lt;=32309400 and  (1 = 0) \n01-11-2017 10:10:25 CST run_dataimport INFO - 17/11/01 10:10:25 INFO orm.CompilationManager: HADOOP_MAPRED_HOME is /opt/cloudera/parcels/CDH/lib/hadoop-mapreduce\n01-11-2017 10:10:27 CST run_dataimport INFO - Note: /tmp/sqoop-root/compile/9f82a203e0f4d154a66e65e3115fe8a9/QueryResult.java uses or overrides a deprecated API.\n01-11-2017 10:10:27 CST run_dataimport INFO - Note: Recompile with -Xlint:deprecation for details.\n01-11-2017 10:10:27 CST run_dataimport INFO - 17/11/01 10:10:27 INFO orm.CompilationManager: Writing jar file: /tmp/sqoop-root/compile/9f82a203e0f4d154a66e65e3115fe8a9/QueryResult.jar\n01-11-2017 10:10:28 CST run_dataimport INFO - 17/11/01 10:10:28 INFO tool.ImportTool: Maximal id query for free form incremental import: SELECT MAX(uid) FROM (select top 50000 * from jzj.dbo.jzj_test where uid&gt;(select uid from (select top 100000 uid from jzj.dbo.jzj_test where uid&gt;31612700 and uid&lt;=32309400) a except select uid from (select top 99999 uid from jzj.dbo.jzj_test where uid&gt;31612700 and uid&lt;=32309400) a) and uid &lt;=32309400 and (1 = 1)) sqoop_import_query_alias\n01-11-2017 10:20:33 CST run_dataimport INFO - 17/11/01 10:20:33 INFO tool.ImportTool: Incremental import based on column uid\n01-11-2017 10:20:33 CST run_dataimport INFO - 17/11/01 10:20:33 INFO tool.ImportTool: Upper bound value: 31762700\n01-11-2017 10:20:33 CST run_dataimport INFO - 17/11/01 10:20:33 INFO mapreduce.ImportJobBase: Beginning query import.\n01-11-2017 10:20:33 CST run_dataimport INFO - 17/11/01 10:20:33 INFO Configuration.deprecation: mapred.jar is deprecated. Instead, use mapreduce.job.jar\n01-11-2017 10:20:33 CST run_dataimport INFO - 17/11/01 10:20:33 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n01-11-2017 10:20:33 CST run_dataimport INFO - 17/11/01 10:20:33 INFO client.RMProxy: Connecting to ResourceManager at master/172.23.27.203:8032\n01-11-2017 10:20:40 CST run_dataimport INFO - 17/11/01 10:20:40 INFO db.DBInputFormat: Using read commited transaction isolation\n01-11-2017 10:20:40 CST run_dataimport INFO - 17/11/01 10:20:40 INFO db.DataDrivenDBInputFormat: BoundingValsQuery: SELECT MIN(uid), MAX(uid) FROM (select top 50000 * from jzj.dbo.jzj_test where uid&gt;(select uid from (select top 100000 uid from jzj.dbo.jzj_test where uid&gt;31612700 and uid&lt;=32309400) a except select uid from (select top 99999 uid from jzj.dbo.jzj_test where uid&gt;31612700 and uid&lt;=32309400) a) and uid &lt;=32309400 and uid &lt;= 31762700 AND  (1 = 1) ) AS t1\n01-11-2017 10:30:36 CST run_dataimport INFO - 17/11/01 10:30:36 INFO mapreduce.JobSubmitter: number of splits:4\n01-11-2017 10:30:37 CST run_dataimport INFO - 17/11/01 10:30:37 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1508671365448_1899\n01-11-2017 10:30:37 CST run_dataimport INFO - 17/11/01 10:30:37 INFO impl.YarnClientImpl: Submitted application application_1508671365448_1899\n01-11-2017 10:30:37 CST run_dataimport INFO - 17/11/01 10:30:37 INFO mapreduce.Job: The url to track the job: http://master:8088/proxy/application_1508671365448_1899/\n01-11-2017 10:30:37 CST run_dataimport INFO - 17/11/01 10:30:37 INFO mapreduce.Job: Running job: job_1508671365448_1899\n01-11-2017 10:30:44 CST run_dataimport INFO - 17/11/01 10:30:44 INFO mapreduce.Job: Job job_1508671365448_1899 running in uber mode : false\n01-11-2017 10:30:44 CST run_dataimport INFO - 17/11/01 10:30:44 INFO mapreduce.Job:  map 0% reduce 0%\n01-11-2017 10:40:52 CST run_dataimport INFO - 17/11/01 10:40:52 INFO mapreduce.Job:  map 25% reduce 0%\n01-11-2017 10:40:53 CST run_dataimport INFO - 17/11/01 10:40:53 INFO mapreduce.Job:  map 50% reduce 0%\n01-11-2017 10:41:00 CST run_dataimport INFO - 17/11/01 10:41:00 INFO mapreduce.Job:  map 75% reduce 0%\n01-11-2017 10:41:02 CST run_dataimport INFO - 17/11/01 10:41:02 INFO mapreduce.Job:  map 100% reduce 0%\n01-11-2017 10:41:03 CST run_dataimport INFO - 17/11/01 10:41:03 INFO mapreduce.Job: Job job_1508671365448_1899 completed successfully\n01-11-2017 10:41:03 CST run_dataimport INFO - 17/11/01 10:41:03 INFO mapreduce.Job: Counters: 30\n01-11-2017 10:41:03 CST run_dataimport INFO - \tFile System Counters\n01-11-2017 10:41:03 CST run_dataimport INFO - \t\tFILE: Number of bytes read=0\n01-11-2017 10:41:03 CST run_dataimport INFO - \t\tFILE: Number of bytes written=588428\n01-11-2017 10:41:03 CST run_dataimport INFO - \t\tFILE: Number of read operations=0\n01-11-2017 10:41:03 CST run_dataimport INFO - \t\tFILE: Number of large read operations=0\n01-11-2017 10:41:03 CST run_dataimport INFO - \t\tFILE: Number of write operations=0\n01-11-2017 10:41:03 CST run_dataimport INFO - \t\tHDFS: Number of bytes read=462\n01-11-2017 10:41:03 CST run_dataimport INFO - \t\tHDFS: Number of bytes written=23450000\n01-11-2017 10:41:03 CST run_dataimport INFO - \t\tHDFS: Number of read operations=16\n01-11-2017 10:41:03 CST run_dataimport INFO - \t\tHDFS: Number of large read operations=0\n01-11-2017 10:41:03 CST run_dataimport INFO - \t\tHDFS: Number of write operations=8\n01-11-2017 10:41:03 CST run_dataimport INFO - \tJob Counters \n01-11-2017 10:41:03 CST run_dataimport INFO - \t\tLaunched map tasks=4\n01-11-2017 10:41:03 CST run_dataimport INFO - \t\tOther local map tasks=4\n01-11-2017 10:41:03 CST run_dataimport INFO - \t\tTotal time spent by all maps in occupied slots (ms)=2440026\n01-11-2017 10:41:03 CST run_dataimport INFO - \t\tTotal time spent by all reduces in occupied slots (ms)=0\n01-11-2017 10:41:03 CST run_dataimport INFO - \t\tTotal time spent by all map tasks (ms)=2440026\n01-11-2017 10:41:03 CST run_dataimport INFO - \t\tTotal vcore-seconds taken by all map tasks=2440026\n01-11-2017 10:41:03 CST run_dataimport INFO - \t\tTotal megabyte-seconds taken by all map tasks=2498586624\n01-11-2017 10:41:03 CST run_dataimport INFO - \tMap-Reduce Framework\n01-11-2017 10:41:03 CST run_dataimport INFO - \t\tMap input records=50000\n01-11-2017 10:41:03 CST run_dataimport INFO - \t\tMap output records=50000\n01-11-2017 10:41:03 CST run_dataimport INFO - \t\tInput split bytes=462\n01-11-2017 10:41:03 CST run_dataimport INFO - \t\tSpilled Records=0\n01-11-2017 10:41:03 CST run_dataimport INFO - \t\tFailed Shuffles=0\n01-11-2017 10:41:03 CST run_dataimport INFO - \t\tMerged Map outputs=0\n01-11-2017 10:41:03 CST run_dataimport INFO - \t\tGC time elapsed (ms)=327\n01-11-2017 10:41:03 CST run_dataimport INFO - \t\tCPU time spent (ms)=33410\n01-11-2017 10:41:03 CST run_dataimport INFO - \t\tPhysical memory (bytes) snapshot=1476767744\n01-11-2017 10:41:03 CST run_dataimport INFO - \t\tVirtual memory (bytes) snapshot=6458535936\n01-11-2017 10:41:03 CST run_dataimport INFO - \t\tTotal committed heap usage (bytes)=3296722944\n01-11-2017 10:41:03 CST run_dataimport INFO - \tFile Input Format Counters \n01-11-2017 10:41:03 CST run_dataimport INFO - \t\tBytes Read=0\n01-11-2017 10:41:03 CST run_dataimport INFO - \tFile Output Format Counters \n01-11-2017 10:41:03 CST run_dataimport INFO - \t\tBytes Written=23450000\n01-11-2017 10:41:03 CST run_dataimport INFO - 17/11/01 10:41:03 INFO mapreduce.ImportJobBase: Transferred 22.3637 MB in 1,229.8978 seconds (18.6198 KB/sec)\n01-11-2017 10:41:03 CST run_dataimport INFO - 17/11/01 10:41:03 INFO mapreduce.ImportJobBase: Retrieved 50000 records.\n01-11-2017 10:41:03 CST run_dataimport INFO - 17/11/01 10:41:03 INFO util.AppendUtils: Appending to directory sql_to_hive\n01-11-2017 10:41:03 CST run_dataimport INFO - 17/11/01 10:41:03 INFO util.AppendUtils: Using found partition 2668\n01-11-2017 10:41:04 CST run_dataimport INFO - 17/11/01 10:41:04 INFO tool.ImportTool: Incremental import complete! To run another incremental import of all data following this import, supply the following arguments:\n01-11-2017 10:41:04 CST run_dataimport INFO - 17/11/01 10:41:04 INFO tool.ImportTool:  --incremental append\n01-11-2017 10:41:04 CST run_dataimport INFO - 17/11/01 10:41:04 INFO tool.ImportTool:   --check-column uid\n01-11-2017 10:41:04 CST run_dataimport INFO - 17/11/01 10:41:04 INFO tool.ImportTool:   --last-value 31762700\n01-11-2017 10:41:04 CST run_dataimport INFO - 17/11/01 10:41:04 INFO tool.ImportTool: (Consider saving this with 'sqoop job --create')\n01-11-2017 10:41:04 CST run_dataimport INFO - 11\n01-11-2017 10:41:04 CST run_dataimport INFO - Warning: /opt/cloudera/parcels/CDH-5.8.0-1.cdh5.8.0.p0.42/bin/../lib/sqoop/../accumulo does not exist! Accumulo imports will fail.\n01-11-2017 10:41:04 CST run_dataimport INFO - Please set $ACCUMULO_HOME to the root of your Accumulo installation.\n01-11-2017 10:41:06 CST run_dataimport INFO - 17/11/01 10:41:06 INFO sqoop.Sqoop: Running Sqoop version: 1.4.6-cdh5.8.0\n01-11-2017 10:41:06 CST run_dataimport INFO - 17/11/01 10:41:06 WARN sqoop.ConnFactory: Parameter --driver is set to an explicit driver however appropriate connection manager is not being set (via --connection-manager). Sqoop is going to fall back to org.apache.sqoop.manager.GenericJdbcManager. Please specify explicitly which connection manager should be used next time.\n01-11-2017 10:41:06 CST run_dataimport INFO - 17/11/01 10:41:06 INFO manager.SqlManager: Using default fetchSize of 1000\n01-11-2017 10:41:06 CST run_dataimport INFO - 17/11/01 10:41:06 INFO tool.CodeGenTool: Beginning code generation\n01-11-2017 10:41:07 CST run_dataimport INFO - 17/11/01 10:41:07 INFO manager.SqlManager: Executing SQL statement: select top 50000 * from jzj.dbo.jzj_test where uid&gt;(select uid from (select top 150000 uid from jzj.dbo.jzj_test where uid&gt;31612700 and uid&lt;=32309400) a except select uid from (select top 149999 uid from jzj.dbo.jzj_test where uid&gt;31612700 and uid&lt;=32309400) a) and uid &lt;=32309400 and  (1 = 0) \n01-11-2017 10:41:07 CST run_dataimport INFO - 17/11/01 10:41:07 INFO manager.SqlManager: Executing SQL statement: select top 50000 * from jzj.dbo.jzj_test where uid&gt;(select uid from (select top 150000 uid from jzj.dbo.jzj_test where uid&gt;31612700 and uid&lt;=32309400) a except select uid from (select top 149999 uid from jzj.dbo.jzj_test where uid&gt;31612700 and uid&lt;=32309400) a) and uid &lt;=32309400 and  (1 = 0) \n01-11-2017 10:41:07 CST run_dataimport INFO - 17/11/01 10:41:07 INFO orm.CompilationManager: HADOOP_MAPRED_HOME is /opt/cloudera/parcels/CDH/lib/hadoop-mapreduce\n01-11-2017 10:41:09 CST run_dataimport INFO - Note: /tmp/sqoop-root/compile/61fb0da5f39ceec0b287b16e9e75fd1b/QueryResult.java uses or overrides a deprecated API.\n01-11-2017 10:41:09 CST run_dataimport INFO - Note: Recompile with -Xlint:deprecation for details.\n01-11-2017 10:41:09 CST run_dataimport INFO - 17/11/01 10:41:09 INFO orm.CompilationManager: Writing jar file: /tmp/sqoop-root/compile/61fb0da5f39ceec0b287b16e9e75fd1b/QueryResult.jar\n01-11-2017 10:41:10 CST run_dataimport INFO - 17/11/01 10:41:10 INFO tool.ImportTool: Maximal id query for free form incremental import: SELECT MAX(uid) FROM (select top 50000 * from jzj.dbo.jzj_test where uid&gt;(select uid from (select top 150000 uid from jzj.dbo.jzj_test where uid&gt;31612700 and uid&lt;=32309400) a except select uid from (select top 149999 uid from jzj.dbo.jzj_test where uid&gt;31612700 and uid&lt;=32309400) a) and uid &lt;=32309400 and (1 = 1)) sqoop_import_query_alias\n01-11-2017 11:03:41 CST run_dataimport INFO - 17/11/01 11:03:41 INFO tool.ImportTool: Incremental import based on column uid\n01-11-2017 11:03:41 CST run_dataimport INFO - 17/11/01 11:03:41 INFO tool.ImportTool: Upper bound value: 31812700\n01-11-2017 11:03:41 CST run_dataimport INFO - 17/11/01 11:03:41 INFO mapreduce.ImportJobBase: Beginning query import.\n01-11-2017 11:03:41 CST run_dataimport INFO - 17/11/01 11:03:41 INFO Configuration.deprecation: mapred.jar is deprecated. Instead, use mapreduce.job.jar\n01-11-2017 11:03:41 CST run_dataimport INFO - 17/11/01 11:03:41 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n01-11-2017 11:03:41 CST run_dataimport INFO - 17/11/01 11:03:41 INFO client.RMProxy: Connecting to ResourceManager at master/172.23.27.203:8032\n01-11-2017 11:03:48 CST run_dataimport INFO - 17/11/01 11:03:48 INFO db.DBInputFormat: Using read commited transaction isolation\n01-11-2017 11:03:48 CST run_dataimport INFO - 17/11/01 11:03:48 INFO db.DataDrivenDBInputFormat: BoundingValsQuery: SELECT MIN(uid), MAX(uid) FROM (select top 50000 * from jzj.dbo.jzj_test where uid&gt;(select uid from (select top 150000 uid from jzj.dbo.jzj_test where uid&gt;31612700 and uid&lt;=32309400) a except select uid from (select top 149999 uid from jzj.dbo.jzj_test where uid&gt;31612700 and uid&lt;=32309400) a) and uid &lt;=32309400 and uid &lt;= 31812700 AND  (1 = 1) ) AS t1\n01-11-2017 11:26:03 CST run_dataimport INFO - 17/11/01 11:26:03 INFO mapreduce.JobSubmitter: number of splits:4\n01-11-2017 11:26:04 CST run_dataimport INFO - 17/11/01 11:26:04 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1508671365448_1900\n01-11-2017 11:26:04 CST run_dataimport INFO - 17/11/01 11:26:04 INFO impl.YarnClientImpl: Submitted application application_1508671365448_1900\n01-11-2017 11:26:04 CST run_dataimport INFO - 17/11/01 11:26:04 INFO mapreduce.Job: The url to track the job: http://master:8088/proxy/application_1508671365448_1900/\n01-11-2017 11:26:04 CST run_dataimport INFO - 17/11/01 11:26:04 INFO mapreduce.Job: Running job: job_1508671365448_1900\n01-11-2017 11:26:11 CST run_dataimport INFO - 17/11/01 11:26:11 INFO mapreduce.Job: Job job_1508671365448_1900 running in uber mode : false\n01-11-2017 11:26:11 CST run_dataimport INFO - 17/11/01 11:26:11 INFO mapreduce.Job:  map 0% reduce 0%\n01-11-2017 11:48:46 CST run_dataimport INFO - 17/11/01 11:48:46 INFO mapreduce.Job:  map 50% reduce 0%\n01-11-2017 11:48:59 CST run_dataimport INFO - 17/11/01 11:48:59 INFO mapreduce.Job:  map 75% reduce 0%\n01-11-2017 11:49:01 CST run_dataimport INFO - 17/11/01 11:49:01 INFO mapreduce.Job:  map 100% reduce 0%\n01-11-2017 11:49:02 CST run_dataimport INFO - 17/11/01 11:49:02 INFO mapreduce.Job: Job job_1508671365448_1900 completed successfully\n01-11-2017 11:49:02 CST run_dataimport INFO - 17/11/01 11:49:02 INFO mapreduce.Job: Counters: 30\n01-11-2017 11:49:02 CST run_dataimport INFO - \tFile System Counters\n01-11-2017 11:49:02 CST run_dataimport INFO - \t\tFILE: Number of bytes read=0\n01-11-2017 11:49:02 CST run_dataimport INFO - \t\tFILE: Number of bytes written=588292\n01-11-2017 11:49:02 CST run_dataimport INFO - \t\tFILE: Number of read operations=0\n01-11-2017 11:49:02 CST run_dataimport INFO - \t\tFILE: Number of large read operations=0\n01-11-2017 11:49:02 CST run_dataimport INFO - \t\tFILE: Number of write operations=0\n01-11-2017 11:49:02 CST run_dataimport INFO - \t\tHDFS: Number of bytes read=462\n01-11-2017 11:49:02 CST run_dataimport INFO - \t\tHDFS: Number of bytes written=23450000\n01-11-2017 11:49:02 CST run_dataimport INFO - \t\tHDFS: Number of read operations=16\n01-11-2017 11:49:02 CST run_dataimport INFO - \t\tHDFS: Number of large read operations=0\n01-11-2017 11:49:02 CST run_dataimport INFO - \t\tHDFS: Number of write operations=8\n01-11-2017 11:49:02 CST run_dataimport INFO - \tJob Counters \n01-11-2017 11:49:02 CST run_dataimport INFO - \t\tLaunched map tasks=4\n01-11-2017 11:49:02 CST run_dataimport INFO - \t\tOther local map tasks=4\n01-11-2017 11:49:02 CST run_dataimport INFO - \t\tTotal time spent by all maps in occupied slots (ms)=5434687\n01-11-2017 11:49:02 CST run_dataimport INFO - \t\tTotal time spent by all reduces in occupied slots (ms)=0\n01-11-2017 11:49:02 CST run_dataimport INFO - \t\tTotal time spent by all map tasks (ms)=5434687\n01-11-2017 11:49:02 CST run_dataimport INFO - \t\tTotal vcore-seconds taken by all map tasks=5434687\n01-11-2017 11:49:02 CST run_dataimport INFO - \t\tTotal megabyte-seconds taken by all map tasks=5565119488\n01-11-2017 11:49:02 CST run_dataimport INFO - \tMap-Reduce Framework\n01-11-2017 11:49:02 CST run_dataimport INFO - \t\tMap input records=50000\n01-11-2017 11:49:02 CST run_dataimport INFO - \t\tMap output records=50000\n01-11-2017 11:49:02 CST run_dataimport INFO - \t\tInput split bytes=462\n01-11-2017 11:49:02 CST run_dataimport INFO - \t\tSpilled Records=0\n01-11-2017 11:49:02 CST run_dataimport INFO - \t\tFailed Shuffles=0\n01-11-2017 11:49:02 CST run_dataimport INFO - \t\tMerged Map outputs=0\n01-11-2017 11:49:02 CST run_dataimport INFO - \t\tGC time elapsed (ms)=480\n01-11-2017 11:49:02 CST run_dataimport INFO - \t\tCPU time spent (ms)=43540\n01-11-2017 11:49:02 CST run_dataimport INFO - \t\tPhysical memory (bytes) snapshot=1492488192\n01-11-2017 11:49:02 CST run_dataimport INFO - \t\tVirtual memory (bytes) snapshot=6561157120\n01-11-2017 11:49:02 CST run_dataimport INFO - \t\tTotal committed heap usage (bytes)=3159883776\n01-11-2017 11:49:02 CST run_dataimport INFO - \tFile Input Format Counters \n01-11-2017 11:49:02 CST run_dataimport INFO - \t\tBytes Read=0\n01-11-2017 11:49:02 CST run_dataimport INFO - \tFile Output Format Counters \n01-11-2017 11:49:02 CST run_dataimport INFO - \t\tBytes Written=23450000\n01-11-2017 11:49:02 CST run_dataimport INFO - 17/11/01 11:49:02 INFO mapreduce.ImportJobBase: Transferred 22.3637 MB in 2,720.5523 seconds (8.4176 KB/sec)\n01-11-2017 11:49:02 CST run_dataimport INFO - 17/11/01 11:49:02 INFO mapreduce.ImportJobBase: Retrieved 50000 records.\n01-11-2017 11:49:02 CST run_dataimport INFO - 17/11/01 11:49:02 INFO util.AppendUtils: Appending to directory sql_to_hive\n01-11-2017 11:49:02 CST run_dataimport INFO - 17/11/01 11:49:02 INFO util.AppendUtils: Using found partition 2672\n01-11-2017 11:49:02 CST run_dataimport INFO - 17/11/01 11:49:02 INFO tool.ImportTool: Incremental import complete! To run another incremental import of all data following this import, supply the following arguments:\n01-11-2017 11:49:02 CST run_dataimport INFO - 17/11/01 11:49:02 INFO tool.ImportTool:  --incremental append\n01-11-2017 11:49:02 CST run_dataimport INFO - 17/11/01 11:49:02 INFO tool.ImportTool:   --check-column uid\n01-11-2017 11:49:02 CST run_dataimport INFO - 17/11/01 11:49:02 INFO tool.ImportTool:   --last-value 31812700\n01-11-2017 11:49:02 CST run_dataimport INFO - 17/11/01 11:49:02 INFO tool.ImportTool: (Consider saving this with 'sqoop job --create')\n01-11-2017 11:49:03 CST run_dataimport INFO - 10\n01-11-2017 11:49:03 CST run_dataimport INFO - Warning: /opt/cloudera/parcels/CDH-5.8.0-1.cdh5.8.0.p0.42/bin/../lib/sqoop/../accumulo does not exist! Accumulo imports will fail.\n01-11-2017 11:49:03 CST run_dataimport INFO - Please set $ACCUMULO_HOME to the root of your Accumulo installation.\n01-11-2017 11:49:05 CST run_dataimport INFO - 17/11/01 11:49:05 INFO sqoop.Sqoop: Running Sqoop version: 1.4.6-cdh5.8.0\n01-11-2017 11:49:05 CST run_dataimport INFO - 17/11/01 11:49:05 WARN sqoop.ConnFactory: Parameter --driver is set to an explicit driver however appropriate connection manager is not being set (via --connection-manager). Sqoop is going to fall back to org.apache.sqoop.manager.GenericJdbcManager. Please specify explicitly which connection manager should be used next time.\n01-11-2017 11:49:05 CST run_dataimport INFO - 17/11/01 11:49:05 INFO manager.SqlManager: Using default fetchSize of 1000\n01-11-2017 11:49:05 CST run_dataimport INFO - 17/11/01 11:49:05 INFO tool.CodeGenTool: Beginning code generation\n01-11-2017 11:49:05 CST run_dataimport INFO - 17/11/01 11:49:05 INFO manager.SqlManager: Executing SQL statement: select top 50000 * from jzj.dbo.jzj_test where uid&gt;(select uid from (select top 200000 uid from jzj.dbo.jzj_test where uid&gt;31612700 and uid&lt;=32309400) a except select uid from (select top 199999 uid from jzj.dbo.jzj_test where uid&gt;31612700 and uid&lt;=32309400) a) and uid &lt;=32309400 and  (1 = 0) \n01-11-2017 11:49:05 CST run_dataimport INFO - 17/11/01 11:49:05 INFO manager.SqlManager: Executing SQL statement: select top 50000 * from jzj.dbo.jzj_test where uid&gt;(select uid from (select top 200000 uid from jzj.dbo.jzj_test where uid&gt;31612700 and uid&lt;=32309400) a except select uid from (select top 199999 uid from jzj.dbo.jzj_test where uid&gt;31612700 and uid&lt;=32309400) a) and uid &lt;=32309400 and  (1 = 0) \n01-11-2017 11:49:05 CST run_dataimport INFO - 17/11/01 11:49:05 INFO orm.CompilationManager: HADOOP_MAPRED_HOME is /opt/cloudera/parcels/CDH/lib/hadoop-mapreduce\n01-11-2017 11:49:08 CST run_dataimport INFO - Note: /tmp/sqoop-root/compile/06b95474064ce20f94a9925c1e1a53da/QueryResult.java uses or overrides a deprecated API.\n01-11-2017 11:49:08 CST run_dataimport INFO - Note: Recompile with -Xlint:deprecation for details.\n01-11-2017 11:49:08 CST run_dataimport INFO - 17/11/01 11:49:08 INFO orm.CompilationManager: Writing jar file: /tmp/sqoop-root/compile/06b95474064ce20f94a9925c1e1a53da/QueryResult.jar\n01-11-2017 11:49:09 CST run_dataimport INFO - 17/11/01 11:49:09 INFO tool.ImportTool: Maximal id query for free form incremental import: SELECT MAX(uid) FROM (select top 50000 * from jzj.dbo.jzj_test where uid&gt;(select uid from (select top 200000 uid from jzj.dbo.jzj_test where uid&gt;31612700 and uid&lt;=32309400) a except select uid from (select top 199999 uid from jzj.dbo.jzj_test where uid&gt;31612700 and uid&lt;=32309400) a) and uid &lt;=32309400 and (1 = 1)) sqoop_import_query_alias\n01-11-2017 12:29:01 CST run_dataimport INFO - 17/11/01 12:29:01 INFO tool.ImportTool: Incremental import based on column uid\n01-11-2017 12:29:01 CST run_dataimport INFO - 17/11/01 12:29:01 INFO tool.ImportTool: Upper bound value: 31862700\n01-11-2017 12:29:01 CST run_dataimport INFO - 17/11/01 12:29:01 INFO mapreduce.ImportJobBase: Beginning query import.\n01-11-2017 12:29:02 CST run_dataimport INFO - 17/11/01 12:29:02 INFO Configuration.deprecation: mapred.jar is deprecated. Instead, use mapreduce.job.jar\n01-11-2017 12:29:02 CST run_dataimport INFO - 17/11/01 12:29:02 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n01-11-2017 12:29:02 CST run_dataimport INFO - 17/11/01 12:29:02 INFO client.RMProxy: Connecting to ResourceManager at master/172.23.27.203:8032\n01-11-2017 12:29:07 CST run_dataimport INFO - 17/11/01 12:29:07 INFO db.DBInputFormat: Using read commited transaction isolation\n01-11-2017 12:29:07 CST run_dataimport INFO - 17/11/01 12:29:07 INFO db.DataDrivenDBInputFormat: BoundingValsQuery: SELECT MIN(uid), MAX(uid) FROM (select top 50000 * from jzj.dbo.jzj_test where uid&gt;(select uid from (select top 200000 uid from jzj.dbo.jzj_test where uid&gt;31612700 and uid&lt;=32309400) a except select uid from (select top 199999 uid from jzj.dbo.jzj_test where uid&gt;31612700 and uid&lt;=32309400) a) and uid &lt;=32309400 and uid &lt;= 31862700 AND  (1 = 1) ) AS t1\n01-11-2017 13:08:33 CST run_dataimport INFO - 17/11/01 13:08:33 INFO mapreduce.JobSubmitter: number of splits:4\n01-11-2017 13:08:33 CST run_dataimport INFO - 17/11/01 13:08:33 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1508671365448_1901\n01-11-2017 13:08:34 CST run_dataimport INFO - 17/11/01 13:08:34 INFO impl.YarnClientImpl: Submitted application application_1508671365448_1901\n01-11-2017 13:08:34 CST run_dataimport INFO - 17/11/01 13:08:34 INFO mapreduce.Job: The url to track the job: http://master:8088/proxy/application_1508671365448_1901/\n01-11-2017 13:08:34 CST run_dataimport INFO - 17/11/01 13:08:34 INFO mapreduce.Job: Running job: job_1508671365448_1901\n01-11-2017 13:08:41 CST run_dataimport INFO - 17/11/01 13:08:41 INFO mapreduce.Job: Job job_1508671365448_1901 running in uber mode : false\n01-11-2017 13:08:41 CST run_dataimport INFO - 17/11/01 13:08:41 INFO mapreduce.Job:  map 0% reduce 0%\n01-11-2017 13:48:22 CST run_dataimport INFO - 17/11/01 13:48:22 INFO mapreduce.Job:  map 25% reduce 0%\n01-11-2017 13:48:23 CST run_dataimport INFO - 17/11/01 13:48:23 INFO mapreduce.Job:  map 50% reduce 0%\n01-11-2017 13:48:53 CST run_dataimport INFO - 17/11/01 13:48:53 INFO mapreduce.Job:  map 75% reduce 0%\n01-11-2017 13:48:56 CST run_dataimport INFO - 17/11/01 13:48:56 INFO mapreduce.Job:  map 100% reduce 0%\n01-11-2017 13:48:56 CST run_dataimport INFO - 17/11/01 13:48:56 INFO mapreduce.Job: Job job_1508671365448_1901 completed successfully\n01-11-2017 13:48:57 CST run_dataimport INFO - 17/11/01 13:48:57 INFO mapreduce.Job: Counters: 30\n01-11-2017 13:48:57 CST run_dataimport INFO - \tFile System Counters\n01-11-2017 13:48:57 CST run_dataimport INFO - \t\tFILE: Number of bytes read=0\n01-11-2017 13:48:57 CST run_dataimport INFO - \t\tFILE: Number of bytes written=588432\n01-11-2017 13:48:57 CST run_dataimport INFO - \t\tFILE: Number of read operations=0\n01-11-2017 13:48:57 CST run_dataimport INFO - \t\tFILE: Number of large read operations=0\n01-11-2017 13:48:57 CST run_dataimport INFO - \t\tFILE: Number of write operations=0\n01-11-2017 13:48:57 CST run_dataimport INFO - \t\tHDFS: Number of bytes read=462\n01-11-2017 13:48:57 CST run_dataimport INFO - \t\tHDFS: Number of bytes written=23450000\n01-11-2017 13:48:57 CST run_dataimport INFO - \t\tHDFS: Number of read operations=16\n01-11-2017 13:48:57 CST run_dataimport INFO - \t\tHDFS: Number of large read operations=0\n01-11-2017 13:48:57 CST run_dataimport INFO - \t\tHDFS: Number of write operations=8\n01-11-2017 13:48:57 CST run_dataimport INFO - \tJob Counters \n01-11-2017 13:48:57 CST run_dataimport INFO - \t\tLaunched map tasks=4\n01-11-2017 13:48:57 CST run_dataimport INFO - \t\tOther local map tasks=4\n01-11-2017 13:48:57 CST run_dataimport INFO - \t\tTotal time spent by all maps in occupied slots (ms)=9580793\n01-11-2017 13:48:57 CST run_dataimport INFO - \t\tTotal time spent by all reduces in occupied slots (ms)=0\n01-11-2017 13:48:57 CST run_dataimport INFO - \t\tTotal time spent by all map tasks (ms)=9580793\n01-11-2017 13:48:57 CST run_dataimport INFO - \t\tTotal vcore-seconds taken by all map tasks=9580793\n01-11-2017 13:48:57 CST run_dataimport INFO - \t\tTotal megabyte-seconds taken by all map tasks=9810732032\n01-11-2017 13:48:57 CST run_dataimport INFO - \tMap-Reduce Framework\n01-11-2017 13:48:57 CST run_dataimport INFO - \t\tMap input records=50000\n01-11-2017 13:48:57 CST run_dataimport INFO - \t\tMap output records=50000\n01-11-2017 13:48:57 CST run_dataimport INFO - \t\tInput split bytes=462\n01-11-2017 13:48:57 CST run_dataimport INFO - \t\tSpilled Records=0\n01-11-2017 13:48:57 CST run_dataimport INFO - \t\tFailed Shuffles=0\n01-11-2017 13:48:57 CST run_dataimport INFO - \t\tMerged Map outputs=0\n01-11-2017 13:48:57 CST run_dataimport INFO - \t\tGC time elapsed (ms)=709\n01-11-2017 13:48:57 CST run_dataimport INFO - \t\tCPU time spent (ms)=55790\n01-11-2017 13:48:57 CST run_dataimport INFO - \t\tPhysical memory (bytes) snapshot=1535881216\n01-11-2017 13:48:57 CST run_dataimport INFO - \t\tVirtual memory (bytes) snapshot=6505910272\n01-11-2017 13:48:57 CST run_dataimport INFO - \t\tTotal committed heap usage (bytes)=3100639232\n01-11-2017 13:48:57 CST run_dataimport INFO - \tFile Input Format Counters \n01-11-2017 13:48:57 CST run_dataimport INFO - \t\tBytes Read=0\n01-11-2017 13:48:57 CST run_dataimport INFO - \tFile Output Format Counters \n01-11-2017 13:48:57 CST run_dataimport INFO - \t\tBytes Written=23450000\n01-11-2017 13:48:57 CST run_dataimport INFO - 17/11/01 13:48:57 INFO mapreduce.ImportJobBase: Transferred 22.3637 MB in 4,795.0252 seconds (4.7759 KB/sec)\n01-11-2017 13:48:57 CST run_dataimport INFO - 17/11/01 13:48:57 INFO mapreduce.ImportJobBase: Retrieved 50000 records.\n01-11-2017 13:48:57 CST run_dataimport INFO - 17/11/01 13:48:57 INFO util.AppendUtils: Appending to directory sql_to_hive\n01-11-2017 13:48:57 CST run_dataimport INFO - 17/11/01 13:48:57 INFO util.AppendUtils: Using found partition 2676\n01-11-2017 13:48:57 CST run_dataimport INFO - 17/11/01 13:48:57 INFO tool.ImportTool: Incremental import complete! To run another incremental import of all data following this import, supply the following arguments:\n01-11-2017 13:48:57 CST run_dataimport INFO - 17/11/01 13:48:57 INFO tool.ImportTool:  --incremental append\n01-11-2017 13:48:57 CST run_dataimport INFO - 17/11/01 13:48:57 INFO tool.ImportTool:   --check-column uid\n01-11-2017 13:48:57 CST run_dataimport INFO - 17/11/01 13:48:57 INFO tool.ImportTool:   --last-value 31862700\n01-11-2017 13:48:57 CST run_dataimport INFO - 17/11/01 13:48:57 INFO tool.ImportTool: (Consider saving this with 'sqoop job --create')\n01-11-2017 13:48:57 CST run_dataimport INFO - 9\n01-11-2017 13:48:57 CST run_dataimport INFO - Warning: /opt/cloudera/parcels/CDH-5.8.0-1.cdh5.8.0.p0.42/bin/../lib/sqoop/../accumulo does not exist! Accumulo imports will fail.\n01-11-2017 13:48:57 CST run_dataimport INFO - Please set $ACCUMULO_HOME to the root of your Accumulo installation.\n01-11-2017 13:48:59 CST run_dataimport INFO - 17/11/01 13:48:59 INFO sqoop.Sqoop: Running Sqoop version: 1.4.6-cdh5.8.0\n01-11-2017 13:48:59 CST run_dataimport INFO - 17/11/01 13:48:59 WARN sqoop.ConnFactory: Parameter --driver is set to an explicit driver however appropriate connection manager is not being set (via --connection-manager). Sqoop is going to fall back to org.apache.sqoop.manager.GenericJdbcManager. Please specify explicitly which connection manager should be used next time.\n01-11-2017 13:48:59 CST run_dataimport INFO - 17/11/01 13:48:59 INFO manager.SqlManager: Using default fetchSize of 1000\n01-11-2017 13:48:59 CST run_dataimport INFO - 17/11/01 13:48:59 INFO tool.CodeGenTool: Beginning code generation\n01-11-2017 13:49:00 CST run_dataimport INFO - 17/11/01 13:49:00 INFO manager.SqlManager: Executing SQL statement: select top 50000 * from jzj.dbo.jzj_test where uid&gt;(select uid from (select top 250000 uid from jzj.dbo.jzj_test where uid&gt;31612700 and uid&lt;=32309400) a except select uid from (select top 249999 uid from jzj.dbo.jzj_test where uid&gt;31612700 and uid&lt;=32309400) a) and uid &lt;=32309400 and  (1 = 0) \n01-11-2017 13:49:00 CST run_dataimport INFO - 17/11/01 13:49:00 INFO manager.SqlManager: Executing SQL statement: select top 50000 * from jzj.dbo.jzj_test where uid&gt;(select uid from (select top 250000 uid from jzj.dbo.jzj_test where uid&gt;31612700 and uid&lt;=32309400) a except select uid from (select top 249999 uid from jzj.dbo.jzj_test where uid&gt;31612700 and uid&lt;=32309400) a) and uid &lt;=32309400 and  (1 = 0) \n01-11-2017 13:49:00 CST run_dataimport INFO - 17/11/01 13:49:00 INFO orm.CompilationManager: HADOOP_MAPRED_HOME is /opt/cloudera/parcels/CDH/lib/hadoop-mapreduce\n01-11-2017 13:49:02 CST run_dataimport INFO - Note: /tmp/sqoop-root/compile/30604370bdf48869bb99ca53d4bcb5e5/QueryResult.java uses or overrides a deprecated API.\n01-11-2017 13:49:02 CST run_dataimport INFO - Note: Recompile with -Xlint:deprecation for details.\n01-11-2017 13:49:02 CST run_dataimport INFO - 17/11/01 13:49:02 INFO orm.CompilationManager: Writing jar file: /tmp/sqoop-root/compile/30604370bdf48869bb99ca53d4bcb5e5/QueryResult.jar\n01-11-2017 13:49:03 CST run_dataimport INFO - 17/11/01 13:49:03 INFO tool.ImportTool: Maximal id query for free form incremental import: SELECT MAX(uid) FROM (select top 50000 * from jzj.dbo.jzj_test where uid&gt;(select uid from (select top 250000 uid from jzj.dbo.jzj_test where uid&gt;31612700 and uid&lt;=32309400) a except select uid from (select top 249999 uid from jzj.dbo.jzj_test where uid&gt;31612700 and uid&lt;=32309400) a) and uid &lt;=32309400 and (1 = 1)) sqoop_import_query_alias\n01-11-2017 14:51:11 CST run_dataimport INFO - 17/11/01 14:51:11 INFO tool.ImportTool: Incremental import based on column uid\n01-11-2017 14:51:11 CST run_dataimport INFO - 17/11/01 14:51:11 INFO tool.ImportTool: Upper bound value: 31912700\n01-11-2017 14:51:11 CST run_dataimport INFO - 17/11/01 14:51:11 INFO mapreduce.ImportJobBase: Beginning query import.\n01-11-2017 14:51:11 CST run_dataimport INFO - 17/11/01 14:51:11 INFO Configuration.deprecation: mapred.jar is deprecated. Instead, use mapreduce.job.jar\n01-11-2017 14:51:11 CST run_dataimport INFO - 17/11/01 14:51:11 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n01-11-2017 14:51:11 CST run_dataimport INFO - 17/11/01 14:51:11 INFO client.RMProxy: Connecting to ResourceManager at master/172.23.27.203:8032\n01-11-2017 14:51:18 CST run_dataimport INFO - 17/11/01 14:51:18 INFO db.DBInputFormat: Using read commited transaction isolation\n01-11-2017 14:51:18 CST run_dataimport INFO - 17/11/01 14:51:18 INFO db.DataDrivenDBInputFormat: BoundingValsQuery: SELECT MIN(uid), MAX(uid) FROM (select top 50000 * from jzj.dbo.jzj_test where uid&gt;(select uid from (select top 250000 uid from jzj.dbo.jzj_test where uid&gt;31612700 and uid&lt;=32309400) a except select uid from (select top 249999 uid from jzj.dbo.jzj_test where uid&gt;31612700 and uid&lt;=32309400) a) and uid &lt;=32309400 and uid &lt;= 31912700 AND  (1 = 1) ) AS t1\n01-11-2017 15:52:23 CST run_dataimport INFO - 17/11/01 15:52:23 INFO mapreduce.JobSubmitter: number of splits:4\n01-11-2017 15:52:24 CST run_dataimport INFO - 17/11/01 15:52:24 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1508671365448_1902\n01-11-2017 15:52:24 CST run_dataimport INFO - 17/11/01 15:52:24 INFO impl.YarnClientImpl: Submitted application application_1508671365448_1902\n01-11-2017 15:52:24 CST run_dataimport INFO - 17/11/01 15:52:24 INFO mapreduce.Job: The url to track the job: http://master:8088/proxy/application_1508671365448_1902/\n01-11-2017 15:52:24 CST run_dataimport INFO - 17/11/01 15:52:24 INFO mapreduce.Job: Running job: job_1508671365448_1902\n01-11-2017 15:52:31 CST run_dataimport INFO - 17/11/01 15:52:31 INFO mapreduce.Job: Job job_1508671365448_1902 running in uber mode : false\n01-11-2017 15:52:31 CST run_dataimport INFO - 17/11/01 15:52:31 INFO mapreduce.Job:  map 0% reduce 0%\n01-11-2017 16:54:01 CST run_dataimport INFO - 17/11/01 16:54:01 INFO mapreduce.Job:  map 25% reduce 0%\n01-11-2017 16:54:13 CST run_dataimport INFO - 17/11/01 16:54:13 INFO mapreduce.Job:  map 50% reduce 0%\n01-11-2017 16:55:01 CST run_dataimport INFO - 17/11/01 16:55:01 INFO mapreduce.Job:  map 75% reduce 0%\n01-11-2017 16:55:14 CST run_dataimport INFO - 17/11/01 16:55:14 INFO mapreduce.Job:  map 100% reduce 0%\n01-11-2017 16:55:15 CST run_dataimport INFO - 17/11/01 16:55:15 INFO mapreduce.Job: Job job_1508671365448_1902 completed successfully\n01-11-2017 16:55:16 CST run_dataimport INFO - 17/11/01 16:55:16 INFO mapreduce.Job: Counters: 30\n01-11-2017 16:55:16 CST run_dataimport INFO - \tFile System Counters\n01-11-2017 16:55:16 CST run_dataimport INFO - \t\tFILE: Number of bytes read=0\n01-11-2017 16:55:16 CST run_dataimport INFO - \t\tFILE: Number of bytes written=588148\n01-11-2017 16:55:16 CST run_dataimport INFO - \t\tFILE: Number of read operations=0\n01-11-2017 16:55:16 CST run_dataimport INFO - \t\tFILE: Number of large read operations=0\n01-11-2017 16:55:16 CST run_dataimport INFO - \t\tFILE: Number of write operations=0\n01-11-2017 16:55:16 CST run_dataimport INFO - \t\tHDFS: Number of bytes read=462\n01-11-2017 16:55:16 CST run_dataimport INFO - \t\tHDFS: Number of bytes written=23450000\n01-11-2017 16:55:16 CST run_dataimport INFO - \t\tHDFS: Number of read operations=16\n01-11-2017 16:55:16 CST run_dataimport INFO - \t\tHDFS: Number of large read operations=0\n01-11-2017 16:55:16 CST run_dataimport INFO - \t\tHDFS: Number of write operations=8\n01-11-2017 16:55:16 CST run_dataimport INFO - \tJob Counters \n01-11-2017 16:55:16 CST run_dataimport INFO - \t\tLaunched map tasks=4\n01-11-2017 16:55:16 CST run_dataimport INFO - \t\tOther local map tasks=4\n01-11-2017 16:55:16 CST run_dataimport INFO - \t\tTotal time spent by all maps in occupied slots (ms)=14892070\n01-11-2017 16:55:16 CST run_dataimport INFO - \t\tTotal time spent by all reduces in occupied slots (ms)=0\n01-11-2017 16:55:16 CST run_dataimport INFO - \t\tTotal time spent by all map tasks (ms)=14892070\n01-11-2017 16:55:16 CST run_dataimport INFO - \t\tTotal vcore-seconds taken by all map tasks=14892070\n01-11-2017 16:55:16 CST run_dataimport INFO - \t\tTotal megabyte-seconds taken by all map tasks=15249479680\n01-11-2017 16:55:16 CST run_dataimport INFO - \tMap-Reduce Framework\n01-11-2017 16:55:16 CST run_dataimport INFO - \t\tMap input records=50000\n01-11-2017 16:55:16 CST run_dataimport INFO - \t\tMap output records=50000\n01-11-2017 16:55:16 CST run_dataimport INFO - \t\tInput split bytes=462\n01-11-2017 16:55:16 CST run_dataimport INFO - \t\tSpilled Records=0\n01-11-2017 16:55:16 CST run_dataimport INFO - \t\tFailed Shuffles=0\n01-11-2017 16:55:16 CST run_dataimport INFO - \t\tMerged Map outputs=0\n01-11-2017 16:55:16 CST run_dataimport INFO - \t\tGC time elapsed (ms)=836\n01-11-2017 16:55:16 CST run_dataimport INFO - \t\tCPU time spent (ms)=68090\n01-11-2017 16:55:16 CST run_dataimport INFO - \t\tPhysical memory (bytes) snapshot=1530011648\n01-11-2017 16:55:16 CST run_dataimport INFO - \t\tVirtual memory (bytes) snapshot=6498086912\n01-11-2017 16:55:16 CST run_dataimport INFO - \t\tTotal committed heap usage (bytes)=3083862016\n01-11-2017 16:55:16 CST run_dataimport INFO - \tFile Input Format Counters \n01-11-2017 16:55:16 CST run_dataimport INFO - \t\tBytes Read=0\n01-11-2017 16:55:16 CST run_dataimport INFO - \tFile Output Format Counters \n01-11-2017 16:55:16 CST run_dataimport INFO - \t\tBytes Written=23450000\n01-11-2017 16:55:16 CST run_dataimport INFO - 17/11/01 16:55:16 INFO mapreduce.ImportJobBase: Transferred 22.3637 MB in 7,444.7403 seconds (3.076 KB/sec)\n01-11-2017 16:55:16 CST run_dataimport INFO - 17/11/01 16:55:16 INFO mapreduce.ImportJobBase: Retrieved 50000 records.\n01-11-2017 16:55:16 CST run_dataimport INFO - 17/11/01 16:55:16 INFO util.AppendUtils: Appending to directory sql_to_hive\n01-11-2017 16:55:16 CST run_dataimport INFO - 17/11/01 16:55:16 INFO util.AppendUtils: Using found partition 2680\n01-11-2017 16:55:16 CST run_dataimport INFO - 17/11/01 16:55:16 INFO tool.ImportTool: Incremental import complete! To run another incremental import of all data following this import, supply the following arguments:\n01-11-2017 16:55:16 CST run_dataimport INFO - 17/11/01 16:55:16 INFO tool.ImportTool:  --incremental append\n01-11-2017 16:55:16 CST run_dataimport INFO - 17/11/01 16:55:16 INFO tool.ImportTool:   --check-column uid\n01-11-2017 16:55:16 CST run_dataimport INFO - 17/11/01 16:55:16 INFO tool.ImportTool:   --last-value 31912700\n01-11-2017 16:55:16 CST run_dataimport INFO - 17/11/01 16:55:16 INFO tool.ImportTool: (Consider saving this with 'sqoop job --create')\n01-11-2017 16:55:16 CST run_dataimport INFO - 8\n01-11-2017 16:55:16 CST run_dataimport INFO - Warning: /opt/cloudera/parcels/CDH-5.8.0-1.cdh5.8.0.p0.42/bin/../lib/sqoop/../accumulo does not exist! Accumulo imports will fail.\n01-11-2017 16:55:16 CST run_dataimport INFO - Please set $ACCUMULO_HOME to the root of your Accumulo installation.\n01-11-2017 16:55:18 CST run_dataimport INFO - 17/11/01 16:55:18 INFO sqoop.Sqoop: Running Sqoop version: 1.4.6-cdh5.8.0\n01-11-2017 16:55:18 CST run_dataimport INFO - 17/11/01 16:55:18 WARN sqoop.ConnFactory: Parameter --driver is set to an explicit driver however appropriate connection manager is not being set (via --connection-manager). Sqoop is going to fall back to org.apache.sqoop.manager.GenericJdbcManager. Please specify explicitly which connection manager should be used next time.\n01-11-2017 16:55:18 CST run_dataimport INFO - 17/11/01 16:55:18 INFO manager.SqlManager: Using default fetchSize of 1000\n01-11-2017 16:55:18 CST run_dataimport INFO - 17/11/01 16:55:18 INFO tool.CodeGenTool: Beginning code generation\n01-11-2017 16:55:19 CST run_dataimport INFO - 17/11/01 16:55:19 INFO manager.SqlManager: Executing SQL statement: select top 50000 * from jzj.dbo.jzj_test where uid&gt;(select uid from (select top 300000 uid from jzj.dbo.jzj_test where uid&gt;31612700 and uid&lt;=32309400) a except select uid from (select top 299999 uid from jzj.dbo.jzj_test where uid&gt;31612700 and uid&lt;=32309400) a) and uid &lt;=32309400 and  (1 = 0) \n01-11-2017 16:55:19 CST run_dataimport INFO - 17/11/01 16:55:19 INFO manager.SqlManager: Executing SQL statement: select top 50000 * from jzj.dbo.jzj_test where uid&gt;(select uid from (select top 300000 uid from jzj.dbo.jzj_test where uid&gt;31612700 and uid&lt;=32309400) a except select uid from (select top 299999 uid from jzj.dbo.jzj_test where uid&gt;31612700 and uid&lt;=32309400) a) and uid &lt;=32309400 and  (1 = 0) \n01-11-2017 16:55:19 CST run_dataimport INFO - 17/11/01 16:55:19 INFO orm.CompilationManager: HADOOP_MAPRED_HOME is /opt/cloudera/parcels/CDH/lib/hadoop-mapreduce\n01-11-2017 16:55:21 CST run_dataimport INFO - Note: /tmp/sqoop-root/compile/22cb16fb8dfcb5e24fdf71641e1601ee/QueryResult.java uses or overrides a deprecated API.\n01-11-2017 16:55:21 CST run_dataimport INFO - Note: Recompile with -Xlint:deprecation for details.\n01-11-2017 16:55:21 CST run_dataimport INFO - 17/11/01 16:55:21 INFO orm.CompilationManager: Writing jar file: /tmp/sqoop-root/compile/22cb16fb8dfcb5e24fdf71641e1601ee/QueryResult.jar\n01-11-2017 16:55:22 CST run_dataimport INFO - 17/11/01 16:55:22 INFO tool.ImportTool: Maximal id query for free form incremental import: SELECT MAX(uid) FROM (select top 50000 * from jzj.dbo.jzj_test where uid&gt;(select uid from (select top 300000 uid from jzj.dbo.jzj_test where uid&gt;31612700 and uid&lt;=32309400) a except select uid from (select top 299999 uid from jzj.dbo.jzj_test where uid&gt;31612700 and uid&lt;=32309400) a) and uid &lt;=32309400 and (1 = 1)) sqoop_import_query_alias\n", 'length': 64213}
[2017-11-20 13:11:02,011] app.hive_mysql.AzkabanMonitor [INFO] URL: https://172.23.27.203:8443
[2017-11-20 13:11:02,175] app.hive_mysql.AzkabanMonitor [INFO] Success: Login https://172.23.27.203:8443 with user azkaban
[2017-11-20 13:11:02,175] app.hive_mysql.AzkabanMonitor [DEBUG] {'session.id': '8842e65e-08e3-4f00-8b0f-e4fcb6616f24', 'status': 'success'}
[2017-11-20 13:11:02,193] app.hive_mysql.AzkabanMonitor [INFO] Success: Fetch Executions of a Flow - start-syn
[2017-11-20 13:11:02,194] app.hive_mysql.AzkabanMonitor [DEBUG] {'total': 7, 'executions': [{'submitTime': 1509534142141, 'submitUser': 'azkaban', 'startTime': 1509534142295, 'endTime': 1509534142537, 'flowId': 'start-syn', 'projectId': 66, 'execId': 1659, 'status': 'KILLED'}, {'submitTime': 1509438157461, 'submitUser': 'azkaban', 'startTime': 1509438157639, 'endTime': 1509438189521, 'flowId': 'start-syn', 'projectId': 66, 'execId': 1652, 'status': 'KILLED'}, {'submitTime': 1509438102634, 'submitUser': 'azkaban', 'startTime': 1509438102800, 'endTime': 1509438119797, 'flowId': 'start-syn', 'projectId': 66, 'execId': 1651, 'status': 'KILLED'}, {'submitTime': 1509435165409, 'submitUser': 'azkaban', 'startTime': 1509435165665, 'endTime': 1509435183654, 'flowId': 'start-syn', 'projectId': 66, 'execId': 1649, 'status': 'KILLED'}, {'submitTime': 1509435136471, 'submitUser': 'azkaban', 'startTime': 1509435136636, 'endTime': 1509435145535, 'flowId': 'start-syn', 'projectId': 66, 'execId': 1648, 'status': 'KILLED'}, {'submitTime': 1509435094803, 'submitUser': 'azkaban', 'startTime': 1509435095118, 'endTime': 1509435128253, 'flowId': 'start-syn', 'projectId': 66, 'execId': 1647, 'status': 'KILLED'}], 'length': 10, 'project': 'mysql_syn', 'from': 1, 'projectId': 66, 'flow': 'start-syn'}
[2017-11-20 13:12:02,213] app.hive_mysql.AzkabanMonitor [INFO] URL: https://172.23.27.203:8443
[2017-11-20 13:12:02,407] app.hive_mysql.AzkabanMonitor [INFO] Success: Login https://172.23.27.203:8443 with user azkaban
[2017-11-20 13:12:02,407] app.hive_mysql.AzkabanMonitor [DEBUG] {'session.id': 'c7a7b977-d5bf-410a-90b5-8e8a0a81b2d1', 'status': 'success'}
[2017-11-20 13:12:02,431] app.hive_mysql.AzkabanMonitor [INFO] Success: Fetch Executions of a Flow - start-syn
[2017-11-20 13:12:02,432] app.hive_mysql.AzkabanMonitor [DEBUG] {'total': 7, 'executions': [{'submitTime': 1509534142141, 'submitUser': 'azkaban', 'startTime': 1509534142295, 'endTime': 1509534142537, 'flowId': 'start-syn', 'projectId': 66, 'execId': 1659, 'status': 'KILLED'}, {'submitTime': 1509438157461, 'submitUser': 'azkaban', 'startTime': 1509438157639, 'endTime': 1509438189521, 'flowId': 'start-syn', 'projectId': 66, 'execId': 1652, 'status': 'KILLED'}, {'submitTime': 1509438102634, 'submitUser': 'azkaban', 'startTime': 1509438102800, 'endTime': 1509438119797, 'flowId': 'start-syn', 'projectId': 66, 'execId': 1651, 'status': 'KILLED'}, {'submitTime': 1509435165409, 'submitUser': 'azkaban', 'startTime': 1509435165665, 'endTime': 1509435183654, 'flowId': 'start-syn', 'projectId': 66, 'execId': 1649, 'status': 'KILLED'}, {'submitTime': 1509435136471, 'submitUser': 'azkaban', 'startTime': 1509435136636, 'endTime': 1509435145535, 'flowId': 'start-syn', 'projectId': 66, 'execId': 1648, 'status': 'KILLED'}, {'submitTime': 1509435094803, 'submitUser': 'azkaban', 'startTime': 1509435095118, 'endTime': 1509435128253, 'flowId': 'start-syn', 'projectId': 66, 'execId': 1647, 'status': 'KILLED'}], 'length': 10, 'project': 'mysql_syn', 'from': 1, 'projectId': 66, 'flow': 'start-syn'}
[2017-11-20 13:12:02,449] app.hive_mysql.AzkabanMonitor [INFO] Success: Fetch Execution Job Logs - 1659-start-syn
[2017-11-20 13:12:02,449] app.hive_mysql.AzkabanMonitor [DEBUG] {'offset': 0, 'data': '', 'length': 0}
[2017-11-20 16:34:20,361] app.hive_mysql.AzkabanMonitor [INFO] URL: https://172.23.27.203:8443
[2017-11-20 16:34:28,372] app.hive_mysql.AzkabanMonitor [INFO] Success: Login https://172.23.27.203:8443 with user azkaban
[2017-11-20 16:34:28,372] app.hive_mysql.AzkabanMonitor [DEBUG] {'session.id': 'f597ea8a-f380-4746-8512-0e7ff45fc156', 'status': 'success'}
[2017-11-20 16:34:28,399] app.hive_mysql.AzkabanMonitor [INFO] Success: Fetch Executions of a Flow - run_dataimport
[2017-11-20 16:34:28,400] app.hive_mysql.AzkabanMonitor [DEBUG] {'total': 16, 'executions': [{'submitTime': 1510041651107, 'submitUser': 'azkaban', 'startTime': 1510041651277, 'endTime': 1510114267057, 'flowId': 'run_dataimport', 'projectId': 68, 'execId': 1670, 'status': 'KILLED'}, {'submitTime': 1510027251078, 'submitUser': 'azkaban', 'startTime': 1510027251395, 'endTime': 1510036906931, 'flowId': 'run_dataimport', 'projectId': 68, 'execId': 1669, 'status': 'SUCCEEDED'}, {'submitTime': 1509897650826, 'submitUser': 'azkaban', 'startTime': 1509897651116, 'endTime': 1510026870568, 'flowId': 'run_dataimport', 'projectId': 68, 'execId': 1668, 'status': 'SUCCEEDED'}, {'submitTime': 1509883250800, 'submitUser': 'azkaban', 'startTime': 1509883251056, 'endTime': 1509892870422, 'flowId': 'run_dataimport', 'projectId': 68, 'execId': 1667, 'status': 'SUCCEEDED'}, {'submitTime': 1509753650545, 'submitUser': 'azkaban', 'startTime': 1509753650705, 'endTime': 1509881095432, 'flowId': 'run_dataimport', 'projectId': 68, 'execId': 1666, 'status': 'SUCCEEDED'}, {'submitTime': 1509746450529, 'submitUser': 'azkaban', 'startTime': 1509746450965, 'endTime': 1509753076918, 'flowId': 'run_dataimport', 'projectId': 68, 'execId': 1665, 'status': 'SUCCEEDED'}, {'submitTime': 1509652850362, 'submitUser': 'azkaban', 'startTime': 1509652850560, 'endTime': 1509744409191, 'flowId': 'run_dataimport', 'projectId': 68, 'execId': 1664, 'status': 'SUCCEEDED'}, {'submitTime': 1509645650349, 'submitUser': 'azkaban', 'startTime': 1509645650498, 'endTime': 1509651159750, 'flowId': 'run_dataimport', 'projectId': 68, 'execId': 1663, 'status': 'SUCCEEDED'}, {'submitTime': 1509566450178, 'submitUser': 'azkaban', 'startTime': 1509566450488, 'endTime': 1509642929867, 'flowId': 'run_dataimport', 'projectId': 68, 'execId': 1661, 'status': 'SUCCEEDED'}, {'submitTime': 1509559250163, 'submitUser': 'azkaban', 'startTime': 1509559250321, 'endTime': 1509563186239, 'flowId': 'run_dataimport', 'projectId': 68, 'execId': 1660, 'status': 'SUCCEEDED'}], 'length': 10, 'project': 'sql_to_hive', 'from': 0, 'projectId': 68, 'flow': 'run_dataimport'}
[2017-11-20 16:34:28,413] app.hive_mysql.AzkabanMonitor [INFO] Success: Fetch Execution Job Logs - 1670-run_dataimport
[2017-11-20 16:34:28,413] app.hive_mysql.AzkabanMonitor [DEBUG] {'offset': 0, 'data': "07-11-2017 16:00:51 CST run_dataimport INFO - Starting job run_dataimport at 1510041651281\n07-11-2017 16:00:51 CST run_dataimport INFO - azkaban.webserver.url property was not set\n07-11-2017 16:00:51 CST run_dataimport INFO - job JVM args: -Dazkaban.flowid=run_dataimport -Dazkaban.execid=1670 -Dazkaban.jobid=run_dataimport\n07-11-2017 16:00:51 CST run_dataimport INFO - Building command job executor. \n07-11-2017 16:00:51 CST run_dataimport INFO - Memory granted for job run_dataimport\n07-11-2017 16:00:51 CST run_dataimport INFO - 1 commands to execute.\n07-11-2017 16:00:51 CST run_dataimport INFO - cwd=/home/azkaban/azkaban_bk/azkaban-exec-server-3.35.0-22-g5517d50/executions/1670\n07-11-2017 16:00:51 CST run_dataimport INFO - effective user is: azkaban\n07-11-2017 16:00:51 CST run_dataimport INFO - Command: sh /home/sql_to_hive/dataimport.sh\n07-11-2017 16:00:51 CST run_dataimport INFO - Environment variables: {JOB_OUTPUT_PROP_FILE=/home/azkaban/azkaban_bk/azkaban-exec-server-3.35.0-22-g5517d50/executions/1670/run_dataimport_output_8807970133616151260_tmp, JOB_PROP_FILE=/home/azkaban/azkaban_bk/azkaban-exec-server-3.35.0-22-g5517d50/executions/1670/run_dataimport_props_6140684690151138768_tmp, KRB5CCNAME=/tmp/krb5cc__sql_to_hive__run_dataimport__run_dataimport__1670__azkaban, JOB_NAME=run_dataimport}\n07-11-2017 16:00:51 CST run_dataimport INFO - Working directory: /home/azkaban/azkaban_bk/azkaban-exec-server-3.35.0-22-g5517d50/executions/1670\n07-11-2017 16:00:51 CST run_dataimport INFO - log4j:WARN No appenders could be found for logger (org.apache.hive.jdbc.Utils).\n07-11-2017 16:00:51 CST run_dataimport INFO - log4j:WARN Please initialize the log4j system properly.\n07-11-2017 16:00:51 CST run_dataimport INFO - log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.\n07-11-2017 16:00:51 CST run_dataimport INFO - SLF4J: Failed to load class &quot;org.slf4j.impl.StaticLoggerBinder&quot;.\n07-11-2017 16:00:51 CST run_dataimport INFO - SLF4J: Defaulting to no-operation (NOP) logger implementation\n07-11-2017 16:00:51 CST run_dataimport INFO - SLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.\n07-11-2017 16:02:05 CST run_dataimport INFO - 1,84598500,83222300,1376200sqlmax:84598500\n07-11-2017 16:02:05 CST run_dataimport INFO - hivemax:83222300\n07-11-2017 16:02:05 CST run_dataimport INFO - counts:1376200\n07-11-2017 16:02:05 CST run_dataimport INFO - 7,176200\n07-11-2017 16:02:05 CST run_dataimport INFO - 7\n07-11-2017 16:02:05 CST run_dataimport INFO - Warning: /opt/cloudera/parcels/CDH-5.8.0-1.cdh5.8.0.p0.42/bin/../lib/sqoop/../accumulo does not exist! Accumulo imports will fail.\n07-11-2017 16:02:05 CST run_dataimport INFO - Please set $ACCUMULO_HOME to the root of your Accumulo installation.\n07-11-2017 16:02:07 CST run_dataimport INFO - 17/11/07 16:02:07 INFO sqoop.Sqoop: Running Sqoop version: 1.4.6-cdh5.8.0\n07-11-2017 16:02:07 CST run_dataimport INFO - 17/11/07 16:02:07 WARN sqoop.ConnFactory: Parameter --driver is set to an explicit driver however appropriate connection manager is not being set (via --connection-manager). Sqoop is going to fall back to org.apache.sqoop.manager.GenericJdbcManager. Please specify explicitly which connection manager should be used next time.\n07-11-2017 16:02:07 CST run_dataimport INFO - 17/11/07 16:02:07 INFO manager.SqlManager: Using default fetchSize of 1000\n07-11-2017 16:02:07 CST run_dataimport INFO - 17/11/07 16:02:07 INFO tool.CodeGenTool: Beginning code generation\n07-11-2017 16:02:08 CST run_dataimport INFO - 17/11/07 16:02:08 INFO manager.SqlManager: Executing SQL statement: select top 200000 * from jzj.dbo.jzj_test where uid&gt;=(select uid from (select top 1 uid from jzj.dbo.jzj_test where uid&gt;83222300 and uid&lt;=84598500) a) and uid &lt;=84598500 and  (1 = 0) \n07-11-2017 16:02:08 CST run_dataimport INFO - 17/11/07 16:02:08 INFO manager.SqlManager: Executing SQL statement: select top 200000 * from jzj.dbo.jzj_test where uid&gt;=(select uid from (select top 1 uid from jzj.dbo.jzj_test where uid&gt;83222300 and uid&lt;=84598500) a) and uid &lt;=84598500 and  (1 = 0) \n07-11-2017 16:02:08 CST run_dataimport INFO - 17/11/07 16:02:08 INFO orm.CompilationManager: HADOOP_MAPRED_HOME is /opt/cloudera/parcels/CDH/lib/hadoop-mapreduce\n07-11-2017 16:02:10 CST run_dataimport INFO - Note: /tmp/sqoop-root/compile/b796f2c0b0198c05bdcf8149b3f9a7fd/QueryResult.java uses or overrides a deprecated API.\n07-11-2017 16:02:10 CST run_dataimport INFO - Note: Recompile with -Xlint:deprecation for details.\n07-11-2017 16:02:10 CST run_dataimport INFO - 17/11/07 16:02:10 INFO orm.CompilationManager: Writing jar file: /tmp/sqoop-root/compile/b796f2c0b0198c05bdcf8149b3f9a7fd/QueryResult.jar\n07-11-2017 16:02:11 CST run_dataimport INFO - 17/11/07 16:02:11 INFO tool.ImportTool: Maximal id query for free form incremental import: SELECT MAX(uid) FROM (select top 200000 * from jzj.dbo.jzj_test where uid&gt;=(select uid from (select top 1 uid from jzj.dbo.jzj_test where uid&gt;83222300 and uid&lt;=84598500) a) and uid &lt;=84598500 and (1 = 1)) sqoop_import_query_alias\n07-11-2017 16:02:11 CST run_dataimport INFO - 17/11/07 16:02:11 INFO tool.ImportTool: Incremental import based on column uid\n07-11-2017 16:02:11 CST run_dataimport INFO - 17/11/07 16:02:11 INFO tool.ImportTool: Upper bound value: 83422300\n07-11-2017 16:02:11 CST run_dataimport INFO - 17/11/07 16:02:11 INFO mapreduce.ImportJobBase: Beginning query import.\n07-11-2017 16:02:11 CST run_dataimport INFO - 17/11/07 16:02:11 INFO Configuration.deprecation: mapred.jar is deprecated. Instead, use mapreduce.job.jar\n07-11-2017 16:02:11 CST run_dataimport INFO - 17/11/07 16:02:11 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n07-11-2017 16:02:11 CST run_dataimport INFO - 17/11/07 16:02:11 INFO client.RMProxy: Connecting to ResourceManager at master/172.23.27.203:8032\n07-11-2017 16:02:18 CST run_dataimport INFO - 17/11/07 16:02:18 INFO db.DBInputFormat: Using read commited transaction isolation\n07-11-2017 16:02:18 CST run_dataimport INFO - 17/11/07 16:02:18 INFO db.DataDrivenDBInputFormat: BoundingValsQuery: SELECT MIN(uid), MAX(uid) FROM (select top 200000 * from jzj.dbo.jzj_test where uid&gt;=(select uid from (select top 1 uid from jzj.dbo.jzj_test where uid&gt;83222300 and uid&lt;=84598500) a) and uid &lt;=84598500 and uid &lt;= 83422300 AND  (1 = 1) ) AS t1\n07-11-2017 16:02:18 CST run_dataimport INFO - 17/11/07 16:02:18 INFO mapreduce.JobSubmitter: number of splits:4\n07-11-2017 16:02:18 CST run_dataimport INFO - 17/11/07 16:02:18 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1508671365448_2954\n07-11-2017 16:02:19 CST run_dataimport INFO - 17/11/07 16:02:19 INFO impl.YarnClientImpl: Submitted application application_1508671365448_2954\n07-11-2017 16:02:19 CST run_dataimport INFO - 17/11/07 16:02:19 INFO mapreduce.Job: The url to track the job: http://master:8088/proxy/application_1508671365448_2954/\n07-11-2017 16:02:19 CST run_dataimport INFO - 17/11/07 16:02:19 INFO mapreduce.Job: Running job: job_1508671365448_2954\n07-11-2017 16:02:26 CST run_dataimport INFO - 17/11/07 16:02:26 INFO mapreduce.Job: Job job_1508671365448_2954 running in uber mode : false\n07-11-2017 16:02:26 CST run_dataimport INFO - 17/11/07 16:02:26 INFO mapreduce.Job:  map 0% reduce 0%\n07-11-2017 16:02:37 CST run_dataimport INFO - 17/11/07 16:02:37 INFO mapreduce.Job:  map 25% reduce 0%\n07-11-2017 16:02:40 CST run_dataimport INFO - 17/11/07 16:02:40 INFO mapreduce.Job:  map 100% reduce 0%\n07-11-2017 16:02:41 CST run_dataimport INFO - 17/11/07 16:02:41 INFO mapreduce.Job: Job job_1508671365448_2954 completed successfully\n07-11-2017 16:02:41 CST run_dataimport INFO - 17/11/07 16:02:41 INFO mapreduce.Job: Counters: 30\n07-11-2017 16:02:41 CST run_dataimport INFO - \tFile System Counters\n07-11-2017 16:02:41 CST run_dataimport INFO - \t\tFILE: Number of bytes read=0\n07-11-2017 16:02:41 CST run_dataimport INFO - \t\tFILE: Number of bytes written=587024\n07-11-2017 16:02:41 CST run_dataimport INFO - \t\tFILE: Number of read operations=0\n07-11-2017 16:02:41 CST run_dataimport INFO - \t\tFILE: Number of large read operations=0\n07-11-2017 16:02:41 CST run_dataimport INFO - \t\tFILE: Number of write operations=0\n07-11-2017 16:02:41 CST run_dataimport INFO - \t\tHDFS: Number of bytes read=462\n07-11-2017 16:02:41 CST run_dataimport INFO - \t\tHDFS: Number of bytes written=93800000\n07-11-2017 16:02:41 CST run_dataimport INFO - \t\tHDFS: Number of read operations=16\n07-11-2017 16:02:41 CST run_dataimport INFO - \t\tHDFS: Number of large read operations=0\n07-11-2017 16:02:41 CST run_dataimport INFO - \t\tHDFS: Number of write operations=8\n07-11-2017 16:02:41 CST run_dataimport INFO - \tJob Counters \n07-11-2017 16:02:41 CST run_dataimport INFO - \t\tLaunched map tasks=4\n07-11-2017 16:02:41 CST run_dataimport INFO - \t\tOther local map tasks=4\n07-11-2017 16:02:41 CST run_dataimport INFO - \t\tTotal time spent by all maps in occupied slots (ms)=44342\n07-11-2017 16:02:41 CST run_dataimport INFO - \t\tTotal time spent by all reduces in occupied slots (ms)=0\n07-11-2017 16:02:41 CST run_dataimport INFO - \t\tTotal time spent by all map tasks (ms)=44342\n07-11-2017 16:02:41 CST run_dataimport INFO - \t\tTotal vcore-seconds taken by all map tasks=44342\n07-11-2017 16:02:41 CST run_dataimport INFO - \t\tTotal megabyte-seconds taken by all map tasks=45406208\n07-11-2017 16:02:41 CST run_dataimport INFO - \tMap-Reduce Framework\n07-11-2017 16:02:41 CST run_dataimport INFO - \t\tMap input records=200000\n07-11-2017 16:02:41 CST run_dataimport INFO - \t\tMap output records=200000\n07-11-2017 16:02:41 CST run_dataimport INFO - \t\tInput split bytes=462\n07-11-2017 16:02:41 CST run_dataimport INFO - \t\tSpilled Records=0\n07-11-2017 16:02:41 CST run_dataimport INFO - \t\tFailed Shuffles=0\n07-11-2017 16:02:41 CST run_dataimport INFO - \t\tMerged Map outputs=0\n07-11-2017 16:02:41 CST run_dataimport INFO - \t\tGC time elapsed (ms)=569\n07-11-2017 16:02:41 CST run_dataimport INFO - \t\tCPU time spent (ms)=36080\n07-11-2017 16:02:41 CST run_dataimport INFO - \t\tPhysical memory (bytes) snapshot=1458102272\n07-11-2017 16:02:41 CST run_dataimport INFO - \t\tVirtual memory (bytes) snapshot=6516502528\n07-11-2017 16:02:41 CST run_dataimport INFO - \t\tTotal committed heap usage (bytes)=3231186944\n07-11-2017 16:02:41 CST run_dataimport INFO - \tFile Input Format Counters \n07-11-2017 16:02:41 CST run_dataimport INFO - \t\tBytes Read=0\n07-11-2017 16:02:41 CST run_dataimport INFO - \tFile Output Format Counters \n07-11-2017 16:02:41 CST run_dataimport INFO - \t\tBytes Written=93800000\n07-11-2017 16:02:41 CST run_dataimport INFO - 17/11/07 16:02:41 INFO mapreduce.ImportJobBase: Transferred 89.4547 MB in 29.9824 seconds (2.9836 MB/sec)\n07-11-2017 16:02:41 CST run_dataimport INFO - 17/11/07 16:02:41 INFO mapreduce.ImportJobBase: Retrieved 200000 records.\n07-11-2017 16:02:41 CST run_dataimport INFO - 17/11/07 16:02:41 INFO util.AppendUtils: Appending to directory sql_to_hive\n07-11-2017 16:02:42 CST run_dataimport INFO - 17/11/07 16:02:42 INFO util.AppendUtils: Using found partition 6800\n07-11-2017 16:02:42 CST run_dataimport INFO - 17/11/07 16:02:42 INFO tool.ImportTool: Incremental import complete! To run another incremental import of all data following this import, supply the following arguments:\n07-11-2017 16:02:42 CST run_dataimport INFO - 17/11/07 16:02:42 INFO tool.ImportTool:  --incremental append\n07-11-2017 16:02:42 CST run_dataimport INFO - 17/11/07 16:02:42 INFO tool.ImportTool:   --check-column uid\n07-11-2017 16:02:42 CST run_dataimport INFO - 17/11/07 16:02:42 INFO tool.ImportTool:   --last-value 83422300\n07-11-2017 16:02:42 CST run_dataimport INFO - 17/11/07 16:02:42 INFO tool.ImportTool: (Consider saving this with 'sqoop job --create')\n07-11-2017 16:02:42 CST run_dataimport INFO - 6\n07-11-2017 16:02:42 CST run_dataimport INFO - Warning: /opt/cloudera/parcels/CDH-5.8.0-1.cdh5.8.0.p0.42/bin/../lib/sqoop/../accumulo does not exist! Accumulo imports will fail.\n07-11-2017 16:02:42 CST run_dataimport INFO - Please set $ACCUMULO_HOME to the root of your Accumulo installation.\n07-11-2017 16:02:44 CST run_dataimport INFO - 17/11/07 16:02:44 INFO sqoop.Sqoop: Running Sqoop version: 1.4.6-cdh5.8.0\n07-11-2017 16:02:44 CST run_dataimport INFO - 17/11/07 16:02:44 WARN sqoop.ConnFactory: Parameter --driver is set to an explicit driver however appropriate connection manager is not being set (via --connection-manager). Sqoop is going to fall back to org.apache.sqoop.manager.GenericJdbcManager. Please specify explicitly which connection manager should be used next time.\n07-11-2017 16:02:44 CST run_dataimport INFO - 17/11/07 16:02:44 INFO manager.SqlManager: Using default fetchSize of 1000\n07-11-2017 16:02:44 CST run_dataimport INFO - 17/11/07 16:02:44 INFO tool.CodeGenTool: Beginning code generation\n07-11-2017 16:02:45 CST run_dataimport INFO - 17/11/07 16:02:45 INFO manager.SqlManager: Executing SQL statement: select top 200000 * from jzj.dbo.jzj_test where uid&gt;(select uid from (select top 200000 uid from jzj.dbo.jzj_test where uid&gt;83222300 and uid&lt;=84598500) a except select uid from (select top 199999 uid from jzj.dbo.jzj_test where uid&gt;83222300 and uid&lt;=84598500) a) and uid &lt;=84598500 and  (1 = 0) \n07-11-2017 16:02:45 CST run_dataimport INFO - 17/11/07 16:02:45 INFO manager.SqlManager: Executing SQL statement: select top 200000 * from jzj.dbo.jzj_test where uid&gt;(select uid from (select top 200000 uid from jzj.dbo.jzj_test where uid&gt;83222300 and uid&lt;=84598500) a except select uid from (select top 199999 uid from jzj.dbo.jzj_test where uid&gt;83222300 and uid&lt;=84598500) a) and uid &lt;=84598500 and  (1 = 0) \n07-11-2017 16:02:45 CST run_dataimport INFO - 17/11/07 16:02:45 INFO orm.CompilationManager: HADOOP_MAPRED_HOME is /opt/cloudera/parcels/CDH/lib/hadoop-mapreduce\n07-11-2017 16:02:47 CST run_dataimport INFO - Note: /tmp/sqoop-root/compile/3e3fe91d38feffbabab3df86c4ddb2c4/QueryResult.java uses or overrides a deprecated API.\n07-11-2017 16:02:47 CST run_dataimport INFO - Note: Recompile with -Xlint:deprecation for details.\n07-11-2017 16:02:47 CST run_dataimport INFO - 17/11/07 16:02:47 INFO orm.CompilationManager: Writing jar file: /tmp/sqoop-root/compile/3e3fe91d38feffbabab3df86c4ddb2c4/QueryResult.jar\n07-11-2017 16:02:48 CST run_dataimport INFO - 17/11/07 16:02:48 INFO tool.ImportTool: Maximal id query for free form incremental import: SELECT MAX(uid) FROM (select top 200000 * from jzj.dbo.jzj_test where uid&gt;(select uid from (select top 200000 uid from jzj.dbo.jzj_test where uid&gt;83222300 and uid&lt;=84598500) a except select uid from (select top 199999 uid from jzj.dbo.jzj_test where uid&gt;83222300 and uid&lt;=84598500) a) and uid &lt;=84598500 and (1 = 1)) sqoop_import_query_alias\n07-11-2017 16:42:28 CST run_dataimport INFO - 17/11/07 16:42:28 INFO tool.ImportTool: Incremental import based on column uid\n07-11-2017 16:42:28 CST run_dataimport INFO - 17/11/07 16:42:28 INFO tool.ImportTool: Upper bound value: 83622300\n07-11-2017 16:42:28 CST run_dataimport INFO - 17/11/07 16:42:28 INFO mapreduce.ImportJobBase: Beginning query import.\n07-11-2017 16:42:28 CST run_dataimport INFO - 17/11/07 16:42:28 INFO Configuration.deprecation: mapred.jar is deprecated. Instead, use mapreduce.job.jar\n07-11-2017 16:42:28 CST run_dataimport INFO - 17/11/07 16:42:28 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n07-11-2017 16:42:28 CST run_dataimport INFO - 17/11/07 16:42:28 INFO client.RMProxy: Connecting to ResourceManager at master/172.23.27.203:8032\n07-11-2017 16:42:34 CST run_dataimport INFO - 17/11/07 16:42:34 INFO db.DBInputFormat: Using read commited transaction isolation\n07-11-2017 16:42:34 CST run_dataimport INFO - 17/11/07 16:42:34 INFO db.DataDrivenDBInputFormat: BoundingValsQuery: SELECT MIN(uid), MAX(uid) FROM (select top 200000 * from jzj.dbo.jzj_test where uid&gt;(select uid from (select top 200000 uid from jzj.dbo.jzj_test where uid&gt;83222300 and uid&lt;=84598500) a except select uid from (select top 199999 uid from jzj.dbo.jzj_test where uid&gt;83222300 and uid&lt;=84598500) a) and uid &lt;=84598500 and uid &lt;= 83622300 AND  (1 = 1) ) AS t1\n07-11-2017 17:22:49 CST run_dataimport INFO - 17/11/07 17:22:49 INFO mapreduce.JobSubmitter: number of splits:4\n07-11-2017 17:22:49 CST run_dataimport INFO - 17/11/07 17:22:49 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1508671365448_2955\n07-11-2017 17:22:49 CST run_dataimport INFO - 17/11/07 17:22:49 INFO impl.YarnClientImpl: Submitted application application_1508671365448_2955\n07-11-2017 17:22:49 CST run_dataimport INFO - 17/11/07 17:22:49 INFO mapreduce.Job: The url to track the job: http://master:8088/proxy/application_1508671365448_2955/\n07-11-2017 17:22:49 CST run_dataimport INFO - 17/11/07 17:22:49 INFO mapreduce.Job: Running job: job_1508671365448_2955\n07-11-2017 17:22:57 CST run_dataimport INFO - 17/11/07 17:22:57 INFO mapreduce.Job: Job job_1508671365448_2955 running in uber mode : false\n07-11-2017 17:22:57 CST run_dataimport INFO - 17/11/07 17:22:57 INFO mapreduce.Job:  map 0% reduce 0%\n07-11-2017 18:02:56 CST run_dataimport INFO - 17/11/07 18:02:56 INFO mapreduce.Job:  map 25% reduce 0%\n07-11-2017 18:03:23 CST run_dataimport INFO - 17/11/07 18:03:23 INFO mapreduce.Job:  map 50% reduce 0%\n07-11-2017 18:03:46 CST run_dataimport INFO - 17/11/07 18:03:46 INFO mapreduce.Job:  map 75% reduce 0%\n07-11-2017 18:03:47 CST run_dataimport INFO - 17/11/07 18:03:47 INFO mapreduce.Job:  map 100% reduce 0%\n07-11-2017 18:03:48 CST run_dataimport INFO - 17/11/07 18:03:48 INFO mapreduce.Job: Job job_1508671365448_2955 completed successfully\n07-11-2017 18:03:48 CST run_dataimport INFO - 17/11/07 18:03:48 INFO mapreduce.Job: Counters: 30\n07-11-2017 18:03:48 CST run_dataimport INFO - \tFile System Counters\n07-11-2017 18:03:48 CST run_dataimport INFO - \t\tFILE: Number of bytes read=0\n07-11-2017 18:03:48 CST run_dataimport INFO - \t\tFILE: Number of bytes written=588156\n07-11-2017 18:03:48 CST run_dataimport INFO - \t\tFILE: Number of read operations=0\n07-11-2017 18:03:48 CST run_dataimport INFO - \t\tFILE: Number of large read operations=0\n07-11-2017 18:03:48 CST run_dataimport INFO - \t\tFILE: Number of write operations=0\n07-11-2017 18:03:48 CST run_dataimport INFO - \t\tHDFS: Number of bytes read=462\n07-11-2017 18:03:48 CST run_dataimport INFO - \t\tHDFS: Number of bytes written=93800000\n07-11-2017 18:03:48 CST run_dataimport INFO - \t\tHDFS: Number of read operations=16\n07-11-2017 18:03:48 CST run_dataimport INFO - \t\tHDFS: Number of large read operations=0\n07-11-2017 18:03:48 CST run_dataimport INFO - \t\tHDFS: Number of write operations=8\n07-11-2017 18:03:48 CST run_dataimport INFO - \tJob Counters \n07-11-2017 18:03:48 CST run_dataimport INFO - \t\tLaunched map tasks=4\n07-11-2017 18:03:48 CST run_dataimport INFO - \t\tOther local map tasks=4\n07-11-2017 18:03:48 CST run_dataimport INFO - \t\tTotal time spent by all maps in occupied slots (ms)=9712787\n07-11-2017 18:03:48 CST run_dataimport INFO - \t\tTotal time spent by all reduces in occupied slots (ms)=0\n07-11-2017 18:03:48 CST run_dataimport INFO - \t\tTotal time spent by all map tasks (ms)=9712787\n07-11-2017 18:03:48 CST run_dataimport INFO - \t\tTotal vcore-seconds taken by all map tasks=9712787\n07-11-2017 18:03:48 CST run_dataimport INFO - \t\tTotal megabyte-seconds taken by all map tasks=9945893888\n07-11-2017 18:03:48 CST run_dataimport INFO - \tMap-Reduce Framework\n07-11-2017 18:03:48 CST run_dataimport INFO - \t\tMap input records=200000\n07-11-2017 18:03:48 CST run_dataimport INFO - \t\tMap output records=200000\n07-11-2017 18:03:48 CST run_dataimport INFO - \t\tInput split bytes=462\n07-11-2017 18:03:48 CST run_dataimport INFO - \t\tSpilled Records=0\n07-11-2017 18:03:48 CST run_dataimport INFO - \t\tFailed Shuffles=0\n07-11-2017 18:03:48 CST run_dataimport INFO - \t\tMerged Map outputs=0\n07-11-2017 18:03:48 CST run_dataimport INFO - \t\tGC time elapsed (ms)=764\n07-11-2017 18:03:48 CST run_dataimport INFO - \t\tCPU time spent (ms)=64470\n07-11-2017 18:03:48 CST run_dataimport INFO - \t\tPhysical memory (bytes) snapshot=1473638400\n07-11-2017 18:03:48 CST run_dataimport INFO - \t\tVirtual memory (bytes) snapshot=6498091008\n07-11-2017 18:03:48 CST run_dataimport INFO - \t\tTotal committed heap usage (bytes)=3240099840\n07-11-2017 18:03:48 CST run_dataimport INFO - \tFile Input Format Counters \n07-11-2017 18:03:48 CST run_dataimport INFO - \t\tBytes Read=0\n07-11-2017 18:03:48 CST run_dataimport INFO - \tFile Output Format Counters \n07-11-2017 18:03:48 CST run_dataimport INFO - \t\tBytes Written=93800000\n07-11-2017 18:03:48 CST run_dataimport INFO - 17/11/07 18:03:48 INFO mapreduce.ImportJobBase: Transferred 89.4547 MB in 4,879.8256 seconds (18.7715 KB/sec)\n07-11-2017 18:03:48 CST run_dataimport INFO - 17/11/07 18:03:48 INFO mapreduce.ImportJobBase: Retrieved 200000 records.\n07-11-2017 18:03:48 CST run_dataimport INFO - 17/11/07 18:03:48 INFO util.AppendUtils: Appending to directory sql_to_hive\n07-11-2017 18:03:48 CST run_dataimport INFO - 17/11/07 18:03:48 INFO util.AppendUtils: Using found partition 6804\n07-11-2017 18:03:49 CST run_dataimport INFO - 17/11/07 18:03:49 INFO tool.ImportTool: Incremental import complete! To run another incremental import of all data following this import, supply the following arguments:\n07-11-2017 18:03:49 CST run_dataimport INFO - 17/11/07 18:03:49 INFO tool.ImportTool:  --incremental append\n07-11-2017 18:03:49 CST run_dataimport INFO - 17/11/07 18:03:49 INFO tool.ImportTool:   --check-column uid\n07-11-2017 18:03:49 CST run_dataimport INFO - 17/11/07 18:03:49 INFO tool.ImportTool:   --last-value 83622300\n07-11-2017 18:03:49 CST run_dataimport INFO - 17/11/07 18:03:49 INFO tool.ImportTool: (Consider saving this with 'sqoop job --create')\n07-11-2017 18:03:49 CST run_dataimport INFO - 5\n07-11-2017 18:03:49 CST run_dataimport INFO - Warning: /opt/cloudera/parcels/CDH-5.8.0-1.cdh5.8.0.p0.42/bin/../lib/sqoop/../accumulo does not exist! Accumulo imports will fail.\n07-11-2017 18:03:49 CST run_dataimport INFO - Please set $ACCUMULO_HOME to the root of your Accumulo installation.\n07-11-2017 18:03:51 CST run_dataimport INFO - 17/11/07 18:03:51 INFO sqoop.Sqoop: Running Sqoop version: 1.4.6-cdh5.8.0\n07-11-2017 18:03:51 CST run_dataimport INFO - 17/11/07 18:03:51 WARN sqoop.ConnFactory: Parameter --driver is set to an explicit driver however appropriate connection manager is not being set (via --connection-manager). Sqoop is going to fall back to org.apache.sqoop.manager.GenericJdbcManager. Please specify explicitly which connection manager should be used next time.\n07-11-2017 18:03:51 CST run_dataimport INFO - 17/11/07 18:03:51 INFO manager.SqlManager: Using default fetchSize of 1000\n07-11-2017 18:03:51 CST run_dataimport INFO - 17/11/07 18:03:51 INFO tool.CodeGenTool: Beginning code generation\n07-11-2017 18:03:52 CST run_dataimport INFO - 17/11/07 18:03:52 INFO manager.SqlManager: Executing SQL statement: select top 200000 * from jzj.dbo.jzj_test where uid&gt;(select uid from (select top 400000 uid from jzj.dbo.jzj_test where uid&gt;83222300 and uid&lt;=84598500) a except select uid from (select top 399999 uid from jzj.dbo.jzj_test where uid&gt;83222300 and uid&lt;=84598500) a) and uid &lt;=84598500 and  (1 = 0) \n07-11-2017 18:03:52 CST run_dataimport INFO - 17/11/07 18:03:52 INFO manager.SqlManager: Executing SQL statement: select top 200000 * from jzj.dbo.jzj_test where uid&gt;(select uid from (select top 400000 uid from jzj.dbo.jzj_test where uid&gt;83222300 and uid&lt;=84598500) a except select uid from (select top 399999 uid from jzj.dbo.jzj_test where uid&gt;83222300 and uid&lt;=84598500) a) and uid &lt;=84598500 and  (1 = 0) \n07-11-2017 18:03:52 CST run_dataimport INFO - 17/11/07 18:03:52 INFO orm.CompilationManager: HADOOP_MAPRED_HOME is /opt/cloudera/parcels/CDH/lib/hadoop-mapreduce\n07-11-2017 18:03:54 CST run_dataimport INFO - Note: /tmp/sqoop-root/compile/21f23a42adb9bf71452da9a3c7187b06/QueryResult.java uses or overrides a deprecated API.\n07-11-2017 18:03:54 CST run_dataimport INFO - Note: Recompile with -Xlint:deprecation for details.\n07-11-2017 18:03:54 CST run_dataimport INFO - 17/11/07 18:03:54 INFO orm.CompilationManager: Writing jar file: /tmp/sqoop-root/compile/21f23a42adb9bf71452da9a3c7187b06/QueryResult.jar\n07-11-2017 18:03:55 CST run_dataimport INFO - 17/11/07 18:03:55 INFO tool.ImportTool: Maximal id query for free form incremental import: SELECT MAX(uid) FROM (select top 200000 * from jzj.dbo.jzj_test where uid&gt;(select uid from (select top 400000 uid from jzj.dbo.jzj_test where uid&gt;83222300 and uid&lt;=84598500) a except select uid from (select top 399999 uid from jzj.dbo.jzj_test where uid&gt;83222300 and uid&lt;=84598500) a) and uid &lt;=84598500 and (1 = 1)) sqoop_import_query_alias\n07-11-2017 20:43:27 CST run_dataimport INFO - 17/11/07 20:43:27 INFO tool.ImportTool: Incremental import based on column uid\n07-11-2017 20:43:27 CST run_dataimport INFO - 17/11/07 20:43:27 INFO tool.ImportTool: Upper bound value: 83822300\n07-11-2017 20:43:27 CST run_dataimport INFO - 17/11/07 20:43:27 INFO mapreduce.ImportJobBase: Beginning query import.\n07-11-2017 20:43:27 CST run_dataimport INFO - 17/11/07 20:43:27 INFO Configuration.deprecation: mapred.jar is deprecated. Instead, use mapreduce.job.jar\n07-11-2017 20:43:27 CST run_dataimport INFO - 17/11/07 20:43:27 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n07-11-2017 20:43:28 CST run_dataimport INFO - 17/11/07 20:43:28 INFO client.RMProxy: Connecting to ResourceManager at master/172.23.27.203:8032\n07-11-2017 20:43:34 CST run_dataimport INFO - 17/11/07 20:43:34 INFO db.DBInputFormat: Using read commited transaction isolation\n07-11-2017 20:43:34 CST run_dataimport INFO - 17/11/07 20:43:34 INFO db.DataDrivenDBInputFormat: BoundingValsQuery: SELECT MIN(uid), MAX(uid) FROM (select top 200000 * from jzj.dbo.jzj_test where uid&gt;(select uid from (select top 400000 uid from jzj.dbo.jzj_test where uid&gt;83222300 and uid&lt;=84598500) a except select uid from (select top 399999 uid from jzj.dbo.jzj_test where uid&gt;83222300 and uid&lt;=84598500) a) and uid &lt;=84598500 and uid &lt;= 83822300 AND  (1 = 1) ) AS t1\n07-11-2017 23:22:41 CST run_dataimport INFO - 17/11/07 23:22:41 INFO mapreduce.JobSubmitter: number of splits:4\n07-11-2017 23:22:41 CST run_dataimport INFO - 17/11/07 23:22:41 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1508671365448_2956\n07-11-2017 23:22:42 CST run_dataimport INFO - 17/11/07 23:22:42 INFO impl.YarnClientImpl: Submitted application application_1508671365448_2956\n07-11-2017 23:22:42 CST run_dataimport INFO - 17/11/07 23:22:42 INFO mapreduce.Job: The url to track the job: http://master:8088/proxy/application_1508671365448_2956/\n07-11-2017 23:22:42 CST run_dataimport INFO - 17/11/07 23:22:42 INFO mapreduce.Job: Running job: job_1508671365448_2956\n07-11-2017 23:22:49 CST run_dataimport INFO - 17/11/07 23:22:49 INFO mapreduce.Job: Job job_1508671365448_2956 running in uber mode : false\n07-11-2017 23:22:49 CST run_dataimport INFO - 17/11/07 23:22:49 INFO mapreduce.Job:  map 0% reduce 0%\n08-11-2017 02:00:13 CST run_dataimport INFO - 17/11/08 02:00:13 INFO mapreduce.Job:  map 25% reduce 0%\n08-11-2017 02:01:28 CST run_dataimport INFO - 17/11/08 02:01:28 INFO mapreduce.Job:  map 50% reduce 0%\n08-11-2017 02:03:21 CST run_dataimport INFO - 17/11/08 02:03:21 INFO mapreduce.Job:  map 75% reduce 0%\n08-11-2017 02:03:22 CST run_dataimport INFO - 17/11/08 02:03:22 INFO mapreduce.Job:  map 100% reduce 0%\n08-11-2017 02:03:23 CST run_dataimport INFO - 17/11/08 02:03:23 INFO mapreduce.Job: Job job_1508671365448_2956 completed successfully\n08-11-2017 02:03:23 CST run_dataimport INFO - 17/11/08 02:03:23 INFO mapreduce.Job: Counters: 30\n08-11-2017 02:03:23 CST run_dataimport INFO - \tFile System Counters\n08-11-2017 02:03:23 CST run_dataimport INFO - \t\tFILE: Number of bytes read=0\n08-11-2017 02:03:23 CST run_dataimport INFO - \t\tFILE: Number of bytes written=588300\n08-11-2017 02:03:23 CST run_dataimport INFO - \t\tFILE: Number of read operations=0\n08-11-2017 02:03:23 CST run_dataimport INFO - \t\tFILE: Number of large read operations=0\n08-11-2017 02:03:23 CST run_dataimport INFO - \t\tFILE: Number of write operations=0\n08-11-2017 02:03:23 CST run_dataimport INFO - \t\tHDFS: Number of bytes read=462\n08-11-2017 02:03:23 CST run_dataimport INFO - \t\tHDFS: Number of bytes written=93800000\n08-11-2017 02:03:23 CST run_dataimport INFO - \t\tHDFS: Number of read operations=16\n08-11-2017 02:03:23 CST run_dataimport INFO - \t\tHDFS: Number of large read operations=0\n08-11-2017 02:03:23 CST run_dataimport INFO - \t\tHDFS: Number of write operations=8\n08-11-2017 02:03:23 CST run_dataimport INFO - \tJob Counters \n08-11-2017 02:03:23 CST run_dataimport INFO - \t\tLaunched map tasks=4\n08-11-2017 02:03:23 CST run_dataimport INFO - \t\tOther local map tasks=4\n08-11-2017 02:03:23 CST run_dataimport INFO - \t\tTotal time spent by all maps in occupied slots (ms)=38216711\n08-11-2017 02:03:23 CST run_dataimport INFO - \t\tTotal time spent by all reduces in occupied slots (ms)=0\n08-11-2017 02:03:23 CST run_dataimport INFO - \t\tTotal time spent by all map tasks (ms)=38216711\n08-11-2017 02:03:23 CST run_dataimport INFO - \t\tTotal vcore-seconds taken by all map tasks=38216711\n08-11-2017 02:03:23 CST run_dataimport INFO - \t\tTotal megabyte-seconds taken by all map tasks=39133912064\n08-11-2017 02:03:23 CST run_dataimport INFO - \tMap-Reduce Framework\n08-11-2017 02:03:23 CST run_dataimport INFO - \t\tMap input records=200000\n08-11-2017 02:03:23 CST run_dataimport INFO - \t\tMap output records=200000\n08-11-2017 02:03:23 CST run_dataimport INFO - \t\tInput split bytes=462\n08-11-2017 02:03:23 CST run_dataimport INFO - \t\tSpilled Records=0\n08-11-2017 02:03:23 CST run_dataimport INFO - \t\tFailed Shuffles=0\n08-11-2017 02:03:23 CST run_dataimport INFO - \t\tMerged Map outputs=0\n08-11-2017 02:03:23 CST run_dataimport INFO - \t\tGC time elapsed (ms)=1588\n08-11-2017 02:03:23 CST run_dataimport INFO - \t\tCPU time spent (ms)=134830\n08-11-2017 02:03:23 CST run_dataimport INFO - \t\tPhysical memory (bytes) snapshot=1547542528\n08-11-2017 02:03:23 CST run_dataimport INFO - \t\tVirtual memory (bytes) snapshot=6530818048\n08-11-2017 02:03:23 CST run_dataimport INFO - \t\tTotal committed heap usage (bytes)=3172990976\n08-11-2017 02:03:23 CST run_dataimport INFO - \tFile Input Format Counters \n08-11-2017 02:03:23 CST run_dataimport INFO - \t\tBytes Read=0\n08-11-2017 02:03:23 CST run_dataimport INFO - \tFile Output Format Counters \n08-11-2017 02:03:23 CST run_dataimport INFO - \t\tBytes Written=93800000\n08-11-2017 02:03:23 CST run_dataimport INFO - 17/11/08 02:03:23 INFO mapreduce.ImportJobBase: Transferred 89.4547 MB in 19,195.7726 seconds (4.772 KB/sec)\n08-11-2017 02:03:23 CST run_dataimport INFO - 17/11/08 02:03:23 INFO mapreduce.ImportJobBase: Retrieved 200000 records.\n08-11-2017 02:03:23 CST run_dataimport INFO - 17/11/08 02:03:23 INFO util.AppendUtils: Appending to directory sql_to_hive\n08-11-2017 02:03:24 CST run_dataimport INFO - 17/11/08 02:03:24 INFO util.AppendUtils: Using found partition 6808\n08-11-2017 02:03:24 CST run_dataimport INFO - 17/11/08 02:03:24 INFO tool.ImportTool: Incremental import complete! To run another incremental import of all data following this import, supply the following arguments:\n08-11-2017 02:03:24 CST run_dataimport INFO - 17/11/08 02:03:24 INFO tool.ImportTool:  --incremental append\n08-11-2017 02:03:24 CST run_dataimport INFO - 17/11/08 02:03:24 INFO tool.ImportTool:   --check-column uid\n08-11-2017 02:03:24 CST run_dataimport INFO - 17/11/08 02:03:24 INFO tool.ImportTool:   --last-value 83822300\n08-11-2017 02:03:24 CST run_dataimport INFO - 17/11/08 02:03:24 INFO tool.ImportTool: (Consider saving this with 'sqoop job --create')\n08-11-2017 02:03:24 CST run_dataimport INFO - 4\n08-11-2017 02:03:24 CST run_dataimport INFO - Warning: /opt/cloudera/parcels/CDH-5.8.0-1.cdh5.8.0.p0.42/bin/../lib/sqoop/../accumulo does not exist! Accumulo imports will fail.\n08-11-2017 02:03:24 CST run_dataimport INFO - Please set $ACCUMULO_HOME to the root of your Accumulo installation.\n08-11-2017 02:03:26 CST run_dataimport INFO - 17/11/08 02:03:26 INFO sqoop.Sqoop: Running Sqoop version: 1.4.6-cdh5.8.0\n08-11-2017 02:03:26 CST run_dataimport INFO - 17/11/08 02:03:26 WARN sqoop.ConnFactory: Parameter --driver is set to an explicit driver however appropriate connection manager is not being set (via --connection-manager). Sqoop is going to fall back to org.apache.sqoop.manager.GenericJdbcManager. Please specify explicitly which connection manager should be used next time.\n08-11-2017 02:03:26 CST run_dataimport INFO - 17/11/08 02:03:26 INFO manager.SqlManager: Using default fetchSize of 1000\n08-11-2017 02:03:26 CST run_dataimport INFO - 17/11/08 02:03:26 INFO tool.CodeGenTool: Beginning code generation\n08-11-2017 02:03:27 CST run_dataimport INFO - 17/11/08 02:03:27 INFO manager.SqlManager: Executing SQL statement: select top 200000 * from jzj.dbo.jzj_test where uid&gt;(select uid from (select top 600000 uid from jzj.dbo.jzj_test where uid&gt;83222300 and uid&lt;=84598500) a except select uid from (select top 599999 uid from jzj.dbo.jzj_test where uid&gt;83222300 and uid&lt;=84598500) a) and uid &lt;=84598500 and  (1 = 0) \n08-11-2017 02:03:27 CST run_dataimport INFO - 17/11/08 02:03:27 INFO manager.SqlManager: Executing SQL statement: select top 200000 * from jzj.dbo.jzj_test where uid&gt;(select uid from (select top 600000 uid from jzj.dbo.jzj_test where uid&gt;83222300 and uid&lt;=84598500) a except select uid from (select top 599999 uid from jzj.dbo.jzj_test where uid&gt;83222300 and uid&lt;=84598500) a) and uid &lt;=84598500 and  (1 = 0) \n08-11-2017 02:03:27 CST run_dataimport INFO - 17/11/08 02:03:27 INFO orm.CompilationManager: HADOOP_MAPRED_HOME is /opt/cloudera/parcels/CDH/lib/hadoop-mapreduce\n08-11-2017 02:03:29 CST run_dataimport INFO - Note: /tmp/sqoop-root/compile/e8acb4e9422617ba257a228b6dfa0561/QueryResult.java uses or overrides a deprecated API.\n08-11-2017 02:03:29 CST run_dataimport INFO - Note: Recompile with -Xlint:deprecation for details.\n08-11-2017 02:03:29 CST run_dataimport INFO - 17/11/08 02:03:29 INFO orm.CompilationManager: Writing jar file: /tmp/sqoop-root/compile/e8acb4e9422617ba257a228b6dfa0561/QueryResult.jar\n08-11-2017 02:03:30 CST run_dataimport INFO - 17/11/08 02:03:30 INFO tool.ImportTool: Maximal id query for free form incremental import: SELECT MAX(uid) FROM (select top 200000 * from jzj.dbo.jzj_test where uid&gt;(select uid from (select top 600000 uid from jzj.dbo.jzj_test where uid&gt;83222300 and uid&lt;=84598500) a except select uid from (select top 599999 uid from jzj.dbo.jzj_test where uid&gt;83222300 and uid&lt;=84598500) a) and uid &lt;=84598500 and (1 = 1)) sqoop_import_query_alias\n08-11-2017 07:55:53 CST run_dataimport INFO - 17/11/08 07:55:53 INFO tool.ImportTool: Incremental import based on column uid\n08-11-2017 07:55:53 CST run_dataimport INFO - 17/11/08 07:55:53 INFO tool.ImportTool: Upper bound value: 84022300\n08-11-2017 07:55:53 CST run_dataimport INFO - 17/11/08 07:55:53 INFO mapreduce.ImportJobBase: Beginning query import.\n08-11-2017 07:55:53 CST run_dataimport INFO - 17/11/08 07:55:53 INFO Configuration.deprecation: mapred.jar is deprecated. Instead, use mapreduce.job.jar\n08-11-2017 07:55:53 CST run_dataimport INFO - 17/11/08 07:55:53 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n08-11-2017 07:55:53 CST run_dataimport INFO - 17/11/08 07:55:53 INFO client.RMProxy: Connecting to ResourceManager at master/172.23.27.203:8032\n08-11-2017 07:56:00 CST run_dataimport INFO - 17/11/08 07:56:00 INFO db.DBInputFormat: Using read commited transaction isolation\n08-11-2017 07:56:00 CST run_dataimport INFO - 17/11/08 07:56:00 INFO db.DataDrivenDBInputFormat: BoundingValsQuery: SELECT MIN(uid), MAX(uid) FROM (select top 200000 * from jzj.dbo.jzj_test where uid&gt;(select uid from (select top 600000 uid from jzj.dbo.jzj_test where uid&gt;83222300 and uid&lt;=84598500) a except select uid from (select top 599999 uid from jzj.dbo.jzj_test where uid&gt;83222300 and uid&lt;=84598500) a) and uid &lt;=84598500 and uid &lt;= 84022300 AND  (1 = 1) ) AS t1\n08-11-2017 12:11:01 CST run_dataimport ERROR - Kill has been called.\n08-11-2017 12:11:06 CST run_dataimport INFO - Process completed unsuccessfully in 72615 seconds.\n08-11-2017 12:11:06 CST run_dataimport ERROR - Job run killed!\njava.lang.RuntimeException: azkaban.jobExecutor.utils.process.ProcessFailureException\n\tat azkaban.jobExecutor.ProcessJob.run(ProcessJob.java:297)\n\tat azkaban.execapp.JobRunner.runJob(JobRunner.java:748)\n\tat azkaban.execapp.JobRunner.doRun(JobRunner.java:591)\n\tat azkaban.execapp.JobRunner.run(JobRunner.java:552)\n\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: azkaban.jobExecutor.utils.process.ProcessFailureException\n\tat azkaban.jobExecutor.utils.process.AzkabanProcess.run(AzkabanProcess.java:130)\n\tat azkaban.jobExecutor.ProcessJob.run(ProcessJob.java:289)\n\t... 8 more\n08-11-2017 12:11:06 CST run_dataimport ERROR - azkaban.jobExecutor.utils.process.ProcessFailureException cause: azkaban.jobExecutor.utils.process.ProcessFailureException\n08-11-2017 12:11:06 CST run_dataimport INFO - Finishing job run_dataimport at 1510114266951 with status KILLED\n", 'length': 37422}
[2017-11-20 16:35:07,654] app.hive_mysql.AzkabanMonitor [INFO] URL: https://172.23.27.203:8443
[2017-11-20 16:35:10,838] app.hive_mysql.AzkabanMonitor [INFO] Success: Login https://172.23.27.203:8443 with user azkaban
[2017-11-20 16:35:10,838] app.hive_mysql.AzkabanMonitor [DEBUG] {'session.id': 'cb024d5c-ddfe-4e3c-bfde-6c6540317d84', 'status': 'success'}
[2017-11-20 16:35:10,850] app.hive_mysql.AzkabanMonitor [INFO] Success: Fetch Executions of a Flow - run_dataimport
[2017-11-20 16:35:10,851] app.hive_mysql.AzkabanMonitor [DEBUG] {'total': 16, 'executions': [{'submitTime': 1510041651107, 'submitUser': 'azkaban', 'startTime': 1510041651277, 'endTime': 1510114267057, 'flowId': 'run_dataimport', 'projectId': 68, 'execId': 1670, 'status': 'KILLED'}, {'submitTime': 1510027251078, 'submitUser': 'azkaban', 'startTime': 1510027251395, 'endTime': 1510036906931, 'flowId': 'run_dataimport', 'projectId': 68, 'execId': 1669, 'status': 'SUCCEEDED'}, {'submitTime': 1509897650826, 'submitUser': 'azkaban', 'startTime': 1509897651116, 'endTime': 1510026870568, 'flowId': 'run_dataimport', 'projectId': 68, 'execId': 1668, 'status': 'SUCCEEDED'}, {'submitTime': 1509883250800, 'submitUser': 'azkaban', 'startTime': 1509883251056, 'endTime': 1509892870422, 'flowId': 'run_dataimport', 'projectId': 68, 'execId': 1667, 'status': 'SUCCEEDED'}, {'submitTime': 1509753650545, 'submitUser': 'azkaban', 'startTime': 1509753650705, 'endTime': 1509881095432, 'flowId': 'run_dataimport', 'projectId': 68, 'execId': 1666, 'status': 'SUCCEEDED'}, {'submitTime': 1509746450529, 'submitUser': 'azkaban', 'startTime': 1509746450965, 'endTime': 1509753076918, 'flowId': 'run_dataimport', 'projectId': 68, 'execId': 1665, 'status': 'SUCCEEDED'}, {'submitTime': 1509652850362, 'submitUser': 'azkaban', 'startTime': 1509652850560, 'endTime': 1509744409191, 'flowId': 'run_dataimport', 'projectId': 68, 'execId': 1664, 'status': 'SUCCEEDED'}, {'submitTime': 1509645650349, 'submitUser': 'azkaban', 'startTime': 1509645650498, 'endTime': 1509651159750, 'flowId': 'run_dataimport', 'projectId': 68, 'execId': 1663, 'status': 'SUCCEEDED'}, {'submitTime': 1509566450178, 'submitUser': 'azkaban', 'startTime': 1509566450488, 'endTime': 1509642929867, 'flowId': 'run_dataimport', 'projectId': 68, 'execId': 1661, 'status': 'SUCCEEDED'}, {'submitTime': 1509559250163, 'submitUser': 'azkaban', 'startTime': 1509559250321, 'endTime': 1509563186239, 'flowId': 'run_dataimport', 'projectId': 68, 'execId': 1660, 'status': 'SUCCEEDED'}], 'length': 10, 'project': 'sql_to_hive', 'from': 0, 'projectId': 68, 'flow': 'run_dataimport'}
[2017-11-20 16:35:10,865] app.hive_mysql.AzkabanMonitor [INFO] Success: Fetch Execution Job Logs - 1670-run_dataimport
[2017-11-20 16:35:10,865] app.hive_mysql.AzkabanMonitor [DEBUG] {'offset': 0, 'data': "07-11-2017 16:00:51 CST run_dataimport INFO - Starting job run_dataimport at 1510041651281\n07-11-2017 16:00:51 CST run_dataimport INFO - azkaban.webserver.url property was not set\n07-11-2017 16:00:51 CST run_dataimport INFO - job JVM args: -Dazkaban.flowid=run_dataimport -Dazkaban.execid=1670 -Dazkaban.jobid=run_dataimport\n07-11-2017 16:00:51 CST run_dataimport INFO - Building command job executor. \n07-11-2017 16:00:51 CST run_dataimport INFO - Memory granted for job run_dataimport\n07-11-2017 16:00:51 CST run_dataimport INFO - 1 commands to execute.\n07-11-2017 16:00:51 CST run_dataimport INFO - cwd=/home/azkaban/azkaban_bk/azkaban-exec-server-3.35.0-22-g5517d50/executions/1670\n07-11-2017 16:00:51 CST run_dataimport INFO - effective user is: azkaban\n07-11-2017 16:00:51 CST run_dataimport INFO - Command: sh /home/sql_to_hive/dataimport.sh\n07-11-2017 16:00:51 CST run_dataimport INFO - Environment variables: {JOB_OUTPUT_PROP_FILE=/home/azkaban/azkaban_bk/azkaban-exec-server-3.35.0-22-g5517d50/executions/1670/run_dataimport_output_8807970133616151260_tmp, JOB_PROP_FILE=/home/azkaban/azkaban_bk/azkaban-exec-server-3.35.0-22-g5517d50/executions/1670/run_dataimport_props_6140684690151138768_tmp, KRB5CCNAME=/tmp/krb5cc__sql_to_hive__run_dataimport__run_dataimport__1670__azkaban, JOB_NAME=run_dataimport}\n07-11-2017 16:00:51 CST run_dataimport INFO - Working directory: /home/azkaban/azkaban_bk/azkaban-exec-server-3.35.0-22-g5517d50/executions/1670\n07-11-2017 16:00:51 CST run_dataimport INFO - log4j:WARN No appenders could be found for logger (org.apache.hive.jdbc.Utils).\n07-11-2017 16:00:51 CST run_dataimport INFO - log4j:WARN Please initialize the log4j system properly.\n07-11-2017 16:00:51 CST run_dataimport INFO - log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.\n07-11-2017 16:00:51 CST run_dataimport INFO - SLF4J: Failed to load class &quot;org.slf4j.impl.StaticLoggerBinder&quot;.\n07-11-2017 16:00:51 CST run_dataimport INFO - SLF4J: Defaulting to no-operation (NOP) logger implementation\n07-11-2017 16:00:51 CST run_dataimport INFO - SLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.\n07-11-2017 16:02:05 CST run_dataimport INFO - 1,84598500,83222300,1376200sqlmax:84598500\n07-11-2017 16:02:05 CST run_dataimport INFO - hivemax:83222300\n07-11-2017 16:02:05 CST run_dataimport INFO - counts:1376200\n07-11-2017 16:02:05 CST run_dataimport INFO - 7,176200\n07-11-2017 16:02:05 CST run_dataimport INFO - 7\n07-11-2017 16:02:05 CST run_dataimport INFO - Warning: /opt/cloudera/parcels/CDH-5.8.0-1.cdh5.8.0.p0.42/bin/../lib/sqoop/../accumulo does not exist! Accumulo imports will fail.\n07-11-2017 16:02:05 CST run_dataimport INFO - Please set $ACCUMULO_HOME to the root of your Accumulo installation.\n07-11-2017 16:02:07 CST run_dataimport INFO - 17/11/07 16:02:07 INFO sqoop.Sqoop: Running Sqoop version: 1.4.6-cdh5.8.0\n07-11-2017 16:02:07 CST run_dataimport INFO - 17/11/07 16:02:07 WARN sqoop.ConnFactory: Parameter --driver is set to an explicit driver however appropriate connection manager is not being set (via --connection-manager). Sqoop is going to fall back to org.apache.sqoop.manager.GenericJdbcManager. Please specify explicitly which connection manager should be used next time.\n07-11-2017 16:02:07 CST run_dataimport INFO - 17/11/07 16:02:07 INFO manager.SqlManager: Using default fetchSize of 1000\n07-11-2017 16:02:07 CST run_dataimport INFO - 17/11/07 16:02:07 INFO tool.CodeGenTool: Beginning code generation\n07-11-2017 16:02:08 CST run_dataimport INFO - 17/11/07 16:02:08 INFO manager.SqlManager: Executing SQL statement: select top 200000 * from jzj.dbo.jzj_test where uid&gt;=(select uid from (select top 1 uid from jzj.dbo.jzj_test where uid&gt;83222300 and uid&lt;=84598500) a) and uid &lt;=84598500 and  (1 = 0) \n07-11-2017 16:02:08 CST run_dataimport INFO - 17/11/07 16:02:08 INFO manager.SqlManager: Executing SQL statement: select top 200000 * from jzj.dbo.jzj_test where uid&gt;=(select uid from (select top 1 uid from jzj.dbo.jzj_test where uid&gt;83222300 and uid&lt;=84598500) a) and uid &lt;=84598500 and  (1 = 0) \n07-11-2017 16:02:08 CST run_dataimport INFO - 17/11/07 16:02:08 INFO orm.CompilationManager: HADOOP_MAPRED_HOME is /opt/cloudera/parcels/CDH/lib/hadoop-mapreduce\n07-11-2017 16:02:10 CST run_dataimport INFO - Note: /tmp/sqoop-root/compile/b796f2c0b0198c05bdcf8149b3f9a7fd/QueryResult.java uses or overrides a deprecated API.\n07-11-2017 16:02:10 CST run_dataimport INFO - Note: Recompile with -Xlint:deprecation for details.\n07-11-2017 16:02:10 CST run_dataimport INFO - 17/11/07 16:02:10 INFO orm.CompilationManager: Writing jar file: /tmp/sqoop-root/compile/b796f2c0b0198c05bdcf8149b3f9a7fd/QueryResult.jar\n07-11-2017 16:02:11 CST run_dataimport INFO - 17/11/07 16:02:11 INFO tool.ImportTool: Maximal id query for free form incremental import: SELECT MAX(uid) FROM (select top 200000 * from jzj.dbo.jzj_test where uid&gt;=(select uid from (select top 1 uid from jzj.dbo.jzj_test where uid&gt;83222300 and uid&lt;=84598500) a) and uid &lt;=84598500 and (1 = 1)) sqoop_import_query_alias\n07-11-2017 16:02:11 CST run_dataimport INFO - 17/11/07 16:02:11 INFO tool.ImportTool: Incremental import based on column uid\n07-11-2017 16:02:11 CST run_dataimport INFO - 17/11/07 16:02:11 INFO tool.ImportTool: Upper bound value: 83422300\n07-11-2017 16:02:11 CST run_dataimport INFO - 17/11/07 16:02:11 INFO mapreduce.ImportJobBase: Beginning query import.\n07-11-2017 16:02:11 CST run_dataimport INFO - 17/11/07 16:02:11 INFO Configuration.deprecation: mapred.jar is deprecated. Instead, use mapreduce.job.jar\n07-11-2017 16:02:11 CST run_dataimport INFO - 17/11/07 16:02:11 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n07-11-2017 16:02:11 CST run_dataimport INFO - 17/11/07 16:02:11 INFO client.RMProxy: Connecting to ResourceManager at master/172.23.27.203:8032\n07-11-2017 16:02:18 CST run_dataimport INFO - 17/11/07 16:02:18 INFO db.DBInputFormat: Using read commited transaction isolation\n07-11-2017 16:02:18 CST run_dataimport INFO - 17/11/07 16:02:18 INFO db.DataDrivenDBInputFormat: BoundingValsQuery: SELECT MIN(uid), MAX(uid) FROM (select top 200000 * from jzj.dbo.jzj_test where uid&gt;=(select uid from (select top 1 uid from jzj.dbo.jzj_test where uid&gt;83222300 and uid&lt;=84598500) a) and uid &lt;=84598500 and uid &lt;= 83422300 AND  (1 = 1) ) AS t1\n07-11-2017 16:02:18 CST run_dataimport INFO - 17/11/07 16:02:18 INFO mapreduce.JobSubmitter: number of splits:4\n07-11-2017 16:02:18 CST run_dataimport INFO - 17/11/07 16:02:18 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1508671365448_2954\n07-11-2017 16:02:19 CST run_dataimport INFO - 17/11/07 16:02:19 INFO impl.YarnClientImpl: Submitted application application_1508671365448_2954\n07-11-2017 16:02:19 CST run_dataimport INFO - 17/11/07 16:02:19 INFO mapreduce.Job: The url to track the job: http://master:8088/proxy/application_1508671365448_2954/\n07-11-2017 16:02:19 CST run_dataimport INFO - 17/11/07 16:02:19 INFO mapreduce.Job: Running job: job_1508671365448_2954\n07-11-2017 16:02:26 CST run_dataimport INFO - 17/11/07 16:02:26 INFO mapreduce.Job: Job job_1508671365448_2954 running in uber mode : false\n07-11-2017 16:02:26 CST run_dataimport INFO - 17/11/07 16:02:26 INFO mapreduce.Job:  map 0% reduce 0%\n07-11-2017 16:02:37 CST run_dataimport INFO - 17/11/07 16:02:37 INFO mapreduce.Job:  map 25% reduce 0%\n07-11-2017 16:02:40 CST run_dataimport INFO - 17/11/07 16:02:40 INFO mapreduce.Job:  map 100% reduce 0%\n07-11-2017 16:02:41 CST run_dataimport INFO - 17/11/07 16:02:41 INFO mapreduce.Job: Job job_1508671365448_2954 completed successfully\n07-11-2017 16:02:41 CST run_dataimport INFO - 17/11/07 16:02:41 INFO mapreduce.Job: Counters: 30\n07-11-2017 16:02:41 CST run_dataimport INFO - \tFile System Counters\n07-11-2017 16:02:41 CST run_dataimport INFO - \t\tFILE: Number of bytes read=0\n07-11-2017 16:02:41 CST run_dataimport INFO - \t\tFILE: Number of bytes written=587024\n07-11-2017 16:02:41 CST run_dataimport INFO - \t\tFILE: Number of read operations=0\n07-11-2017 16:02:41 CST run_dataimport INFO - \t\tFILE: Number of large read operations=0\n07-11-2017 16:02:41 CST run_dataimport INFO - \t\tFILE: Number of write operations=0\n07-11-2017 16:02:41 CST run_dataimport INFO - \t\tHDFS: Number of bytes read=462\n07-11-2017 16:02:41 CST run_dataimport INFO - \t\tHDFS: Number of bytes written=93800000\n07-11-2017 16:02:41 CST run_dataimport INFO - \t\tHDFS: Number of read operations=16\n07-11-2017 16:02:41 CST run_dataimport INFO - \t\tHDFS: Number of large read operations=0\n07-11-2017 16:02:41 CST run_dataimport INFO - \t\tHDFS: Number of write operations=8\n07-11-2017 16:02:41 CST run_dataimport INFO - \tJob Counters \n07-11-2017 16:02:41 CST run_dataimport INFO - \t\tLaunched map tasks=4\n07-11-2017 16:02:41 CST run_dataimport INFO - \t\tOther local map tasks=4\n07-11-2017 16:02:41 CST run_dataimport INFO - \t\tTotal time spent by all maps in occupied slots (ms)=44342\n07-11-2017 16:02:41 CST run_dataimport INFO - \t\tTotal time spent by all reduces in occupied slots (ms)=0\n07-11-2017 16:02:41 CST run_dataimport INFO - \t\tTotal time spent by all map tasks (ms)=44342\n07-11-2017 16:02:41 CST run_dataimport INFO - \t\tTotal vcore-seconds taken by all map tasks=44342\n07-11-2017 16:02:41 CST run_dataimport INFO - \t\tTotal megabyte-seconds taken by all map tasks=45406208\n07-11-2017 16:02:41 CST run_dataimport INFO - \tMap-Reduce Framework\n07-11-2017 16:02:41 CST run_dataimport INFO - \t\tMap input records=200000\n07-11-2017 16:02:41 CST run_dataimport INFO - \t\tMap output records=200000\n07-11-2017 16:02:41 CST run_dataimport INFO - \t\tInput split bytes=462\n07-11-2017 16:02:41 CST run_dataimport INFO - \t\tSpilled Records=0\n07-11-2017 16:02:41 CST run_dataimport INFO - \t\tFailed Shuffles=0\n07-11-2017 16:02:41 CST run_dataimport INFO - \t\tMerged Map outputs=0\n07-11-2017 16:02:41 CST run_dataimport INFO - \t\tGC time elapsed (ms)=569\n07-11-2017 16:02:41 CST run_dataimport INFO - \t\tCPU time spent (ms)=36080\n07-11-2017 16:02:41 CST run_dataimport INFO - \t\tPhysical memory (bytes) snapshot=1458102272\n07-11-2017 16:02:41 CST run_dataimport INFO - \t\tVirtual memory (bytes) snapshot=6516502528\n07-11-2017 16:02:41 CST run_dataimport INFO - \t\tTotal committed heap usage (bytes)=3231186944\n07-11-2017 16:02:41 CST run_dataimport INFO - \tFile Input Format Counters \n07-11-2017 16:02:41 CST run_dataimport INFO - \t\tBytes Read=0\n07-11-2017 16:02:41 CST run_dataimport INFO - \tFile Output Format Counters \n07-11-2017 16:02:41 CST run_dataimport INFO - \t\tBytes Written=93800000\n07-11-2017 16:02:41 CST run_dataimport INFO - 17/11/07 16:02:41 INFO mapreduce.ImportJobBase: Transferred 89.4547 MB in 29.9824 seconds (2.9836 MB/sec)\n07-11-2017 16:02:41 CST run_dataimport INFO - 17/11/07 16:02:41 INFO mapreduce.ImportJobBase: Retrieved 200000 records.\n07-11-2017 16:02:41 CST run_dataimport INFO - 17/11/07 16:02:41 INFO util.AppendUtils: Appending to directory sql_to_hive\n07-11-2017 16:02:42 CST run_dataimport INFO - 17/11/07 16:02:42 INFO util.AppendUtils: Using found partition 6800\n07-11-2017 16:02:42 CST run_dataimport INFO - 17/11/07 16:02:42 INFO tool.ImportTool: Incremental import complete! To run another incremental import of all data following this import, supply the following arguments:\n07-11-2017 16:02:42 CST run_dataimport INFO - 17/11/07 16:02:42 INFO tool.ImportTool:  --incremental append\n07-11-2017 16:02:42 CST run_dataimport INFO - 17/11/07 16:02:42 INFO tool.ImportTool:   --check-column uid\n07-11-2017 16:02:42 CST run_dataimport INFO - 17/11/07 16:02:42 INFO tool.ImportTool:   --last-value 83422300\n07-11-2017 16:02:42 CST run_dataimport INFO - 17/11/07 16:02:42 INFO tool.ImportTool: (Consider saving this with 'sqoop job --create')\n07-11-2017 16:02:42 CST run_dataimport INFO - 6\n07-11-2017 16:02:42 CST run_dataimport INFO - Warning: /opt/cloudera/parcels/CDH-5.8.0-1.cdh5.8.0.p0.42/bin/../lib/sqoop/../accumulo does not exist! Accumulo imports will fail.\n07-11-2017 16:02:42 CST run_dataimport INFO - Please set $ACCUMULO_HOME to the root of your Accumulo installation.\n07-11-2017 16:02:44 CST run_dataimport INFO - 17/11/07 16:02:44 INFO sqoop.Sqoop: Running Sqoop version: 1.4.6-cdh5.8.0\n07-11-2017 16:02:44 CST run_dataimport INFO - 17/11/07 16:02:44 WARN sqoop.ConnFactory: Parameter --driver is set to an explicit driver however appropriate connection manager is not being set (via --connection-manager). Sqoop is going to fall back to org.apache.sqoop.manager.GenericJdbcManager. Please specify explicitly which connection manager should be used next time.\n07-11-2017 16:02:44 CST run_dataimport INFO - 17/11/07 16:02:44 INFO manager.SqlManager: Using default fetchSize of 1000\n07-11-2017 16:02:44 CST run_dataimport INFO - 17/11/07 16:02:44 INFO tool.CodeGenTool: Beginning code generation\n07-11-2017 16:02:45 CST run_dataimport INFO - 17/11/07 16:02:45 INFO manager.SqlManager: Executing SQL statement: select top 200000 * from jzj.dbo.jzj_test where uid&gt;(select uid from (select top 200000 uid from jzj.dbo.jzj_test where uid&gt;83222300 and uid&lt;=84598500) a except select uid from (select top 199999 uid from jzj.dbo.jzj_test where uid&gt;83222300 and uid&lt;=84598500) a) and uid &lt;=84598500 and  (1 = 0) \n07-11-2017 16:02:45 CST run_dataimport INFO - 17/11/07 16:02:45 INFO manager.SqlManager: Executing SQL statement: select top 200000 * from jzj.dbo.jzj_test where uid&gt;(select uid from (select top 200000 uid from jzj.dbo.jzj_test where uid&gt;83222300 and uid&lt;=84598500) a except select uid from (select top 199999 uid from jzj.dbo.jzj_test where uid&gt;83222300 and uid&lt;=84598500) a) and uid &lt;=84598500 and  (1 = 0) \n07-11-2017 16:02:45 CST run_dataimport INFO - 17/11/07 16:02:45 INFO orm.CompilationManager: HADOOP_MAPRED_HOME is /opt/cloudera/parcels/CDH/lib/hadoop-mapreduce\n07-11-2017 16:02:47 CST run_dataimport INFO - Note: /tmp/sqoop-root/compile/3e3fe91d38feffbabab3df86c4ddb2c4/QueryResult.java uses or overrides a deprecated API.\n07-11-2017 16:02:47 CST run_dataimport INFO - Note: Recompile with -Xlint:deprecation for details.\n07-11-2017 16:02:47 CST run_dataimport INFO - 17/11/07 16:02:47 INFO orm.CompilationManager: Writing jar file: /tmp/sqoop-root/compile/3e3fe91d38feffbabab3df86c4ddb2c4/QueryResult.jar\n07-11-2017 16:02:48 CST run_dataimport INFO - 17/11/07 16:02:48 INFO tool.ImportTool: Maximal id query for free form incremental import: SELECT MAX(uid) FROM (select top 200000 * from jzj.dbo.jzj_test where uid&gt;(select uid from (select top 200000 uid from jzj.dbo.jzj_test where uid&gt;83222300 and uid&lt;=84598500) a except select uid from (select top 199999 uid from jzj.dbo.jzj_test where uid&gt;83222300 and uid&lt;=84598500) a) and uid &lt;=84598500 and (1 = 1)) sqoop_import_query_alias\n07-11-2017 16:42:28 CST run_dataimport INFO - 17/11/07 16:42:28 INFO tool.ImportTool: Incremental import based on column uid\n07-11-2017 16:42:28 CST run_dataimport INFO - 17/11/07 16:42:28 INFO tool.ImportTool: Upper bound value: 83622300\n07-11-2017 16:42:28 CST run_dataimport INFO - 17/11/07 16:42:28 INFO mapreduce.ImportJobBase: Beginning query import.\n07-11-2017 16:42:28 CST run_dataimport INFO - 17/11/07 16:42:28 INFO Configuration.deprecation: mapred.jar is deprecated. Instead, use mapreduce.job.jar\n07-11-2017 16:42:28 CST run_dataimport INFO - 17/11/07 16:42:28 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n07-11-2017 16:42:28 CST run_dataimport INFO - 17/11/07 16:42:28 INFO client.RMProxy: Connecting to ResourceManager at master/172.23.27.203:8032\n07-11-2017 16:42:34 CST run_dataimport INFO - 17/11/07 16:42:34 INFO db.DBInputFormat: Using read commited transaction isolation\n07-11-2017 16:42:34 CST run_dataimport INFO - 17/11/07 16:42:34 INFO db.DataDrivenDBInputFormat: BoundingValsQuery: SELECT MIN(uid), MAX(uid) FROM (select top 200000 * from jzj.dbo.jzj_test where uid&gt;(select uid from (select top 200000 uid from jzj.dbo.jzj_test where uid&gt;83222300 and uid&lt;=84598500) a except select uid from (select top 199999 uid from jzj.dbo.jzj_test where uid&gt;83222300 and uid&lt;=84598500) a) and uid &lt;=84598500 and uid &lt;= 83622300 AND  (1 = 1) ) AS t1\n07-11-2017 17:22:49 CST run_dataimport INFO - 17/11/07 17:22:49 INFO mapreduce.JobSubmitter: number of splits:4\n07-11-2017 17:22:49 CST run_dataimport INFO - 17/11/07 17:22:49 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1508671365448_2955\n07-11-2017 17:22:49 CST run_dataimport INFO - 17/11/07 17:22:49 INFO impl.YarnClientImpl: Submitted application application_1508671365448_2955\n07-11-2017 17:22:49 CST run_dataimport INFO - 17/11/07 17:22:49 INFO mapreduce.Job: The url to track the job: http://master:8088/proxy/application_1508671365448_2955/\n07-11-2017 17:22:49 CST run_dataimport INFO - 17/11/07 17:22:49 INFO mapreduce.Job: Running job: job_1508671365448_2955\n07-11-2017 17:22:57 CST run_dataimport INFO - 17/11/07 17:22:57 INFO mapreduce.Job: Job job_1508671365448_2955 running in uber mode : false\n07-11-2017 17:22:57 CST run_dataimport INFO - 17/11/07 17:22:57 INFO mapreduce.Job:  map 0% reduce 0%\n07-11-2017 18:02:56 CST run_dataimport INFO - 17/11/07 18:02:56 INFO mapreduce.Job:  map 25% reduce 0%\n07-11-2017 18:03:23 CST run_dataimport INFO - 17/11/07 18:03:23 INFO mapreduce.Job:  map 50% reduce 0%\n07-11-2017 18:03:46 CST run_dataimport INFO - 17/11/07 18:03:46 INFO mapreduce.Job:  map 75% reduce 0%\n07-11-2017 18:03:47 CST run_dataimport INFO - 17/11/07 18:03:47 INFO mapreduce.Job:  map 100% reduce 0%\n07-11-2017 18:03:48 CST run_dataimport INFO - 17/11/07 18:03:48 INFO mapreduce.Job: Job job_1508671365448_2955 completed successfully\n07-11-2017 18:03:48 CST run_dataimport INFO - 17/11/07 18:03:48 INFO mapreduce.Job: Counters: 30\n07-11-2017 18:03:48 CST run_dataimport INFO - \tFile System Counters\n07-11-2017 18:03:48 CST run_dataimport INFO - \t\tFILE: Number of bytes read=0\n07-11-2017 18:03:48 CST run_dataimport INFO - \t\tFILE: Number of bytes written=588156\n07-11-2017 18:03:48 CST run_dataimport INFO - \t\tFILE: Number of read operations=0\n07-11-2017 18:03:48 CST run_dataimport INFO - \t\tFILE: Number of large read operations=0\n07-11-2017 18:03:48 CST run_dataimport INFO - \t\tFILE: Number of write operations=0\n07-11-2017 18:03:48 CST run_dataimport INFO - \t\tHDFS: Number of bytes read=462\n07-11-2017 18:03:48 CST run_dataimport INFO - \t\tHDFS: Number of bytes written=93800000\n07-11-2017 18:03:48 CST run_dataimport INFO - \t\tHDFS: Number of read operations=16\n07-11-2017 18:03:48 CST run_dataimport INFO - \t\tHDFS: Number of large read operations=0\n07-11-2017 18:03:48 CST run_dataimport INFO - \t\tHDFS: Number of write operations=8\n07-11-2017 18:03:48 CST run_dataimport INFO - \tJob Counters \n07-11-2017 18:03:48 CST run_dataimport INFO - \t\tLaunched map tasks=4\n07-11-2017 18:03:48 CST run_dataimport INFO - \t\tOther local map tasks=4\n07-11-2017 18:03:48 CST run_dataimport INFO - \t\tTotal time spent by all maps in occupied slots (ms)=9712787\n07-11-2017 18:03:48 CST run_dataimport INFO - \t\tTotal time spent by all reduces in occupied slots (ms)=0\n07-11-2017 18:03:48 CST run_dataimport INFO - \t\tTotal time spent by all map tasks (ms)=9712787\n07-11-2017 18:03:48 CST run_dataimport INFO - \t\tTotal vcore-seconds taken by all map tasks=9712787\n07-11-2017 18:03:48 CST run_dataimport INFO - \t\tTotal megabyte-seconds taken by all map tasks=9945893888\n07-11-2017 18:03:48 CST run_dataimport INFO - \tMap-Reduce Framework\n07-11-2017 18:03:48 CST run_dataimport INFO - \t\tMap input records=200000\n07-11-2017 18:03:48 CST run_dataimport INFO - \t\tMap output records=200000\n07-11-2017 18:03:48 CST run_dataimport INFO - \t\tInput split bytes=462\n07-11-2017 18:03:48 CST run_dataimport INFO - \t\tSpilled Records=0\n07-11-2017 18:03:48 CST run_dataimport INFO - \t\tFailed Shuffles=0\n07-11-2017 18:03:48 CST run_dataimport INFO - \t\tMerged Map outputs=0\n07-11-2017 18:03:48 CST run_dataimport INFO - \t\tGC time elapsed (ms)=764\n07-11-2017 18:03:48 CST run_dataimport INFO - \t\tCPU time spent (ms)=64470\n07-11-2017 18:03:48 CST run_dataimport INFO - \t\tPhysical memory (bytes) snapshot=1473638400\n07-11-2017 18:03:48 CST run_dataimport INFO - \t\tVirtual memory (bytes) snapshot=6498091008\n07-11-2017 18:03:48 CST run_dataimport INFO - \t\tTotal committed heap usage (bytes)=3240099840\n07-11-2017 18:03:48 CST run_dataimport INFO - \tFile Input Format Counters \n07-11-2017 18:03:48 CST run_dataimport INFO - \t\tBytes Read=0\n07-11-2017 18:03:48 CST run_dataimport INFO - \tFile Output Format Counters \n07-11-2017 18:03:48 CST run_dataimport INFO - \t\tBytes Written=93800000\n07-11-2017 18:03:48 CST run_dataimport INFO - 17/11/07 18:03:48 INFO mapreduce.ImportJobBase: Transferred 89.4547 MB in 4,879.8256 seconds (18.7715 KB/sec)\n07-11-2017 18:03:48 CST run_dataimport INFO - 17/11/07 18:03:48 INFO mapreduce.ImportJobBase: Retrieved 200000 records.\n07-11-2017 18:03:48 CST run_dataimport INFO - 17/11/07 18:03:48 INFO util.AppendUtils: Appending to directory sql_to_hive\n07-11-2017 18:03:48 CST run_dataimport INFO - 17/11/07 18:03:48 INFO util.AppendUtils: Using found partition 6804\n07-11-2017 18:03:49 CST run_dataimport INFO - 17/11/07 18:03:49 INFO tool.ImportTool: Incremental import complete! To run another incremental import of all data following this import, supply the following arguments:\n07-11-2017 18:03:49 CST run_dataimport INFO - 17/11/07 18:03:49 INFO tool.ImportTool:  --incremental append\n07-11-2017 18:03:49 CST run_dataimport INFO - 17/11/07 18:03:49 INFO tool.ImportTool:   --check-column uid\n07-11-2017 18:03:49 CST run_dataimport INFO - 17/11/07 18:03:49 INFO tool.ImportTool:   --last-value 83622300\n07-11-2017 18:03:49 CST run_dataimport INFO - 17/11/07 18:03:49 INFO tool.ImportTool: (Consider saving this with 'sqoop job --create')\n07-11-2017 18:03:49 CST run_dataimport INFO - 5\n07-11-2017 18:03:49 CST run_dataimport INFO - Warning: /opt/cloudera/parcels/CDH-5.8.0-1.cdh5.8.0.p0.42/bin/../lib/sqoop/../accumulo does not exist! Accumulo imports will fail.\n07-11-2017 18:03:49 CST run_dataimport INFO - Please set $ACCUMULO_HOME to the root of your Accumulo installation.\n07-11-2017 18:03:51 CST run_dataimport INFO - 17/11/07 18:03:51 INFO sqoop.Sqoop: Running Sqoop version: 1.4.6-cdh5.8.0\n07-11-2017 18:03:51 CST run_dataimport INFO - 17/11/07 18:03:51 WARN sqoop.ConnFactory: Parameter --driver is set to an explicit driver however appropriate connection manager is not being set (via --connection-manager). Sqoop is going to fall back to org.apache.sqoop.manager.GenericJdbcManager. Please specify explicitly which connection manager should be used next time.\n07-11-2017 18:03:51 CST run_dataimport INFO - 17/11/07 18:03:51 INFO manager.SqlManager: Using default fetchSize of 1000\n07-11-2017 18:03:51 CST run_dataimport INFO - 17/11/07 18:03:51 INFO tool.CodeGenTool: Beginning code generation\n07-11-2017 18:03:52 CST run_dataimport INFO - 17/11/07 18:03:52 INFO manager.SqlManager: Executing SQL statement: select top 200000 * from jzj.dbo.jzj_test where uid&gt;(select uid from (select top 400000 uid from jzj.dbo.jzj_test where uid&gt;83222300 and uid&lt;=84598500) a except select uid from (select top 399999 uid from jzj.dbo.jzj_test where uid&gt;83222300 and uid&lt;=84598500) a) and uid &lt;=84598500 and  (1 = 0) \n07-11-2017 18:03:52 CST run_dataimport INFO - 17/11/07 18:03:52 INFO manager.SqlManager: Executing SQL statement: select top 200000 * from jzj.dbo.jzj_test where uid&gt;(select uid from (select top 400000 uid from jzj.dbo.jzj_test where uid&gt;83222300 and uid&lt;=84598500) a except select uid from (select top 399999 uid from jzj.dbo.jzj_test where uid&gt;83222300 and uid&lt;=84598500) a) and uid &lt;=84598500 and  (1 = 0) \n07-11-2017 18:03:52 CST run_dataimport INFO - 17/11/07 18:03:52 INFO orm.CompilationManager: HADOOP_MAPRED_HOME is /opt/cloudera/parcels/CDH/lib/hadoop-mapreduce\n07-11-2017 18:03:54 CST run_dataimport INFO - Note: /tmp/sqoop-root/compile/21f23a42adb9bf71452da9a3c7187b06/QueryResult.java uses or overrides a deprecated API.\n07-11-2017 18:03:54 CST run_dataimport INFO - Note: Recompile with -Xlint:deprecation for details.\n07-11-2017 18:03:54 CST run_dataimport INFO - 17/11/07 18:03:54 INFO orm.CompilationManager: Writing jar file: /tmp/sqoop-root/compile/21f23a42adb9bf71452da9a3c7187b06/QueryResult.jar\n07-11-2017 18:03:55 CST run_dataimport INFO - 17/11/07 18:03:55 INFO tool.ImportTool: Maximal id query for free form incremental import: SELECT MAX(uid) FROM (select top 200000 * from jzj.dbo.jzj_test where uid&gt;(select uid from (select top 400000 uid from jzj.dbo.jzj_test where uid&gt;83222300 and uid&lt;=84598500) a except select uid from (select top 399999 uid from jzj.dbo.jzj_test where uid&gt;83222300 and uid&lt;=84598500) a) and uid &lt;=84598500 and (1 = 1)) sqoop_import_query_alias\n07-11-2017 20:43:27 CST run_dataimport INFO - 17/11/07 20:43:27 INFO tool.ImportTool: Incremental import based on column uid\n07-11-2017 20:43:27 CST run_dataimport INFO - 17/11/07 20:43:27 INFO tool.ImportTool: Upper bound value: 83822300\n07-11-2017 20:43:27 CST run_dataimport INFO - 17/11/07 20:43:27 INFO mapreduce.ImportJobBase: Beginning query import.\n07-11-2017 20:43:27 CST run_dataimport INFO - 17/11/07 20:43:27 INFO Configuration.deprecation: mapred.jar is deprecated. Instead, use mapreduce.job.jar\n07-11-2017 20:43:27 CST run_dataimport INFO - 17/11/07 20:43:27 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n07-11-2017 20:43:28 CST run_dataimport INFO - 17/11/07 20:43:28 INFO client.RMProxy: Connecting to ResourceManager at master/172.23.27.203:8032\n07-11-2017 20:43:34 CST run_dataimport INFO - 17/11/07 20:43:34 INFO db.DBInputFormat: Using read commited transaction isolation\n07-11-2017 20:43:34 CST run_dataimport INFO - 17/11/07 20:43:34 INFO db.DataDrivenDBInputFormat: BoundingValsQuery: SELECT MIN(uid), MAX(uid) FROM (select top 200000 * from jzj.dbo.jzj_test where uid&gt;(select uid from (select top 400000 uid from jzj.dbo.jzj_test where uid&gt;83222300 and uid&lt;=84598500) a except select uid from (select top 399999 uid from jzj.dbo.jzj_test where uid&gt;83222300 and uid&lt;=84598500) a) and uid &lt;=84598500 and uid &lt;= 83822300 AND  (1 = 1) ) AS t1\n07-11-2017 23:22:41 CST run_dataimport INFO - 17/11/07 23:22:41 INFO mapreduce.JobSubmitter: number of splits:4\n07-11-2017 23:22:41 CST run_dataimport INFO - 17/11/07 23:22:41 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1508671365448_2956\n07-11-2017 23:22:42 CST run_dataimport INFO - 17/11/07 23:22:42 INFO impl.YarnClientImpl: Submitted application application_1508671365448_2956\n07-11-2017 23:22:42 CST run_dataimport INFO - 17/11/07 23:22:42 INFO mapreduce.Job: The url to track the job: http://master:8088/proxy/application_1508671365448_2956/\n07-11-2017 23:22:42 CST run_dataimport INFO - 17/11/07 23:22:42 INFO mapreduce.Job: Running job: job_1508671365448_2956\n07-11-2017 23:22:49 CST run_dataimport INFO - 17/11/07 23:22:49 INFO mapreduce.Job: Job job_1508671365448_2956 running in uber mode : false\n07-11-2017 23:22:49 CST run_dataimport INFO - 17/11/07 23:22:49 INFO mapreduce.Job:  map 0% reduce 0%\n08-11-2017 02:00:13 CST run_dataimport INFO - 17/11/08 02:00:13 INFO mapreduce.Job:  map 25% reduce 0%\n08-11-2017 02:01:28 CST run_dataimport INFO - 17/11/08 02:01:28 INFO mapreduce.Job:  map 50% reduce 0%\n08-11-2017 02:03:21 CST run_dataimport INFO - 17/11/08 02:03:21 INFO mapreduce.Job:  map 75% reduce 0%\n08-11-2017 02:03:22 CST run_dataimport INFO - 17/11/08 02:03:22 INFO mapreduce.Job:  map 100% reduce 0%\n08-11-2017 02:03:23 CST run_dataimport INFO - 17/11/08 02:03:23 INFO mapreduce.Job: Job job_1508671365448_2956 completed successfully\n08-11-2017 02:03:23 CST run_dataimport INFO - 17/11/08 02:03:23 INFO mapreduce.Job: Counters: 30\n08-11-2017 02:03:23 CST run_dataimport INFO - \tFile System Counters\n08-11-2017 02:03:23 CST run_dataimport INFO - \t\tFILE: Number of bytes read=0\n08-11-2017 02:03:23 CST run_dataimport INFO - \t\tFILE: Number of bytes written=588300\n08-11-2017 02:03:23 CST run_dataimport INFO - \t\tFILE: Number of read operations=0\n08-11-2017 02:03:23 CST run_dataimport INFO - \t\tFILE: Number of large read operations=0\n08-11-2017 02:03:23 CST run_dataimport INFO - \t\tFILE: Number of write operations=0\n08-11-2017 02:03:23 CST run_dataimport INFO - \t\tHDFS: Number of bytes read=462\n08-11-2017 02:03:23 CST run_dataimport INFO - \t\tHDFS: Number of bytes written=93800000\n08-11-2017 02:03:23 CST run_dataimport INFO - \t\tHDFS: Number of read operations=16\n08-11-2017 02:03:23 CST run_dataimport INFO - \t\tHDFS: Number of large read operations=0\n08-11-2017 02:03:23 CST run_dataimport INFO - \t\tHDFS: Number of write operations=8\n08-11-2017 02:03:23 CST run_dataimport INFO - \tJob Counters \n08-11-2017 02:03:23 CST run_dataimport INFO - \t\tLaunched map tasks=4\n08-11-2017 02:03:23 CST run_dataimport INFO - \t\tOther local map tasks=4\n08-11-2017 02:03:23 CST run_dataimport INFO - \t\tTotal time spent by all maps in occupied slots (ms)=38216711\n08-11-2017 02:03:23 CST run_dataimport INFO - \t\tTotal time spent by all reduces in occupied slots (ms)=0\n08-11-2017 02:03:23 CST run_dataimport INFO - \t\tTotal time spent by all map tasks (ms)=38216711\n08-11-2017 02:03:23 CST run_dataimport INFO - \t\tTotal vcore-seconds taken by all map tasks=38216711\n08-11-2017 02:03:23 CST run_dataimport INFO - \t\tTotal megabyte-seconds taken by all map tasks=39133912064\n08-11-2017 02:03:23 CST run_dataimport INFO - \tMap-Reduce Framework\n08-11-2017 02:03:23 CST run_dataimport INFO - \t\tMap input records=200000\n08-11-2017 02:03:23 CST run_dataimport INFO - \t\tMap output records=200000\n08-11-2017 02:03:23 CST run_dataimport INFO - \t\tInput split bytes=462\n08-11-2017 02:03:23 CST run_dataimport INFO - \t\tSpilled Records=0\n08-11-2017 02:03:23 CST run_dataimport INFO - \t\tFailed Shuffles=0\n08-11-2017 02:03:23 CST run_dataimport INFO - \t\tMerged Map outputs=0\n08-11-2017 02:03:23 CST run_dataimport INFO - \t\tGC time elapsed (ms)=1588\n08-11-2017 02:03:23 CST run_dataimport INFO - \t\tCPU time spent (ms)=134830\n08-11-2017 02:03:23 CST run_dataimport INFO - \t\tPhysical memory (bytes) snapshot=1547542528\n08-11-2017 02:03:23 CST run_dataimport INFO - \t\tVirtual memory (bytes) snapshot=6530818048\n08-11-2017 02:03:23 CST run_dataimport INFO - \t\tTotal committed heap usage (bytes)=3172990976\n08-11-2017 02:03:23 CST run_dataimport INFO - \tFile Input Format Counters \n08-11-2017 02:03:23 CST run_dataimport INFO - \t\tBytes Read=0\n08-11-2017 02:03:23 CST run_dataimport INFO - \tFile Output Format Counters \n08-11-2017 02:03:23 CST run_dataimport INFO - \t\tBytes Written=93800000\n08-11-2017 02:03:23 CST run_dataimport INFO - 17/11/08 02:03:23 INFO mapreduce.ImportJobBase: Transferred 89.4547 MB in 19,195.7726 seconds (4.772 KB/sec)\n08-11-2017 02:03:23 CST run_dataimport INFO - 17/11/08 02:03:23 INFO mapreduce.ImportJobBase: Retrieved 200000 records.\n08-11-2017 02:03:23 CST run_dataimport INFO - 17/11/08 02:03:23 INFO util.AppendUtils: Appending to directory sql_to_hive\n08-11-2017 02:03:24 CST run_dataimport INFO - 17/11/08 02:03:24 INFO util.AppendUtils: Using found partition 6808\n08-11-2017 02:03:24 CST run_dataimport INFO - 17/11/08 02:03:24 INFO tool.ImportTool: Incremental import complete! To run another incremental import of all data following this import, supply the following arguments:\n08-11-2017 02:03:24 CST run_dataimport INFO - 17/11/08 02:03:24 INFO tool.ImportTool:  --incremental append\n08-11-2017 02:03:24 CST run_dataimport INFO - 17/11/08 02:03:24 INFO tool.ImportTool:   --check-column uid\n08-11-2017 02:03:24 CST run_dataimport INFO - 17/11/08 02:03:24 INFO tool.ImportTool:   --last-value 83822300\n08-11-2017 02:03:24 CST run_dataimport INFO - 17/11/08 02:03:24 INFO tool.ImportTool: (Consider saving this with 'sqoop job --create')\n08-11-2017 02:03:24 CST run_dataimport INFO - 4\n08-11-2017 02:03:24 CST run_dataimport INFO - Warning: /opt/cloudera/parcels/CDH-5.8.0-1.cdh5.8.0.p0.42/bin/../lib/sqoop/../accumulo does not exist! Accumulo imports will fail.\n08-11-2017 02:03:24 CST run_dataimport INFO - Please set $ACCUMULO_HOME to the root of your Accumulo installation.\n08-11-2017 02:03:26 CST run_dataimport INFO - 17/11/08 02:03:26 INFO sqoop.Sqoop: Running Sqoop version: 1.4.6-cdh5.8.0\n08-11-2017 02:03:26 CST run_dataimport INFO - 17/11/08 02:03:26 WARN sqoop.ConnFactory: Parameter --driver is set to an explicit driver however appropriate connection manager is not being set (via --connection-manager). Sqoop is going to fall back to org.apache.sqoop.manager.GenericJdbcManager. Please specify explicitly which connection manager should be used next time.\n08-11-2017 02:03:26 CST run_dataimport INFO - 17/11/08 02:03:26 INFO manager.SqlManager: Using default fetchSize of 1000\n08-11-2017 02:03:26 CST run_dataimport INFO - 17/11/08 02:03:26 INFO tool.CodeGenTool: Beginning code generation\n08-11-2017 02:03:27 CST run_dataimport INFO - 17/11/08 02:03:27 INFO manager.SqlManager: Executing SQL statement: select top 200000 * from jzj.dbo.jzj_test where uid&gt;(select uid from (select top 600000 uid from jzj.dbo.jzj_test where uid&gt;83222300 and uid&lt;=84598500) a except select uid from (select top 599999 uid from jzj.dbo.jzj_test where uid&gt;83222300 and uid&lt;=84598500) a) and uid &lt;=84598500 and  (1 = 0) \n08-11-2017 02:03:27 CST run_dataimport INFO - 17/11/08 02:03:27 INFO manager.SqlManager: Executing SQL statement: select top 200000 * from jzj.dbo.jzj_test where uid&gt;(select uid from (select top 600000 uid from jzj.dbo.jzj_test where uid&gt;83222300 and uid&lt;=84598500) a except select uid from (select top 599999 uid from jzj.dbo.jzj_test where uid&gt;83222300 and uid&lt;=84598500) a) and uid &lt;=84598500 and  (1 = 0) \n08-11-2017 02:03:27 CST run_dataimport INFO - 17/11/08 02:03:27 INFO orm.CompilationManager: HADOOP_MAPRED_HOME is /opt/cloudera/parcels/CDH/lib/hadoop-mapreduce\n08-11-2017 02:03:29 CST run_dataimport INFO - Note: /tmp/sqoop-root/compile/e8acb4e9422617ba257a228b6dfa0561/QueryResult.java uses or overrides a deprecated API.\n08-11-2017 02:03:29 CST run_dataimport INFO - Note: Recompile with -Xlint:deprecation for details.\n08-11-2017 02:03:29 CST run_dataimport INFO - 17/11/08 02:03:29 INFO orm.CompilationManager: Writing jar file: /tmp/sqoop-root/compile/e8acb4e9422617ba257a228b6dfa0561/QueryResult.jar\n08-11-2017 02:03:30 CST run_dataimport INFO - 17/11/08 02:03:30 INFO tool.ImportTool: Maximal id query for free form incremental import: SELECT MAX(uid) FROM (select top 200000 * from jzj.dbo.jzj_test where uid&gt;(select uid from (select top 600000 uid from jzj.dbo.jzj_test where uid&gt;83222300 and uid&lt;=84598500) a except select uid from (select top 599999 uid from jzj.dbo.jzj_test where uid&gt;83222300 and uid&lt;=84598500) a) and uid &lt;=84598500 and (1 = 1)) sqoop_import_query_alias\n08-11-2017 07:55:53 CST run_dataimport INFO - 17/11/08 07:55:53 INFO tool.ImportTool: Incremental import based on column uid\n08-11-2017 07:55:53 CST run_dataimport INFO - 17/11/08 07:55:53 INFO tool.ImportTool: Upper bound value: 84022300\n08-11-2017 07:55:53 CST run_dataimport INFO - 17/11/08 07:55:53 INFO mapreduce.ImportJobBase: Beginning query import.\n08-11-2017 07:55:53 CST run_dataimport INFO - 17/11/08 07:55:53 INFO Configuration.deprecation: mapred.jar is deprecated. Instead, use mapreduce.job.jar\n08-11-2017 07:55:53 CST run_dataimport INFO - 17/11/08 07:55:53 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n08-11-2017 07:55:53 CST run_dataimport INFO - 17/11/08 07:55:53 INFO client.RMProxy: Connecting to ResourceManager at master/172.23.27.203:8032\n08-11-2017 07:56:00 CST run_dataimport INFO - 17/11/08 07:56:00 INFO db.DBInputFormat: Using read commited transaction isolation\n08-11-2017 07:56:00 CST run_dataimport INFO - 17/11/08 07:56:00 INFO db.DataDrivenDBInputFormat: BoundingValsQuery: SELECT MIN(uid), MAX(uid) FROM (select top 200000 * from jzj.dbo.jzj_test where uid&gt;(select uid from (select top 600000 uid from jzj.dbo.jzj_test where uid&gt;83222300 and uid&lt;=84598500) a except select uid from (select top 599999 uid from jzj.dbo.jzj_test where uid&gt;83222300 and uid&lt;=84598500) a) and uid &lt;=84598500 and uid &lt;= 84022300 AND  (1 = 1) ) AS t1\n08-11-2017 12:11:01 CST run_dataimport ERROR - Kill has been called.\n08-11-2017 12:11:06 CST run_dataimport INFO - Process completed unsuccessfully in 72615 seconds.\n08-11-2017 12:11:06 CST run_dataimport ERROR - Job run killed!\njava.lang.RuntimeException: azkaban.jobExecutor.utils.process.ProcessFailureException\n\tat azkaban.jobExecutor.ProcessJob.run(ProcessJob.java:297)\n\tat azkaban.execapp.JobRunner.runJob(JobRunner.java:748)\n\tat azkaban.execapp.JobRunner.doRun(JobRunner.java:591)\n\tat azkaban.execapp.JobRunner.run(JobRunner.java:552)\n\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: azkaban.jobExecutor.utils.process.ProcessFailureException\n\tat azkaban.jobExecutor.utils.process.AzkabanProcess.run(AzkabanProcess.java:130)\n\tat azkaban.jobExecutor.ProcessJob.run(ProcessJob.java:289)\n\t... 8 more\n08-11-2017 12:11:06 CST run_dataimport ERROR - azkaban.jobExecutor.utils.process.ProcessFailureException cause: azkaban.jobExecutor.utils.process.ProcessFailureException\n08-11-2017 12:11:06 CST run_dataimport INFO - Finishing job run_dataimport at 1510114266951 with status KILLED\n", 'length': 37422}
[2017-11-20 16:35:26,258] app.hive_mysql.AzkabanMonitor [INFO] URL: https://172.23.27.203:8443
[2017-11-20 16:35:26,437] app.hive_mysql.AzkabanMonitor [INFO] Success: Login https://172.23.27.203:8443 with user azkaban
[2017-11-20 16:35:26,437] app.hive_mysql.AzkabanMonitor [DEBUG] {'session.id': '88cf733e-2d50-4763-941e-3dd1279fd946', 'status': 'success'}
[2017-11-20 16:35:26,448] app.hive_mysql.AzkabanMonitor [INFO] Success: Fetch Executions of a Flow - run_dataimport
[2017-11-20 16:35:26,449] app.hive_mysql.AzkabanMonitor [DEBUG] {'total': 16, 'executions': [{'submitTime': 1510041651107, 'submitUser': 'azkaban', 'startTime': 1510041651277, 'endTime': 1510114267057, 'flowId': 'run_dataimport', 'projectId': 68, 'execId': 1670, 'status': 'KILLED'}, {'submitTime': 1510027251078, 'submitUser': 'azkaban', 'startTime': 1510027251395, 'endTime': 1510036906931, 'flowId': 'run_dataimport', 'projectId': 68, 'execId': 1669, 'status': 'SUCCEEDED'}, {'submitTime': 1509897650826, 'submitUser': 'azkaban', 'startTime': 1509897651116, 'endTime': 1510026870568, 'flowId': 'run_dataimport', 'projectId': 68, 'execId': 1668, 'status': 'SUCCEEDED'}, {'submitTime': 1509883250800, 'submitUser': 'azkaban', 'startTime': 1509883251056, 'endTime': 1509892870422, 'flowId': 'run_dataimport', 'projectId': 68, 'execId': 1667, 'status': 'SUCCEEDED'}, {'submitTime': 1509753650545, 'submitUser': 'azkaban', 'startTime': 1509753650705, 'endTime': 1509881095432, 'flowId': 'run_dataimport', 'projectId': 68, 'execId': 1666, 'status': 'SUCCEEDED'}, {'submitTime': 1509746450529, 'submitUser': 'azkaban', 'startTime': 1509746450965, 'endTime': 1509753076918, 'flowId': 'run_dataimport', 'projectId': 68, 'execId': 1665, 'status': 'SUCCEEDED'}, {'submitTime': 1509652850362, 'submitUser': 'azkaban', 'startTime': 1509652850560, 'endTime': 1509744409191, 'flowId': 'run_dataimport', 'projectId': 68, 'execId': 1664, 'status': 'SUCCEEDED'}, {'submitTime': 1509645650349, 'submitUser': 'azkaban', 'startTime': 1509645650498, 'endTime': 1509651159750, 'flowId': 'run_dataimport', 'projectId': 68, 'execId': 1663, 'status': 'SUCCEEDED'}, {'submitTime': 1509566450178, 'submitUser': 'azkaban', 'startTime': 1509566450488, 'endTime': 1509642929867, 'flowId': 'run_dataimport', 'projectId': 68, 'execId': 1661, 'status': 'SUCCEEDED'}, {'submitTime': 1509559250163, 'submitUser': 'azkaban', 'startTime': 1509559250321, 'endTime': 1509563186239, 'flowId': 'run_dataimport', 'projectId': 68, 'execId': 1660, 'status': 'SUCCEEDED'}], 'length': 10, 'project': 'sql_to_hive', 'from': 0, 'projectId': 68, 'flow': 'run_dataimport'}
[2017-11-20 16:35:26,459] app.hive_mysql.AzkabanMonitor [INFO] Success: Fetch Execution Job Logs - 1670-run_dataimport
[2017-11-20 16:35:26,459] app.hive_mysql.AzkabanMonitor [DEBUG] {'offset': 0, 'data': "07-11-2017 16:00:51 CST run_dataimport INFO - Starting job run_dataimport at 1510041651281\n07-11-2017 16:00:51 CST run_dataimport INFO - azkaban.webserver.url property was not set\n07-11-2017 16:00:51 CST run_dataimport INFO - job JVM args: -Dazkaban.flowid=run_dataimport -Dazkaban.execid=1670 -Dazkaban.jobid=run_dataimport\n07-11-2017 16:00:51 CST run_dataimport INFO - Building command job executor. \n07-11-2017 16:00:51 CST run_dataimport INFO - Memory granted for job run_dataimport\n07-11-2017 16:00:51 CST run_dataimport INFO - 1 commands to execute.\n07-11-2017 16:00:51 CST run_dataimport INFO - cwd=/home/azkaban/azkaban_bk/azkaban-exec-server-3.35.0-22-g5517d50/executions/1670\n07-11-2017 16:00:51 CST run_dataimport INFO - effective user is: azkaban\n07-11-2017 16:00:51 CST run_dataimport INFO - Command: sh /home/sql_to_hive/dataimport.sh\n07-11-2017 16:00:51 CST run_dataimport INFO - Environment variables: {JOB_OUTPUT_PROP_FILE=/home/azkaban/azkaban_bk/azkaban-exec-server-3.35.0-22-g5517d50/executions/1670/run_dataimport_output_8807970133616151260_tmp, JOB_PROP_FILE=/home/azkaban/azkaban_bk/azkaban-exec-server-3.35.0-22-g5517d50/executions/1670/run_dataimport_props_6140684690151138768_tmp, KRB5CCNAME=/tmp/krb5cc__sql_to_hive__run_dataimport__run_dataimport__1670__azkaban, JOB_NAME=run_dataimport}\n07-11-2017 16:00:51 CST run_dataimport INFO - Working directory: /home/azkaban/azkaban_bk/azkaban-exec-server-3.35.0-22-g5517d50/executions/1670\n07-11-2017 16:00:51 CST run_dataimport INFO - log4j:WARN No appenders could be found for logger (org.apache.hive.jdbc.Utils).\n07-11-2017 16:00:51 CST run_dataimport INFO - log4j:WARN Please initialize the log4j system properly.\n07-11-2017 16:00:51 CST run_dataimport INFO - log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.\n07-11-2017 16:00:51 CST run_dataimport INFO - SLF4J: Failed to load class &quot;org.slf4j.impl.StaticLoggerBinder&quot;.\n07-11-2017 16:00:51 CST run_dataimport INFO - SLF4J: Defaulting to no-operation (NOP) logger implementation\n07-11-2017 16:00:51 CST run_dataimport INFO - SLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.\n07-11-2017 16:02:05 CST run_dataimport INFO - 1,84598500,83222300,1376200sqlmax:84598500\n07-11-2017 16:02:05 CST run_dataimport INFO - hivemax:83222300\n07-11-2017 16:02:05 CST run_dataimport INFO - counts:1376200\n07-11-2017 16:02:05 CST run_dataimport INFO - 7,176200\n07-11-2017 16:02:05 CST run_dataimport INFO - 7\n07-11-2017 16:02:05 CST run_dataimport INFO - Warning: /opt/cloudera/parcels/CDH-5.8.0-1.cdh5.8.0.p0.42/bin/../lib/sqoop/../accumulo does not exist! Accumulo imports will fail.\n07-11-2017 16:02:05 CST run_dataimport INFO - Please set $ACCUMULO_HOME to the root of your Accumulo installation.\n07-11-2017 16:02:07 CST run_dataimport INFO - 17/11/07 16:02:07 INFO sqoop.Sqoop: Running Sqoop version: 1.4.6-cdh5.8.0\n07-11-2017 16:02:07 CST run_dataimport INFO - 17/11/07 16:02:07 WARN sqoop.ConnFactory: Parameter --driver is set to an explicit driver however appropriate connection manager is not being set (via --connection-manager). Sqoop is going to fall back to org.apache.sqoop.manager.GenericJdbcManager. Please specify explicitly which connection manager should be used next time.\n07-11-2017 16:02:07 CST run_dataimport INFO - 17/11/07 16:02:07 INFO manager.SqlManager: Using default fetchSize of 1000\n07-11-2017 16:02:07 CST run_dataimport INFO - 17/11/07 16:02:07 INFO tool.CodeGenTool: Beginning code generation\n07-11-2017 16:02:08 CST run_dataimport INFO - 17/11/07 16:02:08 INFO manager.SqlManager: Executing SQL statement: select top 200000 * from jzj.dbo.jzj_test where uid&gt;=(select uid from (select top 1 uid from jzj.dbo.jzj_test where uid&gt;83222300 and uid&lt;=84598500) a) and uid &lt;=84598500 and  (1 = 0) \n07-11-2017 16:02:08 CST run_dataimport INFO - 17/11/07 16:02:08 INFO manager.SqlManager: Executing SQL statement: select top 200000 * from jzj.dbo.jzj_test where uid&gt;=(select uid from (select top 1 uid from jzj.dbo.jzj_test where uid&gt;83222300 and uid&lt;=84598500) a) and uid &lt;=84598500 and  (1 = 0) \n07-11-2017 16:02:08 CST run_dataimport INFO - 17/11/07 16:02:08 INFO orm.CompilationManager: HADOOP_MAPRED_HOME is /opt/cloudera/parcels/CDH/lib/hadoop-mapreduce\n07-11-2017 16:02:10 CST run_dataimport INFO - Note: /tmp/sqoop-root/compile/b796f2c0b0198c05bdcf8149b3f9a7fd/QueryResult.java uses or overrides a deprecated API.\n07-11-2017 16:02:10 CST run_dataimport INFO - Note: Recompile with -Xlint:deprecation for details.\n07-11-2017 16:02:10 CST run_dataimport INFO - 17/11/07 16:02:10 INFO orm.CompilationManager: Writing jar file: /tmp/sqoop-root/compile/b796f2c0b0198c05bdcf8149b3f9a7fd/QueryResult.jar\n07-11-2017 16:02:11 CST run_dataimport INFO - 17/11/07 16:02:11 INFO tool.ImportTool: Maximal id query for free form incremental import: SELECT MAX(uid) FROM (select top 200000 * from jzj.dbo.jzj_test where uid&gt;=(select uid from (select top 1 uid from jzj.dbo.jzj_test where uid&gt;83222300 and uid&lt;=84598500) a) and uid &lt;=84598500 and (1 = 1)) sqoop_import_query_alias\n07-11-2017 16:02:11 CST run_dataimport INFO - 17/11/07 16:02:11 INFO tool.ImportTool: Incremental import based on column uid\n07-11-2017 16:02:11 CST run_dataimport INFO - 17/11/07 16:02:11 INFO tool.ImportTool: Upper bound value: 83422300\n07-11-2017 16:02:11 CST run_dataimport INFO - 17/11/07 16:02:11 INFO mapreduce.ImportJobBase: Beginning query import.\n07-11-2017 16:02:11 CST run_dataimport INFO - 17/11/07 16:02:11 INFO Configuration.deprecation: mapred.jar is deprecated. Instead, use mapreduce.job.jar\n07-11-2017 16:02:11 CST run_dataimport INFO - 17/11/07 16:02:11 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n07-11-2017 16:02:11 CST run_dataimport INFO - 17/11/07 16:02:11 INFO client.RMProxy: Connecting to ResourceManager at master/172.23.27.203:8032\n07-11-2017 16:02:18 CST run_dataimport INFO - 17/11/07 16:02:18 INFO db.DBInputFormat: Using read commited transaction isolation\n07-11-2017 16:02:18 CST run_dataimport INFO - 17/11/07 16:02:18 INFO db.DataDrivenDBInputFormat: BoundingValsQuery: SELECT MIN(uid), MAX(uid) FROM (select top 200000 * from jzj.dbo.jzj_test where uid&gt;=(select uid from (select top 1 uid from jzj.dbo.jzj_test where uid&gt;83222300 and uid&lt;=84598500) a) and uid &lt;=84598500 and uid &lt;= 83422300 AND  (1 = 1) ) AS t1\n07-11-2017 16:02:18 CST run_dataimport INFO - 17/11/07 16:02:18 INFO mapreduce.JobSubmitter: number of splits:4\n07-11-2017 16:02:18 CST run_dataimport INFO - 17/11/07 16:02:18 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1508671365448_2954\n07-11-2017 16:02:19 CST run_dataimport INFO - 17/11/07 16:02:19 INFO impl.YarnClientImpl: Submitted application application_1508671365448_2954\n07-11-2017 16:02:19 CST run_dataimport INFO - 17/11/07 16:02:19 INFO mapreduce.Job: The url to track the job: http://master:8088/proxy/application_1508671365448_2954/\n07-11-2017 16:02:19 CST run_dataimport INFO - 17/11/07 16:02:19 INFO mapreduce.Job: Running job: job_1508671365448_2954\n07-11-2017 16:02:26 CST run_dataimport INFO - 17/11/07 16:02:26 INFO mapreduce.Job: Job job_1508671365448_2954 running in uber mode : false\n07-11-2017 16:02:26 CST run_dataimport INFO - 17/11/07 16:02:26 INFO mapreduce.Job:  map 0% reduce 0%\n07-11-2017 16:02:37 CST run_dataimport INFO - 17/11/07 16:02:37 INFO mapreduce.Job:  map 25% reduce 0%\n07-11-2017 16:02:40 CST run_dataimport INFO - 17/11/07 16:02:40 INFO mapreduce.Job:  map 100% reduce 0%\n07-11-2017 16:02:41 CST run_dataimport INFO - 17/11/07 16:02:41 INFO mapreduce.Job: Job job_1508671365448_2954 completed successfully\n07-11-2017 16:02:41 CST run_dataimport INFO - 17/11/07 16:02:41 INFO mapreduce.Job: Counters: 30\n07-11-2017 16:02:41 CST run_dataimport INFO - \tFile System Counters\n07-11-2017 16:02:41 CST run_dataimport INFO - \t\tFILE: Number of bytes read=0\n07-11-2017 16:02:41 CST run_dataimport INFO - \t\tFILE: Number of bytes written=587024\n07-11-2017 16:02:41 CST run_dataimport INFO - \t\tFILE: Number of read operations=0\n07-11-2017 16:02:41 CST run_dataimport INFO - \t\tFILE: Number of large read operations=0\n07-11-2017 16:02:41 CST run_dataimport INFO - \t\tFILE: Number of write operations=0\n07-11-2017 16:02:41 CST run_dataimport INFO - \t\tHDFS: Number of bytes read=462\n07-11-2017 16:02:41 CST run_dataimport INFO - \t\tHDFS: Number of bytes written=93800000\n07-11-2017 16:02:41 CST run_dataimport INFO - \t\tHDFS: Number of read operations=16\n07-11-2017 16:02:41 CST run_dataimport INFO - \t\tHDFS: Number of large read operations=0\n07-11-2017 16:02:41 CST run_dataimport INFO - \t\tHDFS: Number of write operations=8\n07-11-2017 16:02:41 CST run_dataimport INFO - \tJob Counters \n07-11-2017 16:02:41 CST run_dataimport INFO - \t\tLaunched map tasks=4\n07-11-2017 16:02:41 CST run_dataimport INFO - \t\tOther local map tasks=4\n07-11-2017 16:02:41 CST run_dataimport INFO - \t\tTotal time spent by all maps in occupied slots (ms)=44342\n07-11-2017 16:02:41 CST run_dataimport INFO - \t\tTotal time spent by all reduces in occupied slots (ms)=0\n07-11-2017 16:02:41 CST run_dataimport INFO - \t\tTotal time spent by all map tasks (ms)=44342\n07-11-2017 16:02:41 CST run_dataimport INFO - \t\tTotal vcore-seconds taken by all map tasks=44342\n07-11-2017 16:02:41 CST run_dataimport INFO - \t\tTotal megabyte-seconds taken by all map tasks=45406208\n07-11-2017 16:02:41 CST run_dataimport INFO - \tMap-Reduce Framework\n07-11-2017 16:02:41 CST run_dataimport INFO - \t\tMap input records=200000\n07-11-2017 16:02:41 CST run_dataimport INFO - \t\tMap output records=200000\n07-11-2017 16:02:41 CST run_dataimport INFO - \t\tInput split bytes=462\n07-11-2017 16:02:41 CST run_dataimport INFO - \t\tSpilled Records=0\n07-11-2017 16:02:41 CST run_dataimport INFO - \t\tFailed Shuffles=0\n07-11-2017 16:02:41 CST run_dataimport INFO - \t\tMerged Map outputs=0\n07-11-2017 16:02:41 CST run_dataimport INFO - \t\tGC time elapsed (ms)=569\n07-11-2017 16:02:41 CST run_dataimport INFO - \t\tCPU time spent (ms)=36080\n07-11-2017 16:02:41 CST run_dataimport INFO - \t\tPhysical memory (bytes) snapshot=1458102272\n07-11-2017 16:02:41 CST run_dataimport INFO - \t\tVirtual memory (bytes) snapshot=6516502528\n07-11-2017 16:02:41 CST run_dataimport INFO - \t\tTotal committed heap usage (bytes)=3231186944\n07-11-2017 16:02:41 CST run_dataimport INFO - \tFile Input Format Counters \n07-11-2017 16:02:41 CST run_dataimport INFO - \t\tBytes Read=0\n07-11-2017 16:02:41 CST run_dataimport INFO - \tFile Output Format Counters \n07-11-2017 16:02:41 CST run_dataimport INFO - \t\tBytes Written=93800000\n07-11-2017 16:02:41 CST run_dataimport INFO - 17/11/07 16:02:41 INFO mapreduce.ImportJobBase: Transferred 89.4547 MB in 29.9824 seconds (2.9836 MB/sec)\n07-11-2017 16:02:41 CST run_dataimport INFO - 17/11/07 16:02:41 INFO mapreduce.ImportJobBase: Retrieved 200000 records.\n07-11-2017 16:02:41 CST run_dataimport INFO - 17/11/07 16:02:41 INFO util.AppendUtils: Appending to directory sql_to_hive\n07-11-2017 16:02:42 CST run_dataimport INFO - 17/11/07 16:02:42 INFO util.AppendUtils: Using found partition 6800\n07-11-2017 16:02:42 CST run_dataimport INFO - 17/11/07 16:02:42 INFO tool.ImportTool: Incremental import complete! To run another incremental import of all data following this import, supply the following arguments:\n07-11-2017 16:02:42 CST run_dataimport INFO - 17/11/07 16:02:42 INFO tool.ImportTool:  --incremental append\n07-11-2017 16:02:42 CST run_dataimport INFO - 17/11/07 16:02:42 INFO tool.ImportTool:   --check-column uid\n07-11-2017 16:02:42 CST run_dataimport INFO - 17/11/07 16:02:42 INFO tool.ImportTool:   --last-value 83422300\n07-11-2017 16:02:42 CST run_dataimport INFO - 17/11/07 16:02:42 INFO tool.ImportTool: (Consider saving this with 'sqoop job --create')\n07-11-2017 16:02:42 CST run_dataimport INFO - 6\n07-11-2017 16:02:42 CST run_dataimport INFO - Warning: /opt/cloudera/parcels/CDH-5.8.0-1.cdh5.8.0.p0.42/bin/../lib/sqoop/../accumulo does not exist! Accumulo imports will fail.\n07-11-2017 16:02:42 CST run_dataimport INFO - Please set $ACCUMULO_HOME to the root of your Accumulo installation.\n07-11-2017 16:02:44 CST run_dataimport INFO - 17/11/07 16:02:44 INFO sqoop.Sqoop: Running Sqoop version: 1.4.6-cdh5.8.0\n07-11-2017 16:02:44 CST run_dataimport INFO - 17/11/07 16:02:44 WARN sqoop.ConnFactory: Parameter --driver is set to an explicit driver however appropriate connection manager is not being set (via --connection-manager). Sqoop is going to fall back to org.apache.sqoop.manager.GenericJdbcManager. Please specify explicitly which connection manager should be used next time.\n07-11-2017 16:02:44 CST run_dataimport INFO - 17/11/07 16:02:44 INFO manager.SqlManager: Using default fetchSize of 1000\n07-11-2017 16:02:44 CST run_dataimport INFO - 17/11/07 16:02:44 INFO tool.CodeGenTool: Beginning code generation\n07-11-2017 16:02:45 CST run_dataimport INFO - 17/11/07 16:02:45 INFO manager.SqlManager: Executing SQL statement: select top 200000 * from jzj.dbo.jzj_test where uid&gt;(select uid from (select top 200000 uid from jzj.dbo.jzj_test where uid&gt;83222300 and uid&lt;=84598500) a except select uid from (select top 199999 uid from jzj.dbo.jzj_test where uid&gt;83222300 and uid&lt;=84598500) a) and uid &lt;=84598500 and  (1 = 0) \n07-11-2017 16:02:45 CST run_dataimport INFO - 17/11/07 16:02:45 INFO manager.SqlManager: Executing SQL statement: select top 200000 * from jzj.dbo.jzj_test where uid&gt;(select uid from (select top 200000 uid from jzj.dbo.jzj_test where uid&gt;83222300 and uid&lt;=84598500) a except select uid from (select top 199999 uid from jzj.dbo.jzj_test where uid&gt;83222300 and uid&lt;=84598500) a) and uid &lt;=84598500 and  (1 = 0) \n07-11-2017 16:02:45 CST run_dataimport INFO - 17/11/07 16:02:45 INFO orm.CompilationManager: HADOOP_MAPRED_HOME is /opt/cloudera/parcels/CDH/lib/hadoop-mapreduce\n07-11-2017 16:02:47 CST run_dataimport INFO - Note: /tmp/sqoop-root/compile/3e3fe91d38feffbabab3df86c4ddb2c4/QueryResult.java uses or overrides a deprecated API.\n07-11-2017 16:02:47 CST run_dataimport INFO - Note: Recompile with -Xlint:deprecation for details.\n07-11-2017 16:02:47 CST run_dataimport INFO - 17/11/07 16:02:47 INFO orm.CompilationManager: Writing jar file: /tmp/sqoop-root/compile/3e3fe91d38feffbabab3df86c4ddb2c4/QueryResult.jar\n07-11-2017 16:02:48 CST run_dataimport INFO - 17/11/07 16:02:48 INFO tool.ImportTool: Maximal id query for free form incremental import: SELECT MAX(uid) FROM (select top 200000 * from jzj.dbo.jzj_test where uid&gt;(select uid from (select top 200000 uid from jzj.dbo.jzj_test where uid&gt;83222300 and uid&lt;=84598500) a except select uid from (select top 199999 uid from jzj.dbo.jzj_test where uid&gt;83222300 and uid&lt;=84598500) a) and uid &lt;=84598500 and (1 = 1)) sqoop_import_query_alias\n07-11-2017 16:42:28 CST run_dataimport INFO - 17/11/07 16:42:28 INFO tool.ImportTool: Incremental import based on column uid\n07-11-2017 16:42:28 CST run_dataimport INFO - 17/11/07 16:42:28 INFO tool.ImportTool: Upper bound value: 83622300\n07-11-2017 16:42:28 CST run_dataimport INFO - 17/11/07 16:42:28 INFO mapreduce.ImportJobBase: Beginning query import.\n07-11-2017 16:42:28 CST run_dataimport INFO - 17/11/07 16:42:28 INFO Configuration.deprecation: mapred.jar is deprecated. Instead, use mapreduce.job.jar\n07-11-2017 16:42:28 CST run_dataimport INFO - 17/11/07 16:42:28 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n07-11-2017 16:42:28 CST run_dataimport INFO - 17/11/07 16:42:28 INFO client.RMProxy: Connecting to ResourceManager at master/172.23.27.203:8032\n07-11-2017 16:42:34 CST run_dataimport INFO - 17/11/07 16:42:34 INFO db.DBInputFormat: Using read commited transaction isolation\n07-11-2017 16:42:34 CST run_dataimport INFO - 17/11/07 16:42:34 INFO db.DataDrivenDBInputFormat: BoundingValsQuery: SELECT MIN(uid), MAX(uid) FROM (select top 200000 * from jzj.dbo.jzj_test where uid&gt;(select uid from (select top 200000 uid from jzj.dbo.jzj_test where uid&gt;83222300 and uid&lt;=84598500) a except select uid from (select top 199999 uid from jzj.dbo.jzj_test where uid&gt;83222300 and uid&lt;=84598500) a) and uid &lt;=84598500 and uid &lt;= 83622300 AND  (1 = 1) ) AS t1\n07-11-2017 17:22:49 CST run_dataimport INFO - 17/11/07 17:22:49 INFO mapreduce.JobSubmitter: number of splits:4\n07-11-2017 17:22:49 CST run_dataimport INFO - 17/11/07 17:22:49 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1508671365448_2955\n07-11-2017 17:22:49 CST run_dataimport INFO - 17/11/07 17:22:49 INFO impl.YarnClientImpl: Submitted application application_1508671365448_2955\n07-11-2017 17:22:49 CST run_dataimport INFO - 17/11/07 17:22:49 INFO mapreduce.Job: The url to track the job: http://master:8088/proxy/application_1508671365448_2955/\n07-11-2017 17:22:49 CST run_dataimport INFO - 17/11/07 17:22:49 INFO mapreduce.Job: Running job: job_1508671365448_2955\n07-11-2017 17:22:57 CST run_dataimport INFO - 17/11/07 17:22:57 INFO mapreduce.Job: Job job_1508671365448_2955 running in uber mode : false\n07-11-2017 17:22:57 CST run_dataimport INFO - 17/11/07 17:22:57 INFO mapreduce.Job:  map 0% reduce 0%\n07-11-2017 18:02:56 CST run_dataimport INFO - 17/11/07 18:02:56 INFO mapreduce.Job:  map 25% reduce 0%\n07-11-2017 18:03:23 CST run_dataimport INFO - 17/11/07 18:03:23 INFO mapreduce.Job:  map 50% reduce 0%\n07-11-2017 18:03:46 CST run_dataimport INFO - 17/11/07 18:03:46 INFO mapreduce.Job:  map 75% reduce 0%\n07-11-2017 18:03:47 CST run_dataimport INFO - 17/11/07 18:03:47 INFO mapreduce.Job:  map 100% reduce 0%\n07-11-2017 18:03:48 CST run_dataimport INFO - 17/11/07 18:03:48 INFO mapreduce.Job: Job job_1508671365448_2955 completed successfully\n07-11-2017 18:03:48 CST run_dataimport INFO - 17/11/07 18:03:48 INFO mapreduce.Job: Counters: 30\n07-11-2017 18:03:48 CST run_dataimport INFO - \tFile System Counters\n07-11-2017 18:03:48 CST run_dataimport INFO - \t\tFILE: Number of bytes read=0\n07-11-2017 18:03:48 CST run_dataimport INFO - \t\tFILE: Number of bytes written=588156\n07-11-2017 18:03:48 CST run_dataimport INFO - \t\tFILE: Number of read operations=0\n07-11-2017 18:03:48 CST run_dataimport INFO - \t\tFILE: Number of large read operations=0\n07-11-2017 18:03:48 CST run_dataimport INFO - \t\tFILE: Number of write operations=0\n07-11-2017 18:03:48 CST run_dataimport INFO - \t\tHDFS: Number of bytes read=462\n07-11-2017 18:03:48 CST run_dataimport INFO - \t\tHDFS: Number of bytes written=93800000\n07-11-2017 18:03:48 CST run_dataimport INFO - \t\tHDFS: Number of read operations=16\n07-11-2017 18:03:48 CST run_dataimport INFO - \t\tHDFS: Number of large read operations=0\n07-11-2017 18:03:48 CST run_dataimport INFO - \t\tHDFS: Number of write operations=8\n07-11-2017 18:03:48 CST run_dataimport INFO - \tJob Counters \n07-11-2017 18:03:48 CST run_dataimport INFO - \t\tLaunched map tasks=4\n07-11-2017 18:03:48 CST run_dataimport INFO - \t\tOther local map tasks=4\n07-11-2017 18:03:48 CST run_dataimport INFO - \t\tTotal time spent by all maps in occupied slots (ms)=9712787\n07-11-2017 18:03:48 CST run_dataimport INFO - \t\tTotal time spent by all reduces in occupied slots (ms)=0\n07-11-2017 18:03:48 CST run_dataimport INFO - \t\tTotal time spent by all map tasks (ms)=9712787\n07-11-2017 18:03:48 CST run_dataimport INFO - \t\tTotal vcore-seconds taken by all map tasks=9712787\n07-11-2017 18:03:48 CST run_dataimport INFO - \t\tTotal megabyte-seconds taken by all map tasks=9945893888\n07-11-2017 18:03:48 CST run_dataimport INFO - \tMap-Reduce Framework\n07-11-2017 18:03:48 CST run_dataimport INFO - \t\tMap input records=200000\n07-11-2017 18:03:48 CST run_dataimport INFO - \t\tMap output records=200000\n07-11-2017 18:03:48 CST run_dataimport INFO - \t\tInput split bytes=462\n07-11-2017 18:03:48 CST run_dataimport INFO - \t\tSpilled Records=0\n07-11-2017 18:03:48 CST run_dataimport INFO - \t\tFailed Shuffles=0\n07-11-2017 18:03:48 CST run_dataimport INFO - \t\tMerged Map outputs=0\n07-11-2017 18:03:48 CST run_dataimport INFO - \t\tGC time elapsed (ms)=764\n07-11-2017 18:03:48 CST run_dataimport INFO - \t\tCPU time spent (ms)=64470\n07-11-2017 18:03:48 CST run_dataimport INFO - \t\tPhysical memory (bytes) snapshot=1473638400\n07-11-2017 18:03:48 CST run_dataimport INFO - \t\tVirtual memory (bytes) snapshot=6498091008\n07-11-2017 18:03:48 CST run_dataimport INFO - \t\tTotal committed heap usage (bytes)=3240099840\n07-11-2017 18:03:48 CST run_dataimport INFO - \tFile Input Format Counters \n07-11-2017 18:03:48 CST run_dataimport INFO - \t\tBytes Read=0\n07-11-2017 18:03:48 CST run_dataimport INFO - \tFile Output Format Counters \n07-11-2017 18:03:48 CST run_dataimport INFO - \t\tBytes Written=93800000\n07-11-2017 18:03:48 CST run_dataimport INFO - 17/11/07 18:03:48 INFO mapreduce.ImportJobBase: Transferred 89.4547 MB in 4,879.8256 seconds (18.7715 KB/sec)\n07-11-2017 18:03:48 CST run_dataimport INFO - 17/11/07 18:03:48 INFO mapreduce.ImportJobBase: Retrieved 200000 records.\n07-11-2017 18:03:48 CST run_dataimport INFO - 17/11/07 18:03:48 INFO util.AppendUtils: Appending to directory sql_to_hive\n07-11-2017 18:03:48 CST run_dataimport INFO - 17/11/07 18:03:48 INFO util.AppendUtils: Using found partition 6804\n07-11-2017 18:03:49 CST run_dataimport INFO - 17/11/07 18:03:49 INFO tool.ImportTool: Incremental import complete! To run another incremental import of all data following this import, supply the following arguments:\n07-11-2017 18:03:49 CST run_dataimport INFO - 17/11/07 18:03:49 INFO tool.ImportTool:  --incremental append\n07-11-2017 18:03:49 CST run_dataimport INFO - 17/11/07 18:03:49 INFO tool.ImportTool:   --check-column uid\n07-11-2017 18:03:49 CST run_dataimport INFO - 17/11/07 18:03:49 INFO tool.ImportTool:   --last-value 83622300\n07-11-2017 18:03:49 CST run_dataimport INFO - 17/11/07 18:03:49 INFO tool.ImportTool: (Consider saving this with 'sqoop job --create')\n07-11-2017 18:03:49 CST run_dataimport INFO - 5\n07-11-2017 18:03:49 CST run_dataimport INFO - Warning: /opt/cloudera/parcels/CDH-5.8.0-1.cdh5.8.0.p0.42/bin/../lib/sqoop/../accumulo does not exist! Accumulo imports will fail.\n07-11-2017 18:03:49 CST run_dataimport INFO - Please set $ACCUMULO_HOME to the root of your Accumulo installation.\n07-11-2017 18:03:51 CST run_dataimport INFO - 17/11/07 18:03:51 INFO sqoop.Sqoop: Running Sqoop version: 1.4.6-cdh5.8.0\n07-11-2017 18:03:51 CST run_dataimport INFO - 17/11/07 18:03:51 WARN sqoop.ConnFactory: Parameter --driver is set to an explicit driver however appropriate connection manager is not being set (via --connection-manager). Sqoop is going to fall back to org.apache.sqoop.manager.GenericJdbcManager. Please specify explicitly which connection manager should be used next time.\n07-11-2017 18:03:51 CST run_dataimport INFO - 17/11/07 18:03:51 INFO manager.SqlManager: Using default fetchSize of 1000\n07-11-2017 18:03:51 CST run_dataimport INFO - 17/11/07 18:03:51 INFO tool.CodeGenTool: Beginning code generation\n07-11-2017 18:03:52 CST run_dataimport INFO - 17/11/07 18:03:52 INFO manager.SqlManager: Executing SQL statement: select top 200000 * from jzj.dbo.jzj_test where uid&gt;(select uid from (select top 400000 uid from jzj.dbo.jzj_test where uid&gt;83222300 and uid&lt;=84598500) a except select uid from (select top 399999 uid from jzj.dbo.jzj_test where uid&gt;83222300 and uid&lt;=84598500) a) and uid &lt;=84598500 and  (1 = 0) \n07-11-2017 18:03:52 CST run_dataimport INFO - 17/11/07 18:03:52 INFO manager.SqlManager: Executing SQL statement: select top 200000 * from jzj.dbo.jzj_test where uid&gt;(select uid from (select top 400000 uid from jzj.dbo.jzj_test where uid&gt;83222300 and uid&lt;=84598500) a except select uid from (select top 399999 uid from jzj.dbo.jzj_test where uid&gt;83222300 and uid&lt;=84598500) a) and uid &lt;=84598500 and  (1 = 0) \n07-11-2017 18:03:52 CST run_dataimport INFO - 17/11/07 18:03:52 INFO orm.CompilationManager: HADOOP_MAPRED_HOME is /opt/cloudera/parcels/CDH/lib/hadoop-mapreduce\n07-11-2017 18:03:54 CST run_dataimport INFO - Note: /tmp/sqoop-root/compile/21f23a42adb9bf71452da9a3c7187b06/QueryResult.java uses or overrides a deprecated API.\n07-11-2017 18:03:54 CST run_dataimport INFO - Note: Recompile with -Xlint:deprecation for details.\n07-11-2017 18:03:54 CST run_dataimport INFO - 17/11/07 18:03:54 INFO orm.CompilationManager: Writing jar file: /tmp/sqoop-root/compile/21f23a42adb9bf71452da9a3c7187b06/QueryResult.jar\n07-11-2017 18:03:55 CST run_dataimport INFO - 17/11/07 18:03:55 INFO tool.ImportTool: Maximal id query for free form incremental import: SELECT MAX(uid) FROM (select top 200000 * from jzj.dbo.jzj_test where uid&gt;(select uid from (select top 400000 uid from jzj.dbo.jzj_test where uid&gt;83222300 and uid&lt;=84598500) a except select uid from (select top 399999 uid from jzj.dbo.jzj_test where uid&gt;83222300 and uid&lt;=84598500) a) and uid &lt;=84598500 and (1 = 1)) sqoop_import_query_alias\n07-11-2017 20:43:27 CST run_dataimport INFO - 17/11/07 20:43:27 INFO tool.ImportTool: Incremental import based on column uid\n07-11-2017 20:43:27 CST run_dataimport INFO - 17/11/07 20:43:27 INFO tool.ImportTool: Upper bound value: 83822300\n07-11-2017 20:43:27 CST run_dataimport INFO - 17/11/07 20:43:27 INFO mapreduce.ImportJobBase: Beginning query import.\n07-11-2017 20:43:27 CST run_dataimport INFO - 17/11/07 20:43:27 INFO Configuration.deprecation: mapred.jar is deprecated. Instead, use mapreduce.job.jar\n07-11-2017 20:43:27 CST run_dataimport INFO - 17/11/07 20:43:27 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n07-11-2017 20:43:28 CST run_dataimport INFO - 17/11/07 20:43:28 INFO client.RMProxy: Connecting to ResourceManager at master/172.23.27.203:8032\n07-11-2017 20:43:34 CST run_dataimport INFO - 17/11/07 20:43:34 INFO db.DBInputFormat: Using read commited transaction isolation\n07-11-2017 20:43:34 CST run_dataimport INFO - 17/11/07 20:43:34 INFO db.DataDrivenDBInputFormat: BoundingValsQuery: SELECT MIN(uid), MAX(uid) FROM (select top 200000 * from jzj.dbo.jzj_test where uid&gt;(select uid from (select top 400000 uid from jzj.dbo.jzj_test where uid&gt;83222300 and uid&lt;=84598500) a except select uid from (select top 399999 uid from jzj.dbo.jzj_test where uid&gt;83222300 and uid&lt;=84598500) a) and uid &lt;=84598500 and uid &lt;= 83822300 AND  (1 = 1) ) AS t1\n07-11-2017 23:22:41 CST run_dataimport INFO - 17/11/07 23:22:41 INFO mapreduce.JobSubmitter: number of splits:4\n07-11-2017 23:22:41 CST run_dataimport INFO - 17/11/07 23:22:41 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1508671365448_2956\n07-11-2017 23:22:42 CST run_dataimport INFO - 17/11/07 23:22:42 INFO impl.YarnClientImpl: Submitted application application_1508671365448_2956\n07-11-2017 23:22:42 CST run_dataimport INFO - 17/11/07 23:22:42 INFO mapreduce.Job: The url to track the job: http://master:8088/proxy/application_1508671365448_2956/\n07-11-2017 23:22:42 CST run_dataimport INFO - 17/11/07 23:22:42 INFO mapreduce.Job: Running job: job_1508671365448_2956\n07-11-2017 23:22:49 CST run_dataimport INFO - 17/11/07 23:22:49 INFO mapreduce.Job: Job job_1508671365448_2956 running in uber mode : false\n07-11-2017 23:22:49 CST run_dataimport INFO - 17/11/07 23:22:49 INFO mapreduce.Job:  map 0% reduce 0%\n08-11-2017 02:00:13 CST run_dataimport INFO - 17/11/08 02:00:13 INFO mapreduce.Job:  map 25% reduce 0%\n08-11-2017 02:01:28 CST run_dataimport INFO - 17/11/08 02:01:28 INFO mapreduce.Job:  map 50% reduce 0%\n08-11-2017 02:03:21 CST run_dataimport INFO - 17/11/08 02:03:21 INFO mapreduce.Job:  map 75% reduce 0%\n08-11-2017 02:03:22 CST run_dataimport INFO - 17/11/08 02:03:22 INFO mapreduce.Job:  map 100% reduce 0%\n08-11-2017 02:03:23 CST run_dataimport INFO - 17/11/08 02:03:23 INFO mapreduce.Job: Job job_1508671365448_2956 completed successfully\n08-11-2017 02:03:23 CST run_dataimport INFO - 17/11/08 02:03:23 INFO mapreduce.Job: Counters: 30\n08-11-2017 02:03:23 CST run_dataimport INFO - \tFile System Counters\n08-11-2017 02:03:23 CST run_dataimport INFO - \t\tFILE: Number of bytes read=0\n08-11-2017 02:03:23 CST run_dataimport INFO - \t\tFILE: Number of bytes written=588300\n08-11-2017 02:03:23 CST run_dataimport INFO - \t\tFILE: Number of read operations=0\n08-11-2017 02:03:23 CST run_dataimport INFO - \t\tFILE: Number of large read operations=0\n08-11-2017 02:03:23 CST run_dataimport INFO - \t\tFILE: Number of write operations=0\n08-11-2017 02:03:23 CST run_dataimport INFO - \t\tHDFS: Number of bytes read=462\n08-11-2017 02:03:23 CST run_dataimport INFO - \t\tHDFS: Number of bytes written=93800000\n08-11-2017 02:03:23 CST run_dataimport INFO - \t\tHDFS: Number of read operations=16\n08-11-2017 02:03:23 CST run_dataimport INFO - \t\tHDFS: Number of large read operations=0\n08-11-2017 02:03:23 CST run_dataimport INFO - \t\tHDFS: Number of write operations=8\n08-11-2017 02:03:23 CST run_dataimport INFO - \tJob Counters \n08-11-2017 02:03:23 CST run_dataimport INFO - \t\tLaunched map tasks=4\n08-11-2017 02:03:23 CST run_dataimport INFO - \t\tOther local map tasks=4\n08-11-2017 02:03:23 CST run_dataimport INFO - \t\tTotal time spent by all maps in occupied slots (ms)=38216711\n08-11-2017 02:03:23 CST run_dataimport INFO - \t\tTotal time spent by all reduces in occupied slots (ms)=0\n08-11-2017 02:03:23 CST run_dataimport INFO - \t\tTotal time spent by all map tasks (ms)=38216711\n08-11-2017 02:03:23 CST run_dataimport INFO - \t\tTotal vcore-seconds taken by all map tasks=38216711\n08-11-2017 02:03:23 CST run_dataimport INFO - \t\tTotal megabyte-seconds taken by all map tasks=39133912064\n08-11-2017 02:03:23 CST run_dataimport INFO - \tMap-Reduce Framework\n08-11-2017 02:03:23 CST run_dataimport INFO - \t\tMap input records=200000\n08-11-2017 02:03:23 CST run_dataimport INFO - \t\tMap output records=200000\n08-11-2017 02:03:23 CST run_dataimport INFO - \t\tInput split bytes=462\n08-11-2017 02:03:23 CST run_dataimport INFO - \t\tSpilled Records=0\n08-11-2017 02:03:23 CST run_dataimport INFO - \t\tFailed Shuffles=0\n08-11-2017 02:03:23 CST run_dataimport INFO - \t\tMerged Map outputs=0\n08-11-2017 02:03:23 CST run_dataimport INFO - \t\tGC time elapsed (ms)=1588\n08-11-2017 02:03:23 CST run_dataimport INFO - \t\tCPU time spent (ms)=134830\n08-11-2017 02:03:23 CST run_dataimport INFO - \t\tPhysical memory (bytes) snapshot=1547542528\n08-11-2017 02:03:23 CST run_dataimport INFO - \t\tVirtual memory (bytes) snapshot=6530818048\n08-11-2017 02:03:23 CST run_dataimport INFO - \t\tTotal committed heap usage (bytes)=3172990976\n08-11-2017 02:03:23 CST run_dataimport INFO - \tFile Input Format Counters \n08-11-2017 02:03:23 CST run_dataimport INFO - \t\tBytes Read=0\n08-11-2017 02:03:23 CST run_dataimport INFO - \tFile Output Format Counters \n08-11-2017 02:03:23 CST run_dataimport INFO - \t\tBytes Written=93800000\n08-11-2017 02:03:23 CST run_dataimport INFO - 17/11/08 02:03:23 INFO mapreduce.ImportJobBase: Transferred 89.4547 MB in 19,195.7726 seconds (4.772 KB/sec)\n08-11-2017 02:03:23 CST run_dataimport INFO - 17/11/08 02:03:23 INFO mapreduce.ImportJobBase: Retrieved 200000 records.\n08-11-2017 02:03:23 CST run_dataimport INFO - 17/11/08 02:03:23 INFO util.AppendUtils: Appending to directory sql_to_hive\n08-11-2017 02:03:24 CST run_dataimport INFO - 17/11/08 02:03:24 INFO util.AppendUtils: Using found partition 6808\n08-11-2017 02:03:24 CST run_dataimport INFO - 17/11/08 02:03:24 INFO tool.ImportTool: Incremental import complete! To run another incremental import of all data following this import, supply the following arguments:\n08-11-2017 02:03:24 CST run_dataimport INFO - 17/11/08 02:03:24 INFO tool.ImportTool:  --incremental append\n08-11-2017 02:03:24 CST run_dataimport INFO - 17/11/08 02:03:24 INFO tool.ImportTool:   --check-column uid\n08-11-2017 02:03:24 CST run_dataimport INFO - 17/11/08 02:03:24 INFO tool.ImportTool:   --last-value 83822300\n08-11-2017 02:03:24 CST run_dataimport INFO - 17/11/08 02:03:24 INFO tool.ImportTool: (Consider saving this with 'sqoop job --create')\n08-11-2017 02:03:24 CST run_dataimport INFO - 4\n08-11-2017 02:03:24 CST run_dataimport INFO - Warning: /opt/cloudera/parcels/CDH-5.8.0-1.cdh5.8.0.p0.42/bin/../lib/sqoop/../accumulo does not exist! Accumulo imports will fail.\n08-11-2017 02:03:24 CST run_dataimport INFO - Please set $ACCUMULO_HOME to the root of your Accumulo installation.\n08-11-2017 02:03:26 CST run_dataimport INFO - 17/11/08 02:03:26 INFO sqoop.Sqoop: Running Sqoop version: 1.4.6-cdh5.8.0\n08-11-2017 02:03:26 CST run_dataimport INFO - 17/11/08 02:03:26 WARN sqoop.ConnFactory: Parameter --driver is set to an explicit driver however appropriate connection manager is not being set (via --connection-manager). Sqoop is going to fall back to org.apache.sqoop.manager.GenericJdbcManager. Please specify explicitly which connection manager should be used next time.\n08-11-2017 02:03:26 CST run_dataimport INFO - 17/11/08 02:03:26 INFO manager.SqlManager: Using default fetchSize of 1000\n08-11-2017 02:03:26 CST run_dataimport INFO - 17/11/08 02:03:26 INFO tool.CodeGenTool: Beginning code generation\n08-11-2017 02:03:27 CST run_dataimport INFO - 17/11/08 02:03:27 INFO manager.SqlManager: Executing SQL statement: select top 200000 * from jzj.dbo.jzj_test where uid&gt;(select uid from (select top 600000 uid from jzj.dbo.jzj_test where uid&gt;83222300 and uid&lt;=84598500) a except select uid from (select top 599999 uid from jzj.dbo.jzj_test where uid&gt;83222300 and uid&lt;=84598500) a) and uid &lt;=84598500 and  (1 = 0) \n08-11-2017 02:03:27 CST run_dataimport INFO - 17/11/08 02:03:27 INFO manager.SqlManager: Executing SQL statement: select top 200000 * from jzj.dbo.jzj_test where uid&gt;(select uid from (select top 600000 uid from jzj.dbo.jzj_test where uid&gt;83222300 and uid&lt;=84598500) a except select uid from (select top 599999 uid from jzj.dbo.jzj_test where uid&gt;83222300 and uid&lt;=84598500) a) and uid &lt;=84598500 and  (1 = 0) \n08-11-2017 02:03:27 CST run_dataimport INFO - 17/11/08 02:03:27 INFO orm.CompilationManager: HADOOP_MAPRED_HOME is /opt/cloudera/parcels/CDH/lib/hadoop-mapreduce\n08-11-2017 02:03:29 CST run_dataimport INFO - Note: /tmp/sqoop-root/compile/e8acb4e9422617ba257a228b6dfa0561/QueryResult.java uses or overrides a deprecated API.\n08-11-2017 02:03:29 CST run_dataimport INFO - Note: Recompile with -Xlint:deprecation for details.\n08-11-2017 02:03:29 CST run_dataimport INFO - 17/11/08 02:03:29 INFO orm.CompilationManager: Writing jar file: /tmp/sqoop-root/compile/e8acb4e9422617ba257a228b6dfa0561/QueryResult.jar\n08-11-2017 02:03:30 CST run_dataimport INFO - 17/11/08 02:03:30 INFO tool.ImportTool: Maximal id query for free form incremental import: SELECT MAX(uid) FROM (select top 200000 * from jzj.dbo.jzj_test where uid&gt;(select uid from (select top 600000 uid from jzj.dbo.jzj_test where uid&gt;83222300 and uid&lt;=84598500) a except select uid from (select top 599999 uid from jzj.dbo.jzj_test where uid&gt;83222300 and uid&lt;=84598500) a) and uid &lt;=84598500 and (1 = 1)) sqoop_import_query_alias\n08-11-2017 07:55:53 CST run_dataimport INFO - 17/11/08 07:55:53 INFO tool.ImportTool: Incremental import based on column uid\n08-11-2017 07:55:53 CST run_dataimport INFO - 17/11/08 07:55:53 INFO tool.ImportTool: Upper bound value: 84022300\n08-11-2017 07:55:53 CST run_dataimport INFO - 17/11/08 07:55:53 INFO mapreduce.ImportJobBase: Beginning query import.\n08-11-2017 07:55:53 CST run_dataimport INFO - 17/11/08 07:55:53 INFO Configuration.deprecation: mapred.jar is deprecated. Instead, use mapreduce.job.jar\n08-11-2017 07:55:53 CST run_dataimport INFO - 17/11/08 07:55:53 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n08-11-2017 07:55:53 CST run_dataimport INFO - 17/11/08 07:55:53 INFO client.RMProxy: Connecting to ResourceManager at master/172.23.27.203:8032\n08-11-2017 07:56:00 CST run_dataimport INFO - 17/11/08 07:56:00 INFO db.DBInputFormat: Using read commited transaction isolation\n08-11-2017 07:56:00 CST run_dataimport INFO - 17/11/08 07:56:00 INFO db.DataDrivenDBInputFormat: BoundingValsQuery: SELECT MIN(uid), MAX(uid) FROM (select top 200000 * from jzj.dbo.jzj_test where uid&gt;(select uid from (select top 600000 uid from jzj.dbo.jzj_test where uid&gt;83222300 and uid&lt;=84598500) a except select uid from (select top 599999 uid from jzj.dbo.jzj_test where uid&gt;83222300 and uid&lt;=84598500) a) and uid &lt;=84598500 and uid &lt;= 84022300 AND  (1 = 1) ) AS t1\n08-11-2017 12:11:01 CST run_dataimport ERROR - Kill has been called.\n08-11-2017 12:11:06 CST run_dataimport INFO - Process completed unsuccessfully in 72615 seconds.\n08-11-2017 12:11:06 CST run_dataimport ERROR - Job run killed!\njava.lang.RuntimeException: azkaban.jobExecutor.utils.process.ProcessFailureException\n\tat azkaban.jobExecutor.ProcessJob.run(ProcessJob.java:297)\n\tat azkaban.execapp.JobRunner.runJob(JobRunner.java:748)\n\tat azkaban.execapp.JobRunner.doRun(JobRunner.java:591)\n\tat azkaban.execapp.JobRunner.run(JobRunner.java:552)\n\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: azkaban.jobExecutor.utils.process.ProcessFailureException\n\tat azkaban.jobExecutor.utils.process.AzkabanProcess.run(AzkabanProcess.java:130)\n\tat azkaban.jobExecutor.ProcessJob.run(ProcessJob.java:289)\n\t... 8 more\n08-11-2017 12:11:06 CST run_dataimport ERROR - azkaban.jobExecutor.utils.process.ProcessFailureException cause: azkaban.jobExecutor.utils.process.ProcessFailureException\n08-11-2017 12:11:06 CST run_dataimport INFO - Finishing job run_dataimport at 1510114266951 with status KILLED\n", 'length': 37422}
[2017-11-20 16:35:38,833] app.hive_mysql.AzkabanMonitor [INFO] URL: https://172.23.27.203:8443
[2017-11-20 16:35:39,007] app.hive_mysql.AzkabanMonitor [INFO] Success: Login https://172.23.27.203:8443 with user azkaban
[2017-11-20 16:35:39,007] app.hive_mysql.AzkabanMonitor [DEBUG] {'session.id': 'e093e1d0-97fe-461a-92a9-9697e0a47972', 'status': 'success'}
[2017-11-20 16:35:39,022] app.hive_mysql.AzkabanMonitor [INFO] Success: Fetch Executions of a Flow - run_dataimport
[2017-11-20 16:35:39,022] app.hive_mysql.AzkabanMonitor [DEBUG] {'total': 16, 'executions': [{'submitTime': 1510041651107, 'submitUser': 'azkaban', 'startTime': 1510041651277, 'endTime': 1510114267057, 'flowId': 'run_dataimport', 'projectId': 68, 'execId': 1670, 'status': 'KILLED'}, {'submitTime': 1510027251078, 'submitUser': 'azkaban', 'startTime': 1510027251395, 'endTime': 1510036906931, 'flowId': 'run_dataimport', 'projectId': 68, 'execId': 1669, 'status': 'SUCCEEDED'}, {'submitTime': 1509897650826, 'submitUser': 'azkaban', 'startTime': 1509897651116, 'endTime': 1510026870568, 'flowId': 'run_dataimport', 'projectId': 68, 'execId': 1668, 'status': 'SUCCEEDED'}, {'submitTime': 1509883250800, 'submitUser': 'azkaban', 'startTime': 1509883251056, 'endTime': 1509892870422, 'flowId': 'run_dataimport', 'projectId': 68, 'execId': 1667, 'status': 'SUCCEEDED'}, {'submitTime': 1509753650545, 'submitUser': 'azkaban', 'startTime': 1509753650705, 'endTime': 1509881095432, 'flowId': 'run_dataimport', 'projectId': 68, 'execId': 1666, 'status': 'SUCCEEDED'}, {'submitTime': 1509746450529, 'submitUser': 'azkaban', 'startTime': 1509746450965, 'endTime': 1509753076918, 'flowId': 'run_dataimport', 'projectId': 68, 'execId': 1665, 'status': 'SUCCEEDED'}, {'submitTime': 1509652850362, 'submitUser': 'azkaban', 'startTime': 1509652850560, 'endTime': 1509744409191, 'flowId': 'run_dataimport', 'projectId': 68, 'execId': 1664, 'status': 'SUCCEEDED'}, {'submitTime': 1509645650349, 'submitUser': 'azkaban', 'startTime': 1509645650498, 'endTime': 1509651159750, 'flowId': 'run_dataimport', 'projectId': 68, 'execId': 1663, 'status': 'SUCCEEDED'}, {'submitTime': 1509566450178, 'submitUser': 'azkaban', 'startTime': 1509566450488, 'endTime': 1509642929867, 'flowId': 'run_dataimport', 'projectId': 68, 'execId': 1661, 'status': 'SUCCEEDED'}, {'submitTime': 1509559250163, 'submitUser': 'azkaban', 'startTime': 1509559250321, 'endTime': 1509563186239, 'flowId': 'run_dataimport', 'projectId': 68, 'execId': 1660, 'status': 'SUCCEEDED'}], 'length': 10, 'project': 'sql_to_hive', 'from': 0, 'projectId': 68, 'flow': 'run_dataimport'}
[2017-11-20 16:35:39,038] app.hive_mysql.AzkabanMonitor [INFO] Success: Fetch Execution Job Logs - 1670-run_dataimport
[2017-11-20 16:35:39,038] app.hive_mysql.AzkabanMonitor [DEBUG] {'offset': 0, 'data': "07-11-2017 16:00:51 CST run_dataimport INFO - Starting job run_dataimport at 1510041651281\n07-11-2017 16:00:51 CST run_dataimport INFO - azkaban.webserver.url property was not set\n07-11-2017 16:00:51 CST run_dataimport INFO - job JVM args: -Dazkaban.flowid=run_dataimport -Dazkaban.execid=1670 -Dazkaban.jobid=run_dataimport\n07-11-2017 16:00:51 CST run_dataimport INFO - Building command job executor. \n07-11-2017 16:00:51 CST run_dataimport INFO - Memory granted for job run_dataimport\n07-11-2017 16:00:51 CST run_dataimport INFO - 1 commands to execute.\n07-11-2017 16:00:51 CST run_dataimport INFO - cwd=/home/azkaban/azkaban_bk/azkaban-exec-server-3.35.0-22-g5517d50/executions/1670\n07-11-2017 16:00:51 CST run_dataimport INFO - effective user is: azkaban\n07-11-2017 16:00:51 CST run_dataimport INFO - Command: sh /home/sql_to_hive/dataimport.sh\n07-11-2017 16:00:51 CST run_dataimport INFO - Environment variables: {JOB_OUTPUT_PROP_FILE=/home/azkaban/azkaban_bk/azkaban-exec-server-3.35.0-22-g5517d50/executions/1670/run_dataimport_output_8807970133616151260_tmp, JOB_PROP_FILE=/home/azkaban/azkaban_bk/azkaban-exec-server-3.35.0-22-g5517d50/executions/1670/run_dataimport_props_6140684690151138768_tmp, KRB5CCNAME=/tmp/krb5cc__sql_to_hive__run_dataimport__run_dataimport__1670__azkaban, JOB_NAME=run_dataimport}\n07-11-2017 16:00:51 CST run_dataimport INFO - Working directory: /home/azkaban/azkaban_bk/azkaban-exec-server-3.35.0-22-g5517d50/executions/1670\n07-11-2017 16:00:51 CST run_dataimport INFO - log4j:WARN No appenders could be found for logger (org.apache.hive.jdbc.Utils).\n07-11-2017 16:00:51 CST run_dataimport INFO - log4j:WARN Please initialize the log4j system properly.\n07-11-2017 16:00:51 CST run_dataimport INFO - log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.\n07-11-2017 16:00:51 CST run_dataimport INFO - SLF4J: Failed to load class &quot;org.slf4j.impl.StaticLoggerBinder&quot;.\n07-11-2017 16:00:51 CST run_dataimport INFO - SLF4J: Defaulting to no-operation (NOP) logger implementation\n07-11-2017 16:00:51 CST run_dataimport INFO - SLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.\n07-11-2017 16:02:05 CST run_dataimport INFO - 1,84598500,83222300,1376200sqlmax:84598500\n07-11-2017 16:02:05 CST run_dataimport INFO - hivemax:83222300\n07-11-2017 16:02:05 CST run_dataimport INFO - counts:1376200\n07-11-2017 16:02:05 CST run_dataimport INFO - 7,176200\n07-11-2017 16:02:05 CST run_dataimport INFO - 7\n07-11-2017 16:02:05 CST run_dataimport INFO - Warning: /opt/cloudera/parcels/CDH-5.8.0-1.cdh5.8.0.p0.42/bin/../lib/sqoop/../accumulo does not exist! Accumulo imports will fail.\n07-11-2017 16:02:05 CST run_dataimport INFO - Please set $ACCUMULO_HOME to the root of your Accumulo installation.\n07-11-2017 16:02:07 CST run_dataimport INFO - 17/11/07 16:02:07 INFO sqoop.Sqoop: Running Sqoop version: 1.4.6-cdh5.8.0\n07-11-2017 16:02:07 CST run_dataimport INFO - 17/11/07 16:02:07 WARN sqoop.ConnFactory: Parameter --driver is set to an explicit driver however appropriate connection manager is not being set (via --connection-manager). Sqoop is going to fall back to org.apache.sqoop.manager.GenericJdbcManager. Please specify explicitly which connection manager should be used next time.\n07-11-2017 16:02:07 CST run_dataimport INFO - 17/11/07 16:02:07 INFO manager.SqlManager: Using default fetchSize of 1000\n07-11-2017 16:02:07 CST run_dataimport INFO - 17/11/07 16:02:07 INFO tool.CodeGenTool: Beginning code generation\n07-11-2017 16:02:08 CST run_dataimport INFO - 17/11/07 16:02:08 INFO manager.SqlManager: Executing SQL statement: select top 200000 * from jzj.dbo.jzj_test where uid&gt;=(select uid from (select top 1 uid from jzj.dbo.jzj_test where uid&gt;83222300 and uid&lt;=84598500) a) and uid &lt;=84598500 and  (1 = 0) \n07-11-2017 16:02:08 CST run_dataimport INFO - 17/11/07 16:02:08 INFO manager.SqlManager: Executing SQL statement: select top 200000 * from jzj.dbo.jzj_test where uid&gt;=(select uid from (select top 1 uid from jzj.dbo.jzj_test where uid&gt;83222300 and uid&lt;=84598500) a) and uid &lt;=84598500 and  (1 = 0) \n07-11-2017 16:02:08 CST run_dataimport INFO - 17/11/07 16:02:08 INFO orm.CompilationManager: HADOOP_MAPRED_HOME is /opt/cloudera/parcels/CDH/lib/hadoop-mapreduce\n07-11-2017 16:02:10 CST run_dataimport INFO - Note: /tmp/sqoop-root/compile/b796f2c0b0198c05bdcf8149b3f9a7fd/QueryResult.java uses or overrides a deprecated API.\n07-11-2017 16:02:10 CST run_dataimport INFO - Note: Recompile with -Xlint:deprecation for details.\n07-11-2017 16:02:10 CST run_dataimport INFO - 17/11/07 16:02:10 INFO orm.CompilationManager: Writing jar file: /tmp/sqoop-root/compile/b796f2c0b0198c05bdcf8149b3f9a7fd/QueryResult.jar\n07-11-2017 16:02:11 CST run_dataimport INFO - 17/11/07 16:02:11 INFO tool.ImportTool: Maximal id query for free form incremental import: SELECT MAX(uid) FROM (select top 200000 * from jzj.dbo.jzj_test where uid&gt;=(select uid from (select top 1 uid from jzj.dbo.jzj_test where uid&gt;83222300 and uid&lt;=84598500) a) and uid &lt;=84598500 and (1 = 1)) sqoop_import_query_alias\n07-11-2017 16:02:11 CST run_dataimport INFO - 17/11/07 16:02:11 INFO tool.ImportTool: Incremental import based on column uid\n07-11-2017 16:02:11 CST run_dataimport INFO - 17/11/07 16:02:11 INFO tool.ImportTool: Upper bound value: 83422300\n07-11-2017 16:02:11 CST run_dataimport INFO - 17/11/07 16:02:11 INFO mapreduce.ImportJobBase: Beginning query import.\n07-11-2017 16:02:11 CST run_dataimport INFO - 17/11/07 16:02:11 INFO Configuration.deprecation: mapred.jar is deprecated. Instead, use mapreduce.job.jar\n07-11-2017 16:02:11 CST run_dataimport INFO - 17/11/07 16:02:11 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n07-11-2017 16:02:11 CST run_dataimport INFO - 17/11/07 16:02:11 INFO client.RMProxy: Connecting to ResourceManager at master/172.23.27.203:8032\n07-11-2017 16:02:18 CST run_dataimport INFO - 17/11/07 16:02:18 INFO db.DBInputFormat: Using read commited transaction isolation\n07-11-2017 16:02:18 CST run_dataimport INFO - 17/11/07 16:02:18 INFO db.DataDrivenDBInputFormat: BoundingValsQuery: SELECT MIN(uid), MAX(uid) FROM (select top 200000 * from jzj.dbo.jzj_test where uid&gt;=(select uid from (select top 1 uid from jzj.dbo.jzj_test where uid&gt;83222300 and uid&lt;=84598500) a) and uid &lt;=84598500 and uid &lt;= 83422300 AND  (1 = 1) ) AS t1\n07-11-2017 16:02:18 CST run_dataimport INFO - 17/11/07 16:02:18 INFO mapreduce.JobSubmitter: number of splits:4\n07-11-2017 16:02:18 CST run_dataimport INFO - 17/11/07 16:02:18 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1508671365448_2954\n07-11-2017 16:02:19 CST run_dataimport INFO - 17/11/07 16:02:19 INFO impl.YarnClientImpl: Submitted application application_1508671365448_2954\n07-11-2017 16:02:19 CST run_dataimport INFO - 17/11/07 16:02:19 INFO mapreduce.Job: The url to track the job: http://master:8088/proxy/application_1508671365448_2954/\n07-11-2017 16:02:19 CST run_dataimport INFO - 17/11/07 16:02:19 INFO mapreduce.Job: Running job: job_1508671365448_2954\n07-11-2017 16:02:26 CST run_dataimport INFO - 17/11/07 16:02:26 INFO mapreduce.Job: Job job_1508671365448_2954 running in uber mode : false\n07-11-2017 16:02:26 CST run_dataimport INFO - 17/11/07 16:02:26 INFO mapreduce.Job:  map 0% reduce 0%\n07-11-2017 16:02:37 CST run_dataimport INFO - 17/11/07 16:02:37 INFO mapreduce.Job:  map 25% reduce 0%\n07-11-2017 16:02:40 CST run_dataimport INFO - 17/11/07 16:02:40 INFO mapreduce.Job:  map 100% reduce 0%\n07-11-2017 16:02:41 CST run_dataimport INFO - 17/11/07 16:02:41 INFO mapreduce.Job: Job job_1508671365448_2954 completed successfully\n07-11-2017 16:02:41 CST run_dataimport INFO - 17/11/07 16:02:41 INFO mapreduce.Job: Counters: 30\n07-11-2017 16:02:41 CST run_dataimport INFO - \tFile System Counters\n07-11-2017 16:02:41 CST run_dataimport INFO - \t\tFILE: Number of bytes read=0\n07-11-2017 16:02:41 CST run_dataimport INFO - \t\tFILE: Number of bytes written=587024\n07-11-2017 16:02:41 CST run_dataimport INFO - \t\tFILE: Number of read operations=0\n07-11-2017 16:02:41 CST run_dataimport INFO - \t\tFILE: Number of large read operations=0\n07-11-2017 16:02:41 CST run_dataimport INFO - \t\tFILE: Number of write operations=0\n07-11-2017 16:02:41 CST run_dataimport INFO - \t\tHDFS: Number of bytes read=462\n07-11-2017 16:02:41 CST run_dataimport INFO - \t\tHDFS: Number of bytes written=93800000\n07-11-2017 16:02:41 CST run_dataimport INFO - \t\tHDFS: Number of read operations=16\n07-11-2017 16:02:41 CST run_dataimport INFO - \t\tHDFS: Number of large read operations=0\n07-11-2017 16:02:41 CST run_dataimport INFO - \t\tHDFS: Number of write operations=8\n07-11-2017 16:02:41 CST run_dataimport INFO - \tJob Counters \n07-11-2017 16:02:41 CST run_dataimport INFO - \t\tLaunched map tasks=4\n07-11-2017 16:02:41 CST run_dataimport INFO - \t\tOther local map tasks=4\n07-11-2017 16:02:41 CST run_dataimport INFO - \t\tTotal time spent by all maps in occupied slots (ms)=44342\n07-11-2017 16:02:41 CST run_dataimport INFO - \t\tTotal time spent by all reduces in occupied slots (ms)=0\n07-11-2017 16:02:41 CST run_dataimport INFO - \t\tTotal time spent by all map tasks (ms)=44342\n07-11-2017 16:02:41 CST run_dataimport INFO - \t\tTotal vcore-seconds taken by all map tasks=44342\n07-11-2017 16:02:41 CST run_dataimport INFO - \t\tTotal megabyte-seconds taken by all map tasks=45406208\n07-11-2017 16:02:41 CST run_dataimport INFO - \tMap-Reduce Framework\n07-11-2017 16:02:41 CST run_dataimport INFO - \t\tMap input records=200000\n07-11-2017 16:02:41 CST run_dataimport INFO - \t\tMap output records=200000\n07-11-2017 16:02:41 CST run_dataimport INFO - \t\tInput split bytes=462\n07-11-2017 16:02:41 CST run_dataimport INFO - \t\tSpilled Records=0\n07-11-2017 16:02:41 CST run_dataimport INFO - \t\tFailed Shuffles=0\n07-11-2017 16:02:41 CST run_dataimport INFO - \t\tMerged Map outputs=0\n07-11-2017 16:02:41 CST run_dataimport INFO - \t\tGC time elapsed (ms)=569\n07-11-2017 16:02:41 CST run_dataimport INFO - \t\tCPU time spent (ms)=36080\n07-11-2017 16:02:41 CST run_dataimport INFO - \t\tPhysical memory (bytes) snapshot=1458102272\n07-11-2017 16:02:41 CST run_dataimport INFO - \t\tVirtual memory (bytes) snapshot=6516502528\n07-11-2017 16:02:41 CST run_dataimport INFO - \t\tTotal committed heap usage (bytes)=3231186944\n07-11-2017 16:02:41 CST run_dataimport INFO - \tFile Input Format Counters \n07-11-2017 16:02:41 CST run_dataimport INFO - \t\tBytes Read=0\n07-11-2017 16:02:41 CST run_dataimport INFO - \tFile Output Format Counters \n07-11-2017 16:02:41 CST run_dataimport INFO - \t\tBytes Written=93800000\n07-11-2017 16:02:41 CST run_dataimport INFO - 17/11/07 16:02:41 INFO mapreduce.ImportJobBase: Transferred 89.4547 MB in 29.9824 seconds (2.9836 MB/sec)\n07-11-2017 16:02:41 CST run_dataimport INFO - 17/11/07 16:02:41 INFO mapreduce.ImportJobBase: Retrieved 200000 records.\n07-11-2017 16:02:41 CST run_dataimport INFO - 17/11/07 16:02:41 INFO util.AppendUtils: Appending to directory sql_to_hive\n07-11-2017 16:02:42 CST run_dataimport INFO - 17/11/07 16:02:42 INFO util.AppendUtils: Using found partition 6800\n07-11-2017 16:02:42 CST run_dataimport INFO - 17/11/07 16:02:42 INFO tool.ImportTool: Incremental import complete! To run another incremental import of all data following this import, supply the following arguments:\n07-11-2017 16:02:42 CST run_dataimport INFO - 17/11/07 16:02:42 INFO tool.ImportTool:  --incremental append\n07-11-2017 16:02:42 CST run_dataimport INFO - 17/11/07 16:02:42 INFO tool.ImportTool:   --check-column uid\n07-11-2017 16:02:42 CST run_dataimport INFO - 17/11/07 16:02:42 INFO tool.ImportTool:   --last-value 83422300\n07-11-2017 16:02:42 CST run_dataimport INFO - 17/11/07 16:02:42 INFO tool.ImportTool: (Consider saving this with 'sqoop job --create')\n07-11-2017 16:02:42 CST run_dataimport INFO - 6\n07-11-2017 16:02:42 CST run_dataimport INFO - Warning: /opt/cloudera/parcels/CDH-5.8.0-1.cdh5.8.0.p0.42/bin/../lib/sqoop/../accumulo does not exist! Accumulo imports will fail.\n07-11-2017 16:02:42 CST run_dataimport INFO - Please set $ACCUMULO_HOME to the root of your Accumulo installation.\n07-11-2017 16:02:44 CST run_dataimport INFO - 17/11/07 16:02:44 INFO sqoop.Sqoop: Running Sqoop version: 1.4.6-cdh5.8.0\n07-11-2017 16:02:44 CST run_dataimport INFO - 17/11/07 16:02:44 WARN sqoop.ConnFactory: Parameter --driver is set to an explicit driver however appropriate connection manager is not being set (via --connection-manager). Sqoop is going to fall back to org.apache.sqoop.manager.GenericJdbcManager. Please specify explicitly which connection manager should be used next time.\n07-11-2017 16:02:44 CST run_dataimport INFO - 17/11/07 16:02:44 INFO manager.SqlManager: Using default fetchSize of 1000\n07-11-2017 16:02:44 CST run_dataimport INFO - 17/11/07 16:02:44 INFO tool.CodeGenTool: Beginning code generation\n07-11-2017 16:02:45 CST run_dataimport INFO - 17/11/07 16:02:45 INFO manager.SqlManager: Executing SQL statement: select top 200000 * from jzj.dbo.jzj_test where uid&gt;(select uid from (select top 200000 uid from jzj.dbo.jzj_test where uid&gt;83222300 and uid&lt;=84598500) a except select uid from (select top 199999 uid from jzj.dbo.jzj_test where uid&gt;83222300 and uid&lt;=84598500) a) and uid &lt;=84598500 and  (1 = 0) \n07-11-2017 16:02:45 CST run_dataimport INFO - 17/11/07 16:02:45 INFO manager.SqlManager: Executing SQL statement: select top 200000 * from jzj.dbo.jzj_test where uid&gt;(select uid from (select top 200000 uid from jzj.dbo.jzj_test where uid&gt;83222300 and uid&lt;=84598500) a except select uid from (select top 199999 uid from jzj.dbo.jzj_test where uid&gt;83222300 and uid&lt;=84598500) a) and uid &lt;=84598500 and  (1 = 0) \n07-11-2017 16:02:45 CST run_dataimport INFO - 17/11/07 16:02:45 INFO orm.CompilationManager: HADOOP_MAPRED_HOME is /opt/cloudera/parcels/CDH/lib/hadoop-mapreduce\n07-11-2017 16:02:47 CST run_dataimport INFO - Note: /tmp/sqoop-root/compile/3e3fe91d38feffbabab3df86c4ddb2c4/QueryResult.java uses or overrides a deprecated API.\n07-11-2017 16:02:47 CST run_dataimport INFO - Note: Recompile with -Xlint:deprecation for details.\n07-11-2017 16:02:47 CST run_dataimport INFO - 17/11/07 16:02:47 INFO orm.CompilationManager: Writing jar file: /tmp/sqoop-root/compile/3e3fe91d38feffbabab3df86c4ddb2c4/QueryResult.jar\n07-11-2017 16:02:48 CST run_dataimport INFO - 17/11/07 16:02:48 INFO tool.ImportTool: Maximal id query for free form incremental import: SELECT MAX(uid) FROM (select top 200000 * from jzj.dbo.jzj_test where uid&gt;(select uid from (select top 200000 uid from jzj.dbo.jzj_test where uid&gt;83222300 and uid&lt;=84598500) a except select uid from (select top 199999 uid from jzj.dbo.jzj_test where uid&gt;83222300 and uid&lt;=84598500) a) and uid &lt;=84598500 and (1 = 1)) sqoop_import_query_alias\n07-11-2017 16:42:28 CST run_dataimport INFO - 17/11/07 16:42:28 INFO tool.ImportTool: Incremental import based on column uid\n07-11-2017 16:42:28 CST run_dataimport INFO - 17/11/07 16:42:28 INFO tool.ImportTool: Upper bound value: 83622300\n07-11-2017 16:42:28 CST run_dataimport INFO - 17/11/07 16:42:28 INFO mapreduce.ImportJobBase: Beginning query import.\n07-11-2017 16:42:28 CST run_dataimport INFO - 17/11/07 16:42:28 INFO Configuration.deprecation: mapred.jar is deprecated. Instead, use mapreduce.job.jar\n07-11-2017 16:42:28 CST run_dataimport INFO - 17/11/07 16:42:28 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n07-11-2017 16:42:28 CST run_dataimport INFO - 17/11/07 16:42:28 INFO client.RMProxy: Connecting to ResourceManager at master/172.23.27.203:8032\n07-11-2017 16:42:34 CST run_dataimport INFO - 17/11/07 16:42:34 INFO db.DBInputFormat: Using read commited transaction isolation\n07-11-2017 16:42:34 CST run_dataimport INFO - 17/11/07 16:42:34 INFO db.DataDrivenDBInputFormat: BoundingValsQuery: SELECT MIN(uid), MAX(uid) FROM (select top 200000 * from jzj.dbo.jzj_test where uid&gt;(select uid from (select top 200000 uid from jzj.dbo.jzj_test where uid&gt;83222300 and uid&lt;=84598500) a except select uid from (select top 199999 uid from jzj.dbo.jzj_test where uid&gt;83222300 and uid&lt;=84598500) a) and uid &lt;=84598500 and uid &lt;= 83622300 AND  (1 = 1) ) AS t1\n07-11-2017 17:22:49 CST run_dataimport INFO - 17/11/07 17:22:49 INFO mapreduce.JobSubmitter: number of splits:4\n07-11-2017 17:22:49 CST run_dataimport INFO - 17/11/07 17:22:49 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1508671365448_2955\n07-11-2017 17:22:49 CST run_dataimport INFO - 17/11/07 17:22:49 INFO impl.YarnClientImpl: Submitted application application_1508671365448_2955\n07-11-2017 17:22:49 CST run_dataimport INFO - 17/11/07 17:22:49 INFO mapreduce.Job: The url to track the job: http://master:8088/proxy/application_1508671365448_2955/\n07-11-2017 17:22:49 CST run_dataimport INFO - 17/11/07 17:22:49 INFO mapreduce.Job: Running job: job_1508671365448_2955\n07-11-2017 17:22:57 CST run_dataimport INFO - 17/11/07 17:22:57 INFO mapreduce.Job: Job job_1508671365448_2955 running in uber mode : false\n07-11-2017 17:22:57 CST run_dataimport INFO - 17/11/07 17:22:57 INFO mapreduce.Job:  map 0% reduce 0%\n07-11-2017 18:02:56 CST run_dataimport INFO - 17/11/07 18:02:56 INFO mapreduce.Job:  map 25% reduce 0%\n07-11-2017 18:03:23 CST run_dataimport INFO - 17/11/07 18:03:23 INFO mapreduce.Job:  map 50% reduce 0%\n07-11-2017 18:03:46 CST run_dataimport INFO - 17/11/07 18:03:46 INFO mapreduce.Job:  map 75% reduce 0%\n07-11-2017 18:03:47 CST run_dataimport INFO - 17/11/07 18:03:47 INFO mapreduce.Job:  map 100% reduce 0%\n07-11-2017 18:03:48 CST run_dataimport INFO - 17/11/07 18:03:48 INFO mapreduce.Job: Job job_1508671365448_2955 completed successfully\n07-11-2017 18:03:48 CST run_dataimport INFO - 17/11/07 18:03:48 INFO mapreduce.Job: Counters: 30\n07-11-2017 18:03:48 CST run_dataimport INFO - \tFile System Counters\n07-11-2017 18:03:48 CST run_dataimport INFO - \t\tFILE: Number of bytes read=0\n07-11-2017 18:03:48 CST run_dataimport INFO - \t\tFILE: Number of bytes written=588156\n07-11-2017 18:03:48 CST run_dataimport INFO - \t\tFILE: Number of read operations=0\n07-11-2017 18:03:48 CST run_dataimport INFO - \t\tFILE: Number of large read operations=0\n07-11-2017 18:03:48 CST run_dataimport INFO - \t\tFILE: Number of write operations=0\n07-11-2017 18:03:48 CST run_dataimport INFO - \t\tHDFS: Number of bytes read=462\n07-11-2017 18:03:48 CST run_dataimport INFO - \t\tHDFS: Number of bytes written=93800000\n07-11-2017 18:03:48 CST run_dataimport INFO - \t\tHDFS: Number of read operations=16\n07-11-2017 18:03:48 CST run_dataimport INFO - \t\tHDFS: Number of large read operations=0\n07-11-2017 18:03:48 CST run_dataimport INFO - \t\tHDFS: Number of write operations=8\n07-11-2017 18:03:48 CST run_dataimport INFO - \tJob Counters \n07-11-2017 18:03:48 CST run_dataimport INFO - \t\tLaunched map tasks=4\n07-11-2017 18:03:48 CST run_dataimport INFO - \t\tOther local map tasks=4\n07-11-2017 18:03:48 CST run_dataimport INFO - \t\tTotal time spent by all maps in occupied slots (ms)=9712787\n07-11-2017 18:03:48 CST run_dataimport INFO - \t\tTotal time spent by all reduces in occupied slots (ms)=0\n07-11-2017 18:03:48 CST run_dataimport INFO - \t\tTotal time spent by all map tasks (ms)=9712787\n07-11-2017 18:03:48 CST run_dataimport INFO - \t\tTotal vcore-seconds taken by all map tasks=9712787\n07-11-2017 18:03:48 CST run_dataimport INFO - \t\tTotal megabyte-seconds taken by all map tasks=9945893888\n07-11-2017 18:03:48 CST run_dataimport INFO - \tMap-Reduce Framework\n07-11-2017 18:03:48 CST run_dataimport INFO - \t\tMap input records=200000\n07-11-2017 18:03:48 CST run_dataimport INFO - \t\tMap output records=200000\n07-11-2017 18:03:48 CST run_dataimport INFO - \t\tInput split bytes=462\n07-11-2017 18:03:48 CST run_dataimport INFO - \t\tSpilled Records=0\n07-11-2017 18:03:48 CST run_dataimport INFO - \t\tFailed Shuffles=0\n07-11-2017 18:03:48 CST run_dataimport INFO - \t\tMerged Map outputs=0\n07-11-2017 18:03:48 CST run_dataimport INFO - \t\tGC time elapsed (ms)=764\n07-11-2017 18:03:48 CST run_dataimport INFO - \t\tCPU time spent (ms)=64470\n07-11-2017 18:03:48 CST run_dataimport INFO - \t\tPhysical memory (bytes) snapshot=1473638400\n07-11-2017 18:03:48 CST run_dataimport INFO - \t\tVirtual memory (bytes) snapshot=6498091008\n07-11-2017 18:03:48 CST run_dataimport INFO - \t\tTotal committed heap usage (bytes)=3240099840\n07-11-2017 18:03:48 CST run_dataimport INFO - \tFile Input Format Counters \n07-11-2017 18:03:48 CST run_dataimport INFO - \t\tBytes Read=0\n07-11-2017 18:03:48 CST run_dataimport INFO - \tFile Output Format Counters \n07-11-2017 18:03:48 CST run_dataimport INFO - \t\tBytes Written=93800000\n07-11-2017 18:03:48 CST run_dataimport INFO - 17/11/07 18:03:48 INFO mapreduce.ImportJobBase: Transferred 89.4547 MB in 4,879.8256 seconds (18.7715 KB/sec)\n07-11-2017 18:03:48 CST run_dataimport INFO - 17/11/07 18:03:48 INFO mapreduce.ImportJobBase: Retrieved 200000 records.\n07-11-2017 18:03:48 CST run_dataimport INFO - 17/11/07 18:03:48 INFO util.AppendUtils: Appending to directory sql_to_hive\n07-11-2017 18:03:48 CST run_dataimport INFO - 17/11/07 18:03:48 INFO util.AppendUtils: Using found partition 6804\n07-11-2017 18:03:49 CST run_dataimport INFO - 17/11/07 18:03:49 INFO tool.ImportTool: Incremental import complete! To run another incremental import of all data following this import, supply the following arguments:\n07-11-2017 18:03:49 CST run_dataimport INFO - 17/11/07 18:03:49 INFO tool.ImportTool:  --incremental append\n07-11-2017 18:03:49 CST run_dataimport INFO - 17/11/07 18:03:49 INFO tool.ImportTool:   --check-column uid\n07-11-2017 18:03:49 CST run_dataimport INFO - 17/11/07 18:03:49 INFO tool.ImportTool:   --last-value 83622300\n07-11-2017 18:03:49 CST run_dataimport INFO - 17/11/07 18:03:49 INFO tool.ImportTool: (Consider saving this with 'sqoop job --create')\n07-11-2017 18:03:49 CST run_dataimport INFO - 5\n07-11-2017 18:03:49 CST run_dataimport INFO - Warning: /opt/cloudera/parcels/CDH-5.8.0-1.cdh5.8.0.p0.42/bin/../lib/sqoop/../accumulo does not exist! Accumulo imports will fail.\n07-11-2017 18:03:49 CST run_dataimport INFO - Please set $ACCUMULO_HOME to the root of your Accumulo installation.\n07-11-2017 18:03:51 CST run_dataimport INFO - 17/11/07 18:03:51 INFO sqoop.Sqoop: Running Sqoop version: 1.4.6-cdh5.8.0\n07-11-2017 18:03:51 CST run_dataimport INFO - 17/11/07 18:03:51 WARN sqoop.ConnFactory: Parameter --driver is set to an explicit driver however appropriate connection manager is not being set (via --connection-manager). Sqoop is going to fall back to org.apache.sqoop.manager.GenericJdbcManager. Please specify explicitly which connection manager should be used next time.\n07-11-2017 18:03:51 CST run_dataimport INFO - 17/11/07 18:03:51 INFO manager.SqlManager: Using default fetchSize of 1000\n07-11-2017 18:03:51 CST run_dataimport INFO - 17/11/07 18:03:51 INFO tool.CodeGenTool: Beginning code generation\n07-11-2017 18:03:52 CST run_dataimport INFO - 17/11/07 18:03:52 INFO manager.SqlManager: Executing SQL statement: select top 200000 * from jzj.dbo.jzj_test where uid&gt;(select uid from (select top 400000 uid from jzj.dbo.jzj_test where uid&gt;83222300 and uid&lt;=84598500) a except select uid from (select top 399999 uid from jzj.dbo.jzj_test where uid&gt;83222300 and uid&lt;=84598500) a) and uid &lt;=84598500 and  (1 = 0) \n07-11-2017 18:03:52 CST run_dataimport INFO - 17/11/07 18:03:52 INFO manager.SqlManager: Executing SQL statement: select top 200000 * from jzj.dbo.jzj_test where uid&gt;(select uid from (select top 400000 uid from jzj.dbo.jzj_test where uid&gt;83222300 and uid&lt;=84598500) a except select uid from (select top 399999 uid from jzj.dbo.jzj_test where uid&gt;83222300 and uid&lt;=84598500) a) and uid &lt;=84598500 and  (1 = 0) \n07-11-2017 18:03:52 CST run_dataimport INFO - 17/11/07 18:03:52 INFO orm.CompilationManager: HADOOP_MAPRED_HOME is /opt/cloudera/parcels/CDH/lib/hadoop-mapreduce\n07-11-2017 18:03:54 CST run_dataimport INFO - Note: /tmp/sqoop-root/compile/21f23a42adb9bf71452da9a3c7187b06/QueryResult.java uses or overrides a deprecated API.\n07-11-2017 18:03:54 CST run_dataimport INFO - Note: Recompile with -Xlint:deprecation for details.\n07-11-2017 18:03:54 CST run_dataimport INFO - 17/11/07 18:03:54 INFO orm.CompilationManager: Writing jar file: /tmp/sqoop-root/compile/21f23a42adb9bf71452da9a3c7187b06/QueryResult.jar\n07-11-2017 18:03:55 CST run_dataimport INFO - 17/11/07 18:03:55 INFO tool.ImportTool: Maximal id query for free form incremental import: SELECT MAX(uid) FROM (select top 200000 * from jzj.dbo.jzj_test where uid&gt;(select uid from (select top 400000 uid from jzj.dbo.jzj_test where uid&gt;83222300 and uid&lt;=84598500) a except select uid from (select top 399999 uid from jzj.dbo.jzj_test where uid&gt;83222300 and uid&lt;=84598500) a) and uid &lt;=84598500 and (1 = 1)) sqoop_import_query_alias\n07-11-2017 20:43:27 CST run_dataimport INFO - 17/11/07 20:43:27 INFO tool.ImportTool: Incremental import based on column uid\n07-11-2017 20:43:27 CST run_dataimport INFO - 17/11/07 20:43:27 INFO tool.ImportTool: Upper bound value: 83822300\n07-11-2017 20:43:27 CST run_dataimport INFO - 17/11/07 20:43:27 INFO mapreduce.ImportJobBase: Beginning query import.\n07-11-2017 20:43:27 CST run_dataimport INFO - 17/11/07 20:43:27 INFO Configuration.deprecation: mapred.jar is deprecated. Instead, use mapreduce.job.jar\n07-11-2017 20:43:27 CST run_dataimport INFO - 17/11/07 20:43:27 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n07-11-2017 20:43:28 CST run_dataimport INFO - 17/11/07 20:43:28 INFO client.RMProxy: Connecting to ResourceManager at master/172.23.27.203:8032\n07-11-2017 20:43:34 CST run_dataimport INFO - 17/11/07 20:43:34 INFO db.DBInputFormat: Using read commited transaction isolation\n07-11-2017 20:43:34 CST run_dataimport INFO - 17/11/07 20:43:34 INFO db.DataDrivenDBInputFormat: BoundingValsQuery: SELECT MIN(uid), MAX(uid) FROM (select top 200000 * from jzj.dbo.jzj_test where uid&gt;(select uid from (select top 400000 uid from jzj.dbo.jzj_test where uid&gt;83222300 and uid&lt;=84598500) a except select uid from (select top 399999 uid from jzj.dbo.jzj_test where uid&gt;83222300 and uid&lt;=84598500) a) and uid &lt;=84598500 and uid &lt;= 83822300 AND  (1 = 1) ) AS t1\n07-11-2017 23:22:41 CST run_dataimport INFO - 17/11/07 23:22:41 INFO mapreduce.JobSubmitter: number of splits:4\n07-11-2017 23:22:41 CST run_dataimport INFO - 17/11/07 23:22:41 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1508671365448_2956\n07-11-2017 23:22:42 CST run_dataimport INFO - 17/11/07 23:22:42 INFO impl.YarnClientImpl: Submitted application application_1508671365448_2956\n07-11-2017 23:22:42 CST run_dataimport INFO - 17/11/07 23:22:42 INFO mapreduce.Job: The url to track the job: http://master:8088/proxy/application_1508671365448_2956/\n07-11-2017 23:22:42 CST run_dataimport INFO - 17/11/07 23:22:42 INFO mapreduce.Job: Running job: job_1508671365448_2956\n07-11-2017 23:22:49 CST run_dataimport INFO - 17/11/07 23:22:49 INFO mapreduce.Job: Job job_1508671365448_2956 running in uber mode : false\n07-11-2017 23:22:49 CST run_dataimport INFO - 17/11/07 23:22:49 INFO mapreduce.Job:  map 0% reduce 0%\n08-11-2017 02:00:13 CST run_dataimport INFO - 17/11/08 02:00:13 INFO mapreduce.Job:  map 25% reduce 0%\n08-11-2017 02:01:28 CST run_dataimport INFO - 17/11/08 02:01:28 INFO mapreduce.Job:  map 50% reduce 0%\n08-11-2017 02:03:21 CST run_dataimport INFO - 17/11/08 02:03:21 INFO mapreduce.Job:  map 75% reduce 0%\n08-11-2017 02:03:22 CST run_dataimport INFO - 17/11/08 02:03:22 INFO mapreduce.Job:  map 100% reduce 0%\n08-11-2017 02:03:23 CST run_dataimport INFO - 17/11/08 02:03:23 INFO mapreduce.Job: Job job_1508671365448_2956 completed successfully\n08-11-2017 02:03:23 CST run_dataimport INFO - 17/11/08 02:03:23 INFO mapreduce.Job: Counters: 30\n08-11-2017 02:03:23 CST run_dataimport INFO - \tFile System Counters\n08-11-2017 02:03:23 CST run_dataimport INFO - \t\tFILE: Number of bytes read=0\n08-11-2017 02:03:23 CST run_dataimport INFO - \t\tFILE: Number of bytes written=588300\n08-11-2017 02:03:23 CST run_dataimport INFO - \t\tFILE: Number of read operations=0\n08-11-2017 02:03:23 CST run_dataimport INFO - \t\tFILE: Number of large read operations=0\n08-11-2017 02:03:23 CST run_dataimport INFO - \t\tFILE: Number of write operations=0\n08-11-2017 02:03:23 CST run_dataimport INFO - \t\tHDFS: Number of bytes read=462\n08-11-2017 02:03:23 CST run_dataimport INFO - \t\tHDFS: Number of bytes written=93800000\n08-11-2017 02:03:23 CST run_dataimport INFO - \t\tHDFS: Number of read operations=16\n08-11-2017 02:03:23 CST run_dataimport INFO - \t\tHDFS: Number of large read operations=0\n08-11-2017 02:03:23 CST run_dataimport INFO - \t\tHDFS: Number of write operations=8\n08-11-2017 02:03:23 CST run_dataimport INFO - \tJob Counters \n08-11-2017 02:03:23 CST run_dataimport INFO - \t\tLaunched map tasks=4\n08-11-2017 02:03:23 CST run_dataimport INFO - \t\tOther local map tasks=4\n08-11-2017 02:03:23 CST run_dataimport INFO - \t\tTotal time spent by all maps in occupied slots (ms)=38216711\n08-11-2017 02:03:23 CST run_dataimport INFO - \t\tTotal time spent by all reduces in occupied slots (ms)=0\n08-11-2017 02:03:23 CST run_dataimport INFO - \t\tTotal time spent by all map tasks (ms)=38216711\n08-11-2017 02:03:23 CST run_dataimport INFO - \t\tTotal vcore-seconds taken by all map tasks=38216711\n08-11-2017 02:03:23 CST run_dataimport INFO - \t\tTotal megabyte-seconds taken by all map tasks=39133912064\n08-11-2017 02:03:23 CST run_dataimport INFO - \tMap-Reduce Framework\n08-11-2017 02:03:23 CST run_dataimport INFO - \t\tMap input records=200000\n08-11-2017 02:03:23 CST run_dataimport INFO - \t\tMap output records=200000\n08-11-2017 02:03:23 CST run_dataimport INFO - \t\tInput split bytes=462\n08-11-2017 02:03:23 CST run_dataimport INFO - \t\tSpilled Records=0\n08-11-2017 02:03:23 CST run_dataimport INFO - \t\tFailed Shuffles=0\n08-11-2017 02:03:23 CST run_dataimport INFO - \t\tMerged Map outputs=0\n08-11-2017 02:03:23 CST run_dataimport INFO - \t\tGC time elapsed (ms)=1588\n08-11-2017 02:03:23 CST run_dataimport INFO - \t\tCPU time spent (ms)=134830\n08-11-2017 02:03:23 CST run_dataimport INFO - \t\tPhysical memory (bytes) snapshot=1547542528\n08-11-2017 02:03:23 CST run_dataimport INFO - \t\tVirtual memory (bytes) snapshot=6530818048\n08-11-2017 02:03:23 CST run_dataimport INFO - \t\tTotal committed heap usage (bytes)=3172990976\n08-11-2017 02:03:23 CST run_dataimport INFO - \tFile Input Format Counters \n08-11-2017 02:03:23 CST run_dataimport INFO - \t\tBytes Read=0\n08-11-2017 02:03:23 CST run_dataimport INFO - \tFile Output Format Counters \n08-11-2017 02:03:23 CST run_dataimport INFO - \t\tBytes Written=93800000\n08-11-2017 02:03:23 CST run_dataimport INFO - 17/11/08 02:03:23 INFO mapreduce.ImportJobBase: Transferred 89.4547 MB in 19,195.7726 seconds (4.772 KB/sec)\n08-11-2017 02:03:23 CST run_dataimport INFO - 17/11/08 02:03:23 INFO mapreduce.ImportJobBase: Retrieved 200000 records.\n08-11-2017 02:03:23 CST run_dataimport INFO - 17/11/08 02:03:23 INFO util.AppendUtils: Appending to directory sql_to_hive\n08-11-2017 02:03:24 CST run_dataimport INFO - 17/11/08 02:03:24 INFO util.AppendUtils: Using found partition 6808\n08-11-2017 02:03:24 CST run_dataimport INFO - 17/11/08 02:03:24 INFO tool.ImportTool: Incremental import complete! To run another incremental import of all data following this import, supply the following arguments:\n08-11-2017 02:03:24 CST run_dataimport INFO - 17/11/08 02:03:24 INFO tool.ImportTool:  --incremental append\n08-11-2017 02:03:24 CST run_dataimport INFO - 17/11/08 02:03:24 INFO tool.ImportTool:   --check-column uid\n08-11-2017 02:03:24 CST run_dataimport INFO - 17/11/08 02:03:24 INFO tool.ImportTool:   --last-value 83822300\n08-11-2017 02:03:24 CST run_dataimport INFO - 17/11/08 02:03:24 INFO tool.ImportTool: (Consider saving this with 'sqoop job --create')\n08-11-2017 02:03:24 CST run_dataimport INFO - 4\n08-11-2017 02:03:24 CST run_dataimport INFO - Warning: /opt/cloudera/parcels/CDH-5.8.0-1.cdh5.8.0.p0.42/bin/../lib/sqoop/../accumulo does not exist! Accumulo imports will fail.\n08-11-2017 02:03:24 CST run_dataimport INFO - Please set $ACCUMULO_HOME to the root of your Accumulo installation.\n08-11-2017 02:03:26 CST run_dataimport INFO - 17/11/08 02:03:26 INFO sqoop.Sqoop: Running Sqoop version: 1.4.6-cdh5.8.0\n08-11-2017 02:03:26 CST run_dataimport INFO - 17/11/08 02:03:26 WARN sqoop.ConnFactory: Parameter --driver is set to an explicit driver however appropriate connection manager is not being set (via --connection-manager). Sqoop is going to fall back to org.apache.sqoop.manager.GenericJdbcManager. Please specify explicitly which connection manager should be used next time.\n08-11-2017 02:03:26 CST run_dataimport INFO - 17/11/08 02:03:26 INFO manager.SqlManager: Using default fetchSize of 1000\n08-11-2017 02:03:26 CST run_dataimport INFO - 17/11/08 02:03:26 INFO tool.CodeGenTool: Beginning code generation\n08-11-2017 02:03:27 CST run_dataimport INFO - 17/11/08 02:03:27 INFO manager.SqlManager: Executing SQL statement: select top 200000 * from jzj.dbo.jzj_test where uid&gt;(select uid from (select top 600000 uid from jzj.dbo.jzj_test where uid&gt;83222300 and uid&lt;=84598500) a except select uid from (select top 599999 uid from jzj.dbo.jzj_test where uid&gt;83222300 and uid&lt;=84598500) a) and uid &lt;=84598500 and  (1 = 0) \n08-11-2017 02:03:27 CST run_dataimport INFO - 17/11/08 02:03:27 INFO manager.SqlManager: Executing SQL statement: select top 200000 * from jzj.dbo.jzj_test where uid&gt;(select uid from (select top 600000 uid from jzj.dbo.jzj_test where uid&gt;83222300 and uid&lt;=84598500) a except select uid from (select top 599999 uid from jzj.dbo.jzj_test where uid&gt;83222300 and uid&lt;=84598500) a) and uid &lt;=84598500 and  (1 = 0) \n08-11-2017 02:03:27 CST run_dataimport INFO - 17/11/08 02:03:27 INFO orm.CompilationManager: HADOOP_MAPRED_HOME is /opt/cloudera/parcels/CDH/lib/hadoop-mapreduce\n08-11-2017 02:03:29 CST run_dataimport INFO - Note: /tmp/sqoop-root/compile/e8acb4e9422617ba257a228b6dfa0561/QueryResult.java uses or overrides a deprecated API.\n08-11-2017 02:03:29 CST run_dataimport INFO - Note: Recompile with -Xlint:deprecation for details.\n08-11-2017 02:03:29 CST run_dataimport INFO - 17/11/08 02:03:29 INFO orm.CompilationManager: Writing jar file: /tmp/sqoop-root/compile/e8acb4e9422617ba257a228b6dfa0561/QueryResult.jar\n08-11-2017 02:03:30 CST run_dataimport INFO - 17/11/08 02:03:30 INFO tool.ImportTool: Maximal id query for free form incremental import: SELECT MAX(uid) FROM (select top 200000 * from jzj.dbo.jzj_test where uid&gt;(select uid from (select top 600000 uid from jzj.dbo.jzj_test where uid&gt;83222300 and uid&lt;=84598500) a except select uid from (select top 599999 uid from jzj.dbo.jzj_test where uid&gt;83222300 and uid&lt;=84598500) a) and uid &lt;=84598500 and (1 = 1)) sqoop_import_query_alias\n08-11-2017 07:55:53 CST run_dataimport INFO - 17/11/08 07:55:53 INFO tool.ImportTool: Incremental import based on column uid\n08-11-2017 07:55:53 CST run_dataimport INFO - 17/11/08 07:55:53 INFO tool.ImportTool: Upper bound value: 84022300\n08-11-2017 07:55:53 CST run_dataimport INFO - 17/11/08 07:55:53 INFO mapreduce.ImportJobBase: Beginning query import.\n08-11-2017 07:55:53 CST run_dataimport INFO - 17/11/08 07:55:53 INFO Configuration.deprecation: mapred.jar is deprecated. Instead, use mapreduce.job.jar\n08-11-2017 07:55:53 CST run_dataimport INFO - 17/11/08 07:55:53 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n08-11-2017 07:55:53 CST run_dataimport INFO - 17/11/08 07:55:53 INFO client.RMProxy: Connecting to ResourceManager at master/172.23.27.203:8032\n08-11-2017 07:56:00 CST run_dataimport INFO - 17/11/08 07:56:00 INFO db.DBInputFormat: Using read commited transaction isolation\n08-11-2017 07:56:00 CST run_dataimport INFO - 17/11/08 07:56:00 INFO db.DataDrivenDBInputFormat: BoundingValsQuery: SELECT MIN(uid), MAX(uid) FROM (select top 200000 * from jzj.dbo.jzj_test where uid&gt;(select uid from (select top 600000 uid from jzj.dbo.jzj_test where uid&gt;83222300 and uid&lt;=84598500) a except select uid from (select top 599999 uid from jzj.dbo.jzj_test where uid&gt;83222300 and uid&lt;=84598500) a) and uid &lt;=84598500 and uid &lt;= 84022300 AND  (1 = 1) ) AS t1\n08-11-2017 12:11:01 CST run_dataimport ERROR - Kill has been called.\n08-11-2017 12:11:06 CST run_dataimport INFO - Process completed unsuccessfully in 72615 seconds.\n08-11-2017 12:11:06 CST run_dataimport ERROR - Job run killed!\njava.lang.RuntimeException: azkaban.jobExecutor.utils.process.ProcessFailureException\n\tat azkaban.jobExecutor.ProcessJob.run(ProcessJob.java:297)\n\tat azkaban.execapp.JobRunner.runJob(JobRunner.java:748)\n\tat azkaban.execapp.JobRunner.doRun(JobRunner.java:591)\n\tat azkaban.execapp.JobRunner.run(JobRunner.java:552)\n\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: azkaban.jobExecutor.utils.process.ProcessFailureException\n\tat azkaban.jobExecutor.utils.process.AzkabanProcess.run(AzkabanProcess.java:130)\n\tat azkaban.jobExecutor.ProcessJob.run(ProcessJob.java:289)\n\t... 8 more\n08-11-2017 12:11:06 CST run_dataimport ERROR - azkaban.jobExecutor.utils.process.ProcessFailureException cause: azkaban.jobExecutor.utils.process.ProcessFailureException\n08-11-2017 12:11:06 CST run_dataimport INFO - Finishing job run_dataimport at 1510114266951 with status KILLED\n", 'length': 37422}
[2017-11-20 17:16:13,764] app.hive_mysql.AzkabanMonitor [INFO] URL: https://172.23.27.203:8443
[2017-11-20 17:16:13,941] app.hive_mysql.AzkabanMonitor [INFO] Success: Login https://172.23.27.203:8443 with user azkaban
[2017-11-20 17:16:13,941] app.hive_mysql.AzkabanMonitor [DEBUG] {'session.id': 'd80ea6ae-97b8-4137-ae78-e9bfc33b330a', 'status': 'success'}
[2017-11-20 17:16:13,957] app.hive_mysql.AzkabanMonitor [INFO] Success: Fetch Executions of a Flow - run_dataimport
[2017-11-20 17:16:13,958] app.hive_mysql.AzkabanMonitor [DEBUG] {'total': 16, 'executions': [{'submitTime': 1510041651107, 'submitUser': 'azkaban', 'startTime': 1510041651277, 'endTime': 1510114267057, 'flowId': 'run_dataimport', 'projectId': 68, 'execId': 1670, 'status': 'KILLED'}, {'submitTime': 1510027251078, 'submitUser': 'azkaban', 'startTime': 1510027251395, 'endTime': 1510036906931, 'flowId': 'run_dataimport', 'projectId': 68, 'execId': 1669, 'status': 'SUCCEEDED'}, {'submitTime': 1509897650826, 'submitUser': 'azkaban', 'startTime': 1509897651116, 'endTime': 1510026870568, 'flowId': 'run_dataimport', 'projectId': 68, 'execId': 1668, 'status': 'SUCCEEDED'}, {'submitTime': 1509883250800, 'submitUser': 'azkaban', 'startTime': 1509883251056, 'endTime': 1509892870422, 'flowId': 'run_dataimport', 'projectId': 68, 'execId': 1667, 'status': 'SUCCEEDED'}, {'submitTime': 1509753650545, 'submitUser': 'azkaban', 'startTime': 1509753650705, 'endTime': 1509881095432, 'flowId': 'run_dataimport', 'projectId': 68, 'execId': 1666, 'status': 'SUCCEEDED'}, {'submitTime': 1509746450529, 'submitUser': 'azkaban', 'startTime': 1509746450965, 'endTime': 1509753076918, 'flowId': 'run_dataimport', 'projectId': 68, 'execId': 1665, 'status': 'SUCCEEDED'}, {'submitTime': 1509652850362, 'submitUser': 'azkaban', 'startTime': 1509652850560, 'endTime': 1509744409191, 'flowId': 'run_dataimport', 'projectId': 68, 'execId': 1664, 'status': 'SUCCEEDED'}, {'submitTime': 1509645650349, 'submitUser': 'azkaban', 'startTime': 1509645650498, 'endTime': 1509651159750, 'flowId': 'run_dataimport', 'projectId': 68, 'execId': 1663, 'status': 'SUCCEEDED'}, {'submitTime': 1509566450178, 'submitUser': 'azkaban', 'startTime': 1509566450488, 'endTime': 1509642929867, 'flowId': 'run_dataimport', 'projectId': 68, 'execId': 1661, 'status': 'SUCCEEDED'}, {'submitTime': 1509559250163, 'submitUser': 'azkaban', 'startTime': 1509559250321, 'endTime': 1509563186239, 'flowId': 'run_dataimport', 'projectId': 68, 'execId': 1660, 'status': 'SUCCEEDED'}], 'length': 10, 'project': 'sql_to_hive', 'from': 0, 'projectId': 68, 'flow': 'run_dataimport'}
[2017-11-20 17:16:13,971] app.hive_mysql.AzkabanMonitor [INFO] Success: Fetch Execution Job Logs - 1670-run_dataimport
[2017-11-20 17:16:13,971] app.hive_mysql.AzkabanMonitor [DEBUG] {'offset': 0, 'data': "07-11-2017 16:00:51 CST run_dataimport INFO - Starting job run_dataimport at 1510041651281\n07-11-2017 16:00:51 CST run_dataimport INFO - azkaban.webserver.url property was not set\n07-11-2017 16:00:51 CST run_dataimport INFO - job JVM args: -Dazkaban.flowid=run_dataimport -Dazkaban.execid=1670 -Dazkaban.jobid=run_dataimport\n07-11-2017 16:00:51 CST run_dataimport INFO - Building command job executor. \n07-11-2017 16:00:51 CST run_dataimport INFO - Memory granted for job run_dataimport\n07-11-2017 16:00:51 CST run_dataimport INFO - 1 commands to execute.\n07-11-2017 16:00:51 CST run_dataimport INFO - cwd=/home/azkaban/azkaban_bk/azkaban-exec-server-3.35.0-22-g5517d50/executions/1670\n07-11-2017 16:00:51 CST run_dataimport INFO - effective user is: azkaban\n07-11-2017 16:00:51 CST run_dataimport INFO - Command: sh /home/sql_to_hive/dataimport.sh\n07-11-2017 16:00:51 CST run_dataimport INFO - Environment variables: {JOB_OUTPUT_PROP_FILE=/home/azkaban/azkaban_bk/azkaban-exec-server-3.35.0-22-g5517d50/executions/1670/run_dataimport_output_8807970133616151260_tmp, JOB_PROP_FILE=/home/azkaban/azkaban_bk/azkaban-exec-server-3.35.0-22-g5517d50/executions/1670/run_dataimport_props_6140684690151138768_tmp, KRB5CCNAME=/tmp/krb5cc__sql_to_hive__run_dataimport__run_dataimport__1670__azkaban, JOB_NAME=run_dataimport}\n07-11-2017 16:00:51 CST run_dataimport INFO - Working directory: /home/azkaban/azkaban_bk/azkaban-exec-server-3.35.0-22-g5517d50/executions/1670\n07-11-2017 16:00:51 CST run_dataimport INFO - log4j:WARN No appenders could be found for logger (org.apache.hive.jdbc.Utils).\n07-11-2017 16:00:51 CST run_dataimport INFO - log4j:WARN Please initialize the log4j system properly.\n07-11-2017 16:00:51 CST run_dataimport INFO - log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.\n07-11-2017 16:00:51 CST run_dataimport INFO - SLF4J: Failed to load class &quot;org.slf4j.impl.StaticLoggerBinder&quot;.\n07-11-2017 16:00:51 CST run_dataimport INFO - SLF4J: Defaulting to no-operation (NOP) logger implementation\n07-11-2017 16:00:51 CST run_dataimport INFO - SLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.\n07-11-2017 16:02:05 CST run_dataimport INFO - 1,84598500,83222300,1376200sqlmax:84598500\n07-11-2017 16:02:05 CST run_dataimport INFO - hivemax:83222300\n07-11-2017 16:02:05 CST run_dataimport INFO - counts:1376200\n07-11-2017 16:02:05 CST run_dataimport INFO - 7,176200\n07-11-2017 16:02:05 CST run_dataimport INFO - 7\n07-11-2017 16:02:05 CST run_dataimport INFO - Warning: /opt/cloudera/parcels/CDH-5.8.0-1.cdh5.8.0.p0.42/bin/../lib/sqoop/../accumulo does not exist! Accumulo imports will fail.\n07-11-2017 16:02:05 CST run_dataimport INFO - Please set $ACCUMULO_HOME to the root of your Accumulo installation.\n07-11-2017 16:02:07 CST run_dataimport INFO - 17/11/07 16:02:07 INFO sqoop.Sqoop: Running Sqoop version: 1.4.6-cdh5.8.0\n07-11-2017 16:02:07 CST run_dataimport INFO - 17/11/07 16:02:07 WARN sqoop.ConnFactory: Parameter --driver is set to an explicit driver however appropriate connection manager is not being set (via --connection-manager). Sqoop is going to fall back to org.apache.sqoop.manager.GenericJdbcManager. Please specify explicitly which connection manager should be used next time.\n07-11-2017 16:02:07 CST run_dataimport INFO - 17/11/07 16:02:07 INFO manager.SqlManager: Using default fetchSize of 1000\n07-11-2017 16:02:07 CST run_dataimport INFO - 17/11/07 16:02:07 INFO tool.CodeGenTool: Beginning code generation\n07-11-2017 16:02:08 CST run_dataimport INFO - 17/11/07 16:02:08 INFO manager.SqlManager: Executing SQL statement: select top 200000 * from jzj.dbo.jzj_test where uid&gt;=(select uid from (select top 1 uid from jzj.dbo.jzj_test where uid&gt;83222300 and uid&lt;=84598500) a) and uid &lt;=84598500 and  (1 = 0) \n07-11-2017 16:02:08 CST run_dataimport INFO - 17/11/07 16:02:08 INFO manager.SqlManager: Executing SQL statement: select top 200000 * from jzj.dbo.jzj_test where uid&gt;=(select uid from (select top 1 uid from jzj.dbo.jzj_test where uid&gt;83222300 and uid&lt;=84598500) a) and uid &lt;=84598500 and  (1 = 0) \n07-11-2017 16:02:08 CST run_dataimport INFO - 17/11/07 16:02:08 INFO orm.CompilationManager: HADOOP_MAPRED_HOME is /opt/cloudera/parcels/CDH/lib/hadoop-mapreduce\n07-11-2017 16:02:10 CST run_dataimport INFO - Note: /tmp/sqoop-root/compile/b796f2c0b0198c05bdcf8149b3f9a7fd/QueryResult.java uses or overrides a deprecated API.\n07-11-2017 16:02:10 CST run_dataimport INFO - Note: Recompile with -Xlint:deprecation for details.\n07-11-2017 16:02:10 CST run_dataimport INFO - 17/11/07 16:02:10 INFO orm.CompilationManager: Writing jar file: /tmp/sqoop-root/compile/b796f2c0b0198c05bdcf8149b3f9a7fd/QueryResult.jar\n07-11-2017 16:02:11 CST run_dataimport INFO - 17/11/07 16:02:11 INFO tool.ImportTool: Maximal id query for free form incremental import: SELECT MAX(uid) FROM (select top 200000 * from jzj.dbo.jzj_test where uid&gt;=(select uid from (select top 1 uid from jzj.dbo.jzj_test where uid&gt;83222300 and uid&lt;=84598500) a) and uid &lt;=84598500 and (1 = 1)) sqoop_import_query_alias\n07-11-2017 16:02:11 CST run_dataimport INFO - 17/11/07 16:02:11 INFO tool.ImportTool: Incremental import based on column uid\n07-11-2017 16:02:11 CST run_dataimport INFO - 17/11/07 16:02:11 INFO tool.ImportTool: Upper bound value: 83422300\n07-11-2017 16:02:11 CST run_dataimport INFO - 17/11/07 16:02:11 INFO mapreduce.ImportJobBase: Beginning query import.\n07-11-2017 16:02:11 CST run_dataimport INFO - 17/11/07 16:02:11 INFO Configuration.deprecation: mapred.jar is deprecated. Instead, use mapreduce.job.jar\n07-11-2017 16:02:11 CST run_dataimport INFO - 17/11/07 16:02:11 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n07-11-2017 16:02:11 CST run_dataimport INFO - 17/11/07 16:02:11 INFO client.RMProxy: Connecting to ResourceManager at master/172.23.27.203:8032\n07-11-2017 16:02:18 CST run_dataimport INFO - 17/11/07 16:02:18 INFO db.DBInputFormat: Using read commited transaction isolation\n07-11-2017 16:02:18 CST run_dataimport INFO - 17/11/07 16:02:18 INFO db.DataDrivenDBInputFormat: BoundingValsQuery: SELECT MIN(uid), MAX(uid) FROM (select top 200000 * from jzj.dbo.jzj_test where uid&gt;=(select uid from (select top 1 uid from jzj.dbo.jzj_test where uid&gt;83222300 and uid&lt;=84598500) a) and uid &lt;=84598500 and uid &lt;= 83422300 AND  (1 = 1) ) AS t1\n07-11-2017 16:02:18 CST run_dataimport INFO - 17/11/07 16:02:18 INFO mapreduce.JobSubmitter: number of splits:4\n07-11-2017 16:02:18 CST run_dataimport INFO - 17/11/07 16:02:18 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1508671365448_2954\n07-11-2017 16:02:19 CST run_dataimport INFO - 17/11/07 16:02:19 INFO impl.YarnClientImpl: Submitted application application_1508671365448_2954\n07-11-2017 16:02:19 CST run_dataimport INFO - 17/11/07 16:02:19 INFO mapreduce.Job: The url to track the job: http://master:8088/proxy/application_1508671365448_2954/\n07-11-2017 16:02:19 CST run_dataimport INFO - 17/11/07 16:02:19 INFO mapreduce.Job: Running job: job_1508671365448_2954\n07-11-2017 16:02:26 CST run_dataimport INFO - 17/11/07 16:02:26 INFO mapreduce.Job: Job job_1508671365448_2954 running in uber mode : false\n07-11-2017 16:02:26 CST run_dataimport INFO - 17/11/07 16:02:26 INFO mapreduce.Job:  map 0% reduce 0%\n07-11-2017 16:02:37 CST run_dataimport INFO - 17/11/07 16:02:37 INFO mapreduce.Job:  map 25% reduce 0%\n07-11-2017 16:02:40 CST run_dataimport INFO - 17/11/07 16:02:40 INFO mapreduce.Job:  map 100% reduce 0%\n07-11-2017 16:02:41 CST run_dataimport INFO - 17/11/07 16:02:41 INFO mapreduce.Job: Job job_1508671365448_2954 completed successfully\n07-11-2017 16:02:41 CST run_dataimport INFO - 17/11/07 16:02:41 INFO mapreduce.Job: Counters: 30\n07-11-2017 16:02:41 CST run_dataimport INFO - \tFile System Counters\n07-11-2017 16:02:41 CST run_dataimport INFO - \t\tFILE: Number of bytes read=0\n07-11-2017 16:02:41 CST run_dataimport INFO - \t\tFILE: Number of bytes written=587024\n07-11-2017 16:02:41 CST run_dataimport INFO - \t\tFILE: Number of read operations=0\n07-11-2017 16:02:41 CST run_dataimport INFO - \t\tFILE: Number of large read operations=0\n07-11-2017 16:02:41 CST run_dataimport INFO - \t\tFILE: Number of write operations=0\n07-11-2017 16:02:41 CST run_dataimport INFO - \t\tHDFS: Number of bytes read=462\n07-11-2017 16:02:41 CST run_dataimport INFO - \t\tHDFS: Number of bytes written=93800000\n07-11-2017 16:02:41 CST run_dataimport INFO - \t\tHDFS: Number of read operations=16\n07-11-2017 16:02:41 CST run_dataimport INFO - \t\tHDFS: Number of large read operations=0\n07-11-2017 16:02:41 CST run_dataimport INFO - \t\tHDFS: Number of write operations=8\n07-11-2017 16:02:41 CST run_dataimport INFO - \tJob Counters \n07-11-2017 16:02:41 CST run_dataimport INFO - \t\tLaunched map tasks=4\n07-11-2017 16:02:41 CST run_dataimport INFO - \t\tOther local map tasks=4\n07-11-2017 16:02:41 CST run_dataimport INFO - \t\tTotal time spent by all maps in occupied slots (ms)=44342\n07-11-2017 16:02:41 CST run_dataimport INFO - \t\tTotal time spent by all reduces in occupied slots (ms)=0\n07-11-2017 16:02:41 CST run_dataimport INFO - \t\tTotal time spent by all map tasks (ms)=44342\n07-11-2017 16:02:41 CST run_dataimport INFO - \t\tTotal vcore-seconds taken by all map tasks=44342\n07-11-2017 16:02:41 CST run_dataimport INFO - \t\tTotal megabyte-seconds taken by all map tasks=45406208\n07-11-2017 16:02:41 CST run_dataimport INFO - \tMap-Reduce Framework\n07-11-2017 16:02:41 CST run_dataimport INFO - \t\tMap input records=200000\n07-11-2017 16:02:41 CST run_dataimport INFO - \t\tMap output records=200000\n07-11-2017 16:02:41 CST run_dataimport INFO - \t\tInput split bytes=462\n07-11-2017 16:02:41 CST run_dataimport INFO - \t\tSpilled Records=0\n07-11-2017 16:02:41 CST run_dataimport INFO - \t\tFailed Shuffles=0\n07-11-2017 16:02:41 CST run_dataimport INFO - \t\tMerged Map outputs=0\n07-11-2017 16:02:41 CST run_dataimport INFO - \t\tGC time elapsed (ms)=569\n07-11-2017 16:02:41 CST run_dataimport INFO - \t\tCPU time spent (ms)=36080\n07-11-2017 16:02:41 CST run_dataimport INFO - \t\tPhysical memory (bytes) snapshot=1458102272\n07-11-2017 16:02:41 CST run_dataimport INFO - \t\tVirtual memory (bytes) snapshot=6516502528\n07-11-2017 16:02:41 CST run_dataimport INFO - \t\tTotal committed heap usage (bytes)=3231186944\n07-11-2017 16:02:41 CST run_dataimport INFO - \tFile Input Format Counters \n07-11-2017 16:02:41 CST run_dataimport INFO - \t\tBytes Read=0\n07-11-2017 16:02:41 CST run_dataimport INFO - \tFile Output Format Counters \n07-11-2017 16:02:41 CST run_dataimport INFO - \t\tBytes Written=93800000\n07-11-2017 16:02:41 CST run_dataimport INFO - 17/11/07 16:02:41 INFO mapreduce.ImportJobBase: Transferred 89.4547 MB in 29.9824 seconds (2.9836 MB/sec)\n07-11-2017 16:02:41 CST run_dataimport INFO - 17/11/07 16:02:41 INFO mapreduce.ImportJobBase: Retrieved 200000 records.\n07-11-2017 16:02:41 CST run_dataimport INFO - 17/11/07 16:02:41 INFO util.AppendUtils: Appending to directory sql_to_hive\n07-11-2017 16:02:42 CST run_dataimport INFO - 17/11/07 16:02:42 INFO util.AppendUtils: Using found partition 6800\n07-11-2017 16:02:42 CST run_dataimport INFO - 17/11/07 16:02:42 INFO tool.ImportTool: Incremental import complete! To run another incremental import of all data following this import, supply the following arguments:\n07-11-2017 16:02:42 CST run_dataimport INFO - 17/11/07 16:02:42 INFO tool.ImportTool:  --incremental append\n07-11-2017 16:02:42 CST run_dataimport INFO - 17/11/07 16:02:42 INFO tool.ImportTool:   --check-column uid\n07-11-2017 16:02:42 CST run_dataimport INFO - 17/11/07 16:02:42 INFO tool.ImportTool:   --last-value 83422300\n07-11-2017 16:02:42 CST run_dataimport INFO - 17/11/07 16:02:42 INFO tool.ImportTool: (Consider saving this with 'sqoop job --create')\n07-11-2017 16:02:42 CST run_dataimport INFO - 6\n07-11-2017 16:02:42 CST run_dataimport INFO - Warning: /opt/cloudera/parcels/CDH-5.8.0-1.cdh5.8.0.p0.42/bin/../lib/sqoop/../accumulo does not exist! Accumulo imports will fail.\n07-11-2017 16:02:42 CST run_dataimport INFO - Please set $ACCUMULO_HOME to the root of your Accumulo installation.\n07-11-2017 16:02:44 CST run_dataimport INFO - 17/11/07 16:02:44 INFO sqoop.Sqoop: Running Sqoop version: 1.4.6-cdh5.8.0\n07-11-2017 16:02:44 CST run_dataimport INFO - 17/11/07 16:02:44 WARN sqoop.ConnFactory: Parameter --driver is set to an explicit driver however appropriate connection manager is not being set (via --connection-manager). Sqoop is going to fall back to org.apache.sqoop.manager.GenericJdbcManager. Please specify explicitly which connection manager should be used next time.\n07-11-2017 16:02:44 CST run_dataimport INFO - 17/11/07 16:02:44 INFO manager.SqlManager: Using default fetchSize of 1000\n07-11-2017 16:02:44 CST run_dataimport INFO - 17/11/07 16:02:44 INFO tool.CodeGenTool: Beginning code generation\n07-11-2017 16:02:45 CST run_dataimport INFO - 17/11/07 16:02:45 INFO manager.SqlManager: Executing SQL statement: select top 200000 * from jzj.dbo.jzj_test where uid&gt;(select uid from (select top 200000 uid from jzj.dbo.jzj_test where uid&gt;83222300 and uid&lt;=84598500) a except select uid from (select top 199999 uid from jzj.dbo.jzj_test where uid&gt;83222300 and uid&lt;=84598500) a) and uid &lt;=84598500 and  (1 = 0) \n07-11-2017 16:02:45 CST run_dataimport INFO - 17/11/07 16:02:45 INFO manager.SqlManager: Executing SQL statement: select top 200000 * from jzj.dbo.jzj_test where uid&gt;(select uid from (select top 200000 uid from jzj.dbo.jzj_test where uid&gt;83222300 and uid&lt;=84598500) a except select uid from (select top 199999 uid from jzj.dbo.jzj_test where uid&gt;83222300 and uid&lt;=84598500) a) and uid &lt;=84598500 and  (1 = 0) \n07-11-2017 16:02:45 CST run_dataimport INFO - 17/11/07 16:02:45 INFO orm.CompilationManager: HADOOP_MAPRED_HOME is /opt/cloudera/parcels/CDH/lib/hadoop-mapreduce\n07-11-2017 16:02:47 CST run_dataimport INFO - Note: /tmp/sqoop-root/compile/3e3fe91d38feffbabab3df86c4ddb2c4/QueryResult.java uses or overrides a deprecated API.\n07-11-2017 16:02:47 CST run_dataimport INFO - Note: Recompile with -Xlint:deprecation for details.\n07-11-2017 16:02:47 CST run_dataimport INFO - 17/11/07 16:02:47 INFO orm.CompilationManager: Writing jar file: /tmp/sqoop-root/compile/3e3fe91d38feffbabab3df86c4ddb2c4/QueryResult.jar\n07-11-2017 16:02:48 CST run_dataimport INFO - 17/11/07 16:02:48 INFO tool.ImportTool: Maximal id query for free form incremental import: SELECT MAX(uid) FROM (select top 200000 * from jzj.dbo.jzj_test where uid&gt;(select uid from (select top 200000 uid from jzj.dbo.jzj_test where uid&gt;83222300 and uid&lt;=84598500) a except select uid from (select top 199999 uid from jzj.dbo.jzj_test where uid&gt;83222300 and uid&lt;=84598500) a) and uid &lt;=84598500 and (1 = 1)) sqoop_import_query_alias\n07-11-2017 16:42:28 CST run_dataimport INFO - 17/11/07 16:42:28 INFO tool.ImportTool: Incremental import based on column uid\n07-11-2017 16:42:28 CST run_dataimport INFO - 17/11/07 16:42:28 INFO tool.ImportTool: Upper bound value: 83622300\n07-11-2017 16:42:28 CST run_dataimport INFO - 17/11/07 16:42:28 INFO mapreduce.ImportJobBase: Beginning query import.\n07-11-2017 16:42:28 CST run_dataimport INFO - 17/11/07 16:42:28 INFO Configuration.deprecation: mapred.jar is deprecated. Instead, use mapreduce.job.jar\n07-11-2017 16:42:28 CST run_dataimport INFO - 17/11/07 16:42:28 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n07-11-2017 16:42:28 CST run_dataimport INFO - 17/11/07 16:42:28 INFO client.RMProxy: Connecting to ResourceManager at master/172.23.27.203:8032\n07-11-2017 16:42:34 CST run_dataimport INFO - 17/11/07 16:42:34 INFO db.DBInputFormat: Using read commited transaction isolation\n07-11-2017 16:42:34 CST run_dataimport INFO - 17/11/07 16:42:34 INFO db.DataDrivenDBInputFormat: BoundingValsQuery: SELECT MIN(uid), MAX(uid) FROM (select top 200000 * from jzj.dbo.jzj_test where uid&gt;(select uid from (select top 200000 uid from jzj.dbo.jzj_test where uid&gt;83222300 and uid&lt;=84598500) a except select uid from (select top 199999 uid from jzj.dbo.jzj_test where uid&gt;83222300 and uid&lt;=84598500) a) and uid &lt;=84598500 and uid &lt;= 83622300 AND  (1 = 1) ) AS t1\n07-11-2017 17:22:49 CST run_dataimport INFO - 17/11/07 17:22:49 INFO mapreduce.JobSubmitter: number of splits:4\n07-11-2017 17:22:49 CST run_dataimport INFO - 17/11/07 17:22:49 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1508671365448_2955\n07-11-2017 17:22:49 CST run_dataimport INFO - 17/11/07 17:22:49 INFO impl.YarnClientImpl: Submitted application application_1508671365448_2955\n07-11-2017 17:22:49 CST run_dataimport INFO - 17/11/07 17:22:49 INFO mapreduce.Job: The url to track the job: http://master:8088/proxy/application_1508671365448_2955/\n07-11-2017 17:22:49 CST run_dataimport INFO - 17/11/07 17:22:49 INFO mapreduce.Job: Running job: job_1508671365448_2955\n07-11-2017 17:22:57 CST run_dataimport INFO - 17/11/07 17:22:57 INFO mapreduce.Job: Job job_1508671365448_2955 running in uber mode : false\n07-11-2017 17:22:57 CST run_dataimport INFO - 17/11/07 17:22:57 INFO mapreduce.Job:  map 0% reduce 0%\n07-11-2017 18:02:56 CST run_dataimport INFO - 17/11/07 18:02:56 INFO mapreduce.Job:  map 25% reduce 0%\n07-11-2017 18:03:23 CST run_dataimport INFO - 17/11/07 18:03:23 INFO mapreduce.Job:  map 50% reduce 0%\n07-11-2017 18:03:46 CST run_dataimport INFO - 17/11/07 18:03:46 INFO mapreduce.Job:  map 75% reduce 0%\n07-11-2017 18:03:47 CST run_dataimport INFO - 17/11/07 18:03:47 INFO mapreduce.Job:  map 100% reduce 0%\n07-11-2017 18:03:48 CST run_dataimport INFO - 17/11/07 18:03:48 INFO mapreduce.Job: Job job_1508671365448_2955 completed successfully\n07-11-2017 18:03:48 CST run_dataimport INFO - 17/11/07 18:03:48 INFO mapreduce.Job: Counters: 30\n07-11-2017 18:03:48 CST run_dataimport INFO - \tFile System Counters\n07-11-2017 18:03:48 CST run_dataimport INFO - \t\tFILE: Number of bytes read=0\n07-11-2017 18:03:48 CST run_dataimport INFO - \t\tFILE: Number of bytes written=588156\n07-11-2017 18:03:48 CST run_dataimport INFO - \t\tFILE: Number of read operations=0\n07-11-2017 18:03:48 CST run_dataimport INFO - \t\tFILE: Number of large read operations=0\n07-11-2017 18:03:48 CST run_dataimport INFO - \t\tFILE: Number of write operations=0\n07-11-2017 18:03:48 CST run_dataimport INFO - \t\tHDFS: Number of bytes read=462\n07-11-2017 18:03:48 CST run_dataimport INFO - \t\tHDFS: Number of bytes written=93800000\n07-11-2017 18:03:48 CST run_dataimport INFO - \t\tHDFS: Number of read operations=16\n07-11-2017 18:03:48 CST run_dataimport INFO - \t\tHDFS: Number of large read operations=0\n07-11-2017 18:03:48 CST run_dataimport INFO - \t\tHDFS: Number of write operations=8\n07-11-2017 18:03:48 CST run_dataimport INFO - \tJob Counters \n07-11-2017 18:03:48 CST run_dataimport INFO - \t\tLaunched map tasks=4\n07-11-2017 18:03:48 CST run_dataimport INFO - \t\tOther local map tasks=4\n07-11-2017 18:03:48 CST run_dataimport INFO - \t\tTotal time spent by all maps in occupied slots (ms)=9712787\n07-11-2017 18:03:48 CST run_dataimport INFO - \t\tTotal time spent by all reduces in occupied slots (ms)=0\n07-11-2017 18:03:48 CST run_dataimport INFO - \t\tTotal time spent by all map tasks (ms)=9712787\n07-11-2017 18:03:48 CST run_dataimport INFO - \t\tTotal vcore-seconds taken by all map tasks=9712787\n07-11-2017 18:03:48 CST run_dataimport INFO - \t\tTotal megabyte-seconds taken by all map tasks=9945893888\n07-11-2017 18:03:48 CST run_dataimport INFO - \tMap-Reduce Framework\n07-11-2017 18:03:48 CST run_dataimport INFO - \t\tMap input records=200000\n07-11-2017 18:03:48 CST run_dataimport INFO - \t\tMap output records=200000\n07-11-2017 18:03:48 CST run_dataimport INFO - \t\tInput split bytes=462\n07-11-2017 18:03:48 CST run_dataimport INFO - \t\tSpilled Records=0\n07-11-2017 18:03:48 CST run_dataimport INFO - \t\tFailed Shuffles=0\n07-11-2017 18:03:48 CST run_dataimport INFO - \t\tMerged Map outputs=0\n07-11-2017 18:03:48 CST run_dataimport INFO - \t\tGC time elapsed (ms)=764\n07-11-2017 18:03:48 CST run_dataimport INFO - \t\tCPU time spent (ms)=64470\n07-11-2017 18:03:48 CST run_dataimport INFO - \t\tPhysical memory (bytes) snapshot=1473638400\n07-11-2017 18:03:48 CST run_dataimport INFO - \t\tVirtual memory (bytes) snapshot=6498091008\n07-11-2017 18:03:48 CST run_dataimport INFO - \t\tTotal committed heap usage (bytes)=3240099840\n07-11-2017 18:03:48 CST run_dataimport INFO - \tFile Input Format Counters \n07-11-2017 18:03:48 CST run_dataimport INFO - \t\tBytes Read=0\n07-11-2017 18:03:48 CST run_dataimport INFO - \tFile Output Format Counters \n07-11-2017 18:03:48 CST run_dataimport INFO - \t\tBytes Written=93800000\n07-11-2017 18:03:48 CST run_dataimport INFO - 17/11/07 18:03:48 INFO mapreduce.ImportJobBase: Transferred 89.4547 MB in 4,879.8256 seconds (18.7715 KB/sec)\n07-11-2017 18:03:48 CST run_dataimport INFO - 17/11/07 18:03:48 INFO mapreduce.ImportJobBase: Retrieved 200000 records.\n07-11-2017 18:03:48 CST run_dataimport INFO - 17/11/07 18:03:48 INFO util.AppendUtils: Appending to directory sql_to_hive\n07-11-2017 18:03:48 CST run_dataimport INFO - 17/11/07 18:03:48 INFO util.AppendUtils: Using found partition 6804\n07-11-2017 18:03:49 CST run_dataimport INFO - 17/11/07 18:03:49 INFO tool.ImportTool: Incremental import complete! To run another incremental import of all data following this import, supply the following arguments:\n07-11-2017 18:03:49 CST run_dataimport INFO - 17/11/07 18:03:49 INFO tool.ImportTool:  --incremental append\n07-11-2017 18:03:49 CST run_dataimport INFO - 17/11/07 18:03:49 INFO tool.ImportTool:   --check-column uid\n07-11-2017 18:03:49 CST run_dataimport INFO - 17/11/07 18:03:49 INFO tool.ImportTool:   --last-value 83622300\n07-11-2017 18:03:49 CST run_dataimport INFO - 17/11/07 18:03:49 INFO tool.ImportTool: (Consider saving this with 'sqoop job --create')\n07-11-2017 18:03:49 CST run_dataimport INFO - 5\n07-11-2017 18:03:49 CST run_dataimport INFO - Warning: /opt/cloudera/parcels/CDH-5.8.0-1.cdh5.8.0.p0.42/bin/../lib/sqoop/../accumulo does not exist! Accumulo imports will fail.\n07-11-2017 18:03:49 CST run_dataimport INFO - Please set $ACCUMULO_HOME to the root of your Accumulo installation.\n07-11-2017 18:03:51 CST run_dataimport INFO - 17/11/07 18:03:51 INFO sqoop.Sqoop: Running Sqoop version: 1.4.6-cdh5.8.0\n07-11-2017 18:03:51 CST run_dataimport INFO - 17/11/07 18:03:51 WARN sqoop.ConnFactory: Parameter --driver is set to an explicit driver however appropriate connection manager is not being set (via --connection-manager). Sqoop is going to fall back to org.apache.sqoop.manager.GenericJdbcManager. Please specify explicitly which connection manager should be used next time.\n07-11-2017 18:03:51 CST run_dataimport INFO - 17/11/07 18:03:51 INFO manager.SqlManager: Using default fetchSize of 1000\n07-11-2017 18:03:51 CST run_dataimport INFO - 17/11/07 18:03:51 INFO tool.CodeGenTool: Beginning code generation\n07-11-2017 18:03:52 CST run_dataimport INFO - 17/11/07 18:03:52 INFO manager.SqlManager: Executing SQL statement: select top 200000 * from jzj.dbo.jzj_test where uid&gt;(select uid from (select top 400000 uid from jzj.dbo.jzj_test where uid&gt;83222300 and uid&lt;=84598500) a except select uid from (select top 399999 uid from jzj.dbo.jzj_test where uid&gt;83222300 and uid&lt;=84598500) a) and uid &lt;=84598500 and  (1 = 0) \n07-11-2017 18:03:52 CST run_dataimport INFO - 17/11/07 18:03:52 INFO manager.SqlManager: Executing SQL statement: select top 200000 * from jzj.dbo.jzj_test where uid&gt;(select uid from (select top 400000 uid from jzj.dbo.jzj_test where uid&gt;83222300 and uid&lt;=84598500) a except select uid from (select top 399999 uid from jzj.dbo.jzj_test where uid&gt;83222300 and uid&lt;=84598500) a) and uid &lt;=84598500 and  (1 = 0) \n07-11-2017 18:03:52 CST run_dataimport INFO - 17/11/07 18:03:52 INFO orm.CompilationManager: HADOOP_MAPRED_HOME is /opt/cloudera/parcels/CDH/lib/hadoop-mapreduce\n07-11-2017 18:03:54 CST run_dataimport INFO - Note: /tmp/sqoop-root/compile/21f23a42adb9bf71452da9a3c7187b06/QueryResult.java uses or overrides a deprecated API.\n07-11-2017 18:03:54 CST run_dataimport INFO - Note: Recompile with -Xlint:deprecation for details.\n07-11-2017 18:03:54 CST run_dataimport INFO - 17/11/07 18:03:54 INFO orm.CompilationManager: Writing jar file: /tmp/sqoop-root/compile/21f23a42adb9bf71452da9a3c7187b06/QueryResult.jar\n07-11-2017 18:03:55 CST run_dataimport INFO - 17/11/07 18:03:55 INFO tool.ImportTool: Maximal id query for free form incremental import: SELECT MAX(uid) FROM (select top 200000 * from jzj.dbo.jzj_test where uid&gt;(select uid from (select top 400000 uid from jzj.dbo.jzj_test where uid&gt;83222300 and uid&lt;=84598500) a except select uid from (select top 399999 uid from jzj.dbo.jzj_test where uid&gt;83222300 and uid&lt;=84598500) a) and uid &lt;=84598500 and (1 = 1)) sqoop_import_query_alias\n07-11-2017 20:43:27 CST run_dataimport INFO - 17/11/07 20:43:27 INFO tool.ImportTool: Incremental import based on column uid\n07-11-2017 20:43:27 CST run_dataimport INFO - 17/11/07 20:43:27 INFO tool.ImportTool: Upper bound value: 83822300\n07-11-2017 20:43:27 CST run_dataimport INFO - 17/11/07 20:43:27 INFO mapreduce.ImportJobBase: Beginning query import.\n07-11-2017 20:43:27 CST run_dataimport INFO - 17/11/07 20:43:27 INFO Configuration.deprecation: mapred.jar is deprecated. Instead, use mapreduce.job.jar\n07-11-2017 20:43:27 CST run_dataimport INFO - 17/11/07 20:43:27 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n07-11-2017 20:43:28 CST run_dataimport INFO - 17/11/07 20:43:28 INFO client.RMProxy: Connecting to ResourceManager at master/172.23.27.203:8032\n07-11-2017 20:43:34 CST run_dataimport INFO - 17/11/07 20:43:34 INFO db.DBInputFormat: Using read commited transaction isolation\n07-11-2017 20:43:34 CST run_dataimport INFO - 17/11/07 20:43:34 INFO db.DataDrivenDBInputFormat: BoundingValsQuery: SELECT MIN(uid), MAX(uid) FROM (select top 200000 * from jzj.dbo.jzj_test where uid&gt;(select uid from (select top 400000 uid from jzj.dbo.jzj_test where uid&gt;83222300 and uid&lt;=84598500) a except select uid from (select top 399999 uid from jzj.dbo.jzj_test where uid&gt;83222300 and uid&lt;=84598500) a) and uid &lt;=84598500 and uid &lt;= 83822300 AND  (1 = 1) ) AS t1\n07-11-2017 23:22:41 CST run_dataimport INFO - 17/11/07 23:22:41 INFO mapreduce.JobSubmitter: number of splits:4\n07-11-2017 23:22:41 CST run_dataimport INFO - 17/11/07 23:22:41 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1508671365448_2956\n07-11-2017 23:22:42 CST run_dataimport INFO - 17/11/07 23:22:42 INFO impl.YarnClientImpl: Submitted application application_1508671365448_2956\n07-11-2017 23:22:42 CST run_dataimport INFO - 17/11/07 23:22:42 INFO mapreduce.Job: The url to track the job: http://master:8088/proxy/application_1508671365448_2956/\n07-11-2017 23:22:42 CST run_dataimport INFO - 17/11/07 23:22:42 INFO mapreduce.Job: Running job: job_1508671365448_2956\n07-11-2017 23:22:49 CST run_dataimport INFO - 17/11/07 23:22:49 INFO mapreduce.Job: Job job_1508671365448_2956 running in uber mode : false\n07-11-2017 23:22:49 CST run_dataimport INFO - 17/11/07 23:22:49 INFO mapreduce.Job:  map 0% reduce 0%\n08-11-2017 02:00:13 CST run_dataimport INFO - 17/11/08 02:00:13 INFO mapreduce.Job:  map 25% reduce 0%\n08-11-2017 02:01:28 CST run_dataimport INFO - 17/11/08 02:01:28 INFO mapreduce.Job:  map 50% reduce 0%\n08-11-2017 02:03:21 CST run_dataimport INFO - 17/11/08 02:03:21 INFO mapreduce.Job:  map 75% reduce 0%\n08-11-2017 02:03:22 CST run_dataimport INFO - 17/11/08 02:03:22 INFO mapreduce.Job:  map 100% reduce 0%\n08-11-2017 02:03:23 CST run_dataimport INFO - 17/11/08 02:03:23 INFO mapreduce.Job: Job job_1508671365448_2956 completed successfully\n08-11-2017 02:03:23 CST run_dataimport INFO - 17/11/08 02:03:23 INFO mapreduce.Job: Counters: 30\n08-11-2017 02:03:23 CST run_dataimport INFO - \tFile System Counters\n08-11-2017 02:03:23 CST run_dataimport INFO - \t\tFILE: Number of bytes read=0\n08-11-2017 02:03:23 CST run_dataimport INFO - \t\tFILE: Number of bytes written=588300\n08-11-2017 02:03:23 CST run_dataimport INFO - \t\tFILE: Number of read operations=0\n08-11-2017 02:03:23 CST run_dataimport INFO - \t\tFILE: Number of large read operations=0\n08-11-2017 02:03:23 CST run_dataimport INFO - \t\tFILE: Number of write operations=0\n08-11-2017 02:03:23 CST run_dataimport INFO - \t\tHDFS: Number of bytes read=462\n08-11-2017 02:03:23 CST run_dataimport INFO - \t\tHDFS: Number of bytes written=93800000\n08-11-2017 02:03:23 CST run_dataimport INFO - \t\tHDFS: Number of read operations=16\n08-11-2017 02:03:23 CST run_dataimport INFO - \t\tHDFS: Number of large read operations=0\n08-11-2017 02:03:23 CST run_dataimport INFO - \t\tHDFS: Number of write operations=8\n08-11-2017 02:03:23 CST run_dataimport INFO - \tJob Counters \n08-11-2017 02:03:23 CST run_dataimport INFO - \t\tLaunched map tasks=4\n08-11-2017 02:03:23 CST run_dataimport INFO - \t\tOther local map tasks=4\n08-11-2017 02:03:23 CST run_dataimport INFO - \t\tTotal time spent by all maps in occupied slots (ms)=38216711\n08-11-2017 02:03:23 CST run_dataimport INFO - \t\tTotal time spent by all reduces in occupied slots (ms)=0\n08-11-2017 02:03:23 CST run_dataimport INFO - \t\tTotal time spent by all map tasks (ms)=38216711\n08-11-2017 02:03:23 CST run_dataimport INFO - \t\tTotal vcore-seconds taken by all map tasks=38216711\n08-11-2017 02:03:23 CST run_dataimport INFO - \t\tTotal megabyte-seconds taken by all map tasks=39133912064\n08-11-2017 02:03:23 CST run_dataimport INFO - \tMap-Reduce Framework\n08-11-2017 02:03:23 CST run_dataimport INFO - \t\tMap input records=200000\n08-11-2017 02:03:23 CST run_dataimport INFO - \t\tMap output records=200000\n08-11-2017 02:03:23 CST run_dataimport INFO - \t\tInput split bytes=462\n08-11-2017 02:03:23 CST run_dataimport INFO - \t\tSpilled Records=0\n08-11-2017 02:03:23 CST run_dataimport INFO - \t\tFailed Shuffles=0\n08-11-2017 02:03:23 CST run_dataimport INFO - \t\tMerged Map outputs=0\n08-11-2017 02:03:23 CST run_dataimport INFO - \t\tGC time elapsed (ms)=1588\n08-11-2017 02:03:23 CST run_dataimport INFO - \t\tCPU time spent (ms)=134830\n08-11-2017 02:03:23 CST run_dataimport INFO - \t\tPhysical memory (bytes) snapshot=1547542528\n08-11-2017 02:03:23 CST run_dataimport INFO - \t\tVirtual memory (bytes) snapshot=6530818048\n08-11-2017 02:03:23 CST run_dataimport INFO - \t\tTotal committed heap usage (bytes)=3172990976\n08-11-2017 02:03:23 CST run_dataimport INFO - \tFile Input Format Counters \n08-11-2017 02:03:23 CST run_dataimport INFO - \t\tBytes Read=0\n08-11-2017 02:03:23 CST run_dataimport INFO - \tFile Output Format Counters \n08-11-2017 02:03:23 CST run_dataimport INFO - \t\tBytes Written=93800000\n08-11-2017 02:03:23 CST run_dataimport INFO - 17/11/08 02:03:23 INFO mapreduce.ImportJobBase: Transferred 89.4547 MB in 19,195.7726 seconds (4.772 KB/sec)\n08-11-2017 02:03:23 CST run_dataimport INFO - 17/11/08 02:03:23 INFO mapreduce.ImportJobBase: Retrieved 200000 records.\n08-11-2017 02:03:23 CST run_dataimport INFO - 17/11/08 02:03:23 INFO util.AppendUtils: Appending to directory sql_to_hive\n08-11-2017 02:03:24 CST run_dataimport INFO - 17/11/08 02:03:24 INFO util.AppendUtils: Using found partition 6808\n08-11-2017 02:03:24 CST run_dataimport INFO - 17/11/08 02:03:24 INFO tool.ImportTool: Incremental import complete! To run another incremental import of all data following this import, supply the following arguments:\n08-11-2017 02:03:24 CST run_dataimport INFO - 17/11/08 02:03:24 INFO tool.ImportTool:  --incremental append\n08-11-2017 02:03:24 CST run_dataimport INFO - 17/11/08 02:03:24 INFO tool.ImportTool:   --check-column uid\n08-11-2017 02:03:24 CST run_dataimport INFO - 17/11/08 02:03:24 INFO tool.ImportTool:   --last-value 83822300\n08-11-2017 02:03:24 CST run_dataimport INFO - 17/11/08 02:03:24 INFO tool.ImportTool: (Consider saving this with 'sqoop job --create')\n08-11-2017 02:03:24 CST run_dataimport INFO - 4\n08-11-2017 02:03:24 CST run_dataimport INFO - Warning: /opt/cloudera/parcels/CDH-5.8.0-1.cdh5.8.0.p0.42/bin/../lib/sqoop/../accumulo does not exist! Accumulo imports will fail.\n08-11-2017 02:03:24 CST run_dataimport INFO - Please set $ACCUMULO_HOME to the root of your Accumulo installation.\n08-11-2017 02:03:26 CST run_dataimport INFO - 17/11/08 02:03:26 INFO sqoop.Sqoop: Running Sqoop version: 1.4.6-cdh5.8.0\n08-11-2017 02:03:26 CST run_dataimport INFO - 17/11/08 02:03:26 WARN sqoop.ConnFactory: Parameter --driver is set to an explicit driver however appropriate connection manager is not being set (via --connection-manager). Sqoop is going to fall back to org.apache.sqoop.manager.GenericJdbcManager. Please specify explicitly which connection manager should be used next time.\n08-11-2017 02:03:26 CST run_dataimport INFO - 17/11/08 02:03:26 INFO manager.SqlManager: Using default fetchSize of 1000\n08-11-2017 02:03:26 CST run_dataimport INFO - 17/11/08 02:03:26 INFO tool.CodeGenTool: Beginning code generation\n08-11-2017 02:03:27 CST run_dataimport INFO - 17/11/08 02:03:27 INFO manager.SqlManager: Executing SQL statement: select top 200000 * from jzj.dbo.jzj_test where uid&gt;(select uid from (select top 600000 uid from jzj.dbo.jzj_test where uid&gt;83222300 and uid&lt;=84598500) a except select uid from (select top 599999 uid from jzj.dbo.jzj_test where uid&gt;83222300 and uid&lt;=84598500) a) and uid &lt;=84598500 and  (1 = 0) \n08-11-2017 02:03:27 CST run_dataimport INFO - 17/11/08 02:03:27 INFO manager.SqlManager: Executing SQL statement: select top 200000 * from jzj.dbo.jzj_test where uid&gt;(select uid from (select top 600000 uid from jzj.dbo.jzj_test where uid&gt;83222300 and uid&lt;=84598500) a except select uid from (select top 599999 uid from jzj.dbo.jzj_test where uid&gt;83222300 and uid&lt;=84598500) a) and uid &lt;=84598500 and  (1 = 0) \n08-11-2017 02:03:27 CST run_dataimport INFO - 17/11/08 02:03:27 INFO orm.CompilationManager: HADOOP_MAPRED_HOME is /opt/cloudera/parcels/CDH/lib/hadoop-mapreduce\n08-11-2017 02:03:29 CST run_dataimport INFO - Note: /tmp/sqoop-root/compile/e8acb4e9422617ba257a228b6dfa0561/QueryResult.java uses or overrides a deprecated API.\n08-11-2017 02:03:29 CST run_dataimport INFO - Note: Recompile with -Xlint:deprecation for details.\n08-11-2017 02:03:29 CST run_dataimport INFO - 17/11/08 02:03:29 INFO orm.CompilationManager: Writing jar file: /tmp/sqoop-root/compile/e8acb4e9422617ba257a228b6dfa0561/QueryResult.jar\n08-11-2017 02:03:30 CST run_dataimport INFO - 17/11/08 02:03:30 INFO tool.ImportTool: Maximal id query for free form incremental import: SELECT MAX(uid) FROM (select top 200000 * from jzj.dbo.jzj_test where uid&gt;(select uid from (select top 600000 uid from jzj.dbo.jzj_test where uid&gt;83222300 and uid&lt;=84598500) a except select uid from (select top 599999 uid from jzj.dbo.jzj_test where uid&gt;83222300 and uid&lt;=84598500) a) and uid &lt;=84598500 and (1 = 1)) sqoop_import_query_alias\n08-11-2017 07:55:53 CST run_dataimport INFO - 17/11/08 07:55:53 INFO tool.ImportTool: Incremental import based on column uid\n08-11-2017 07:55:53 CST run_dataimport INFO - 17/11/08 07:55:53 INFO tool.ImportTool: Upper bound value: 84022300\n08-11-2017 07:55:53 CST run_dataimport INFO - 17/11/08 07:55:53 INFO mapreduce.ImportJobBase: Beginning query import.\n08-11-2017 07:55:53 CST run_dataimport INFO - 17/11/08 07:55:53 INFO Configuration.deprecation: mapred.jar is deprecated. Instead, use mapreduce.job.jar\n08-11-2017 07:55:53 CST run_dataimport INFO - 17/11/08 07:55:53 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n08-11-2017 07:55:53 CST run_dataimport INFO - 17/11/08 07:55:53 INFO client.RMProxy: Connecting to ResourceManager at master/172.23.27.203:8032\n08-11-2017 07:56:00 CST run_dataimport INFO - 17/11/08 07:56:00 INFO db.DBInputFormat: Using read commited transaction isolation\n08-11-2017 07:56:00 CST run_dataimport INFO - 17/11/08 07:56:00 INFO db.DataDrivenDBInputFormat: BoundingValsQuery: SELECT MIN(uid), MAX(uid) FROM (select top 200000 * from jzj.dbo.jzj_test where uid&gt;(select uid from (select top 600000 uid from jzj.dbo.jzj_test where uid&gt;83222300 and uid&lt;=84598500) a except select uid from (select top 599999 uid from jzj.dbo.jzj_test where uid&gt;83222300 and uid&lt;=84598500) a) and uid &lt;=84598500 and uid &lt;= 84022300 AND  (1 = 1) ) AS t1\n08-11-2017 12:11:01 CST run_dataimport ERROR - Kill has been called.\n08-11-2017 12:11:06 CST run_dataimport INFO - Process completed unsuccessfully in 72615 seconds.\n08-11-2017 12:11:06 CST run_dataimport ERROR - Job run killed!\njava.lang.RuntimeException: azkaban.jobExecutor.utils.process.ProcessFailureException\n\tat azkaban.jobExecutor.ProcessJob.run(ProcessJob.java:297)\n\tat azkaban.execapp.JobRunner.runJob(JobRunner.java:748)\n\tat azkaban.execapp.JobRunner.doRun(JobRunner.java:591)\n\tat azkaban.execapp.JobRunner.run(JobRunner.java:552)\n\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: azkaban.jobExecutor.utils.process.ProcessFailureException\n\tat azkaban.jobExecutor.utils.process.AzkabanProcess.run(AzkabanProcess.java:130)\n\tat azkaban.jobExecutor.ProcessJob.run(ProcessJob.java:289)\n\t... 8 more\n08-11-2017 12:11:06 CST run_dataimport ERROR - azkaban.jobExecutor.utils.process.ProcessFailureException cause: azkaban.jobExecutor.utils.process.ProcessFailureException\n08-11-2017 12:11:06 CST run_dataimport INFO - Finishing job run_dataimport at 1510114266951 with status KILLED\n", 'length': 37422}
[2017-11-20 20:42:12,787] app.hive_mysql.AzkabanMonitor [INFO] URL: https://172.23.27.203:8443
[2017-11-20 20:42:12,999] app.hive_mysql.AzkabanMonitor [INFO] Success: Login https://172.23.27.203:8443 with user azkaban
[2017-11-20 20:42:12,999] app.hive_mysql.AzkabanMonitor [DEBUG] {'session.id': '369624cf-4647-4a68-8390-66435e0a27ad', 'status': 'success'}
[2017-11-20 20:42:13,014] app.hive_mysql.AzkabanMonitor [INFO] Success: Fetch Executions of a Flow - run_dataimport
[2017-11-20 20:42:13,015] app.hive_mysql.AzkabanMonitor [DEBUG] {'total': 16, 'executions': [{'submitTime': 1510041651107, 'submitUser': 'azkaban', 'startTime': 1510041651277, 'endTime': 1510114267057, 'flowId': 'run_dataimport', 'projectId': 68, 'execId': 1670, 'status': 'KILLED'}, {'submitTime': 1510027251078, 'submitUser': 'azkaban', 'startTime': 1510027251395, 'endTime': 1510036906931, 'flowId': 'run_dataimport', 'projectId': 68, 'execId': 1669, 'status': 'SUCCEEDED'}, {'submitTime': 1509897650826, 'submitUser': 'azkaban', 'startTime': 1509897651116, 'endTime': 1510026870568, 'flowId': 'run_dataimport', 'projectId': 68, 'execId': 1668, 'status': 'SUCCEEDED'}, {'submitTime': 1509883250800, 'submitUser': 'azkaban', 'startTime': 1509883251056, 'endTime': 1509892870422, 'flowId': 'run_dataimport', 'projectId': 68, 'execId': 1667, 'status': 'SUCCEEDED'}, {'submitTime': 1509753650545, 'submitUser': 'azkaban', 'startTime': 1509753650705, 'endTime': 1509881095432, 'flowId': 'run_dataimport', 'projectId': 68, 'execId': 1666, 'status': 'SUCCEEDED'}, {'submitTime': 1509746450529, 'submitUser': 'azkaban', 'startTime': 1509746450965, 'endTime': 1509753076918, 'flowId': 'run_dataimport', 'projectId': 68, 'execId': 1665, 'status': 'SUCCEEDED'}, {'submitTime': 1509652850362, 'submitUser': 'azkaban', 'startTime': 1509652850560, 'endTime': 1509744409191, 'flowId': 'run_dataimport', 'projectId': 68, 'execId': 1664, 'status': 'SUCCEEDED'}, {'submitTime': 1509645650349, 'submitUser': 'azkaban', 'startTime': 1509645650498, 'endTime': 1509651159750, 'flowId': 'run_dataimport', 'projectId': 68, 'execId': 1663, 'status': 'SUCCEEDED'}, {'submitTime': 1509566450178, 'submitUser': 'azkaban', 'startTime': 1509566450488, 'endTime': 1509642929867, 'flowId': 'run_dataimport', 'projectId': 68, 'execId': 1661, 'status': 'SUCCEEDED'}, {'submitTime': 1509559250163, 'submitUser': 'azkaban', 'startTime': 1509559250321, 'endTime': 1509563186239, 'flowId': 'run_dataimport', 'projectId': 68, 'execId': 1660, 'status': 'SUCCEEDED'}], 'length': 10, 'project': 'sql_to_hive', 'from': 0, 'projectId': 68, 'flow': 'run_dataimport'}
[2017-11-20 20:42:13,032] app.hive_mysql.AzkabanMonitor [INFO] Success: Fetch Execution Job Logs - 1670-run_dataimport
[2017-11-20 20:42:13,032] app.hive_mysql.AzkabanMonitor [DEBUG] {'offset': 0, 'data': "07-11-2017 16:00:51 CST run_dataimport INFO - Starting job run_dataimport at 1510041651281\n07-11-2017 16:00:51 CST run_dataimport INFO - azkaban.webserver.url property was not set\n07-11-2017 16:00:51 CST run_dataimport INFO - job JVM args: -Dazkaban.flowid=run_dataimport -Dazkaban.execid=1670 -Dazkaban.jobid=run_dataimport\n07-11-2017 16:00:51 CST run_dataimport INFO - Building command job executor. \n07-11-2017 16:00:51 CST run_dataimport INFO - Memory granted for job run_dataimport\n07-11-2017 16:00:51 CST run_dataimport INFO - 1 commands to execute.\n07-11-2017 16:00:51 CST run_dataimport INFO - cwd=/home/azkaban/azkaban_bk/azkaban-exec-server-3.35.0-22-g5517d50/executions/1670\n07-11-2017 16:00:51 CST run_dataimport INFO - effective user is: azkaban\n07-11-2017 16:00:51 CST run_dataimport INFO - Command: sh /home/sql_to_hive/dataimport.sh\n07-11-2017 16:00:51 CST run_dataimport INFO - Environment variables: {JOB_OUTPUT_PROP_FILE=/home/azkaban/azkaban_bk/azkaban-exec-server-3.35.0-22-g5517d50/executions/1670/run_dataimport_output_8807970133616151260_tmp, JOB_PROP_FILE=/home/azkaban/azkaban_bk/azkaban-exec-server-3.35.0-22-g5517d50/executions/1670/run_dataimport_props_6140684690151138768_tmp, KRB5CCNAME=/tmp/krb5cc__sql_to_hive__run_dataimport__run_dataimport__1670__azkaban, JOB_NAME=run_dataimport}\n07-11-2017 16:00:51 CST run_dataimport INFO - Working directory: /home/azkaban/azkaban_bk/azkaban-exec-server-3.35.0-22-g5517d50/executions/1670\n07-11-2017 16:00:51 CST run_dataimport INFO - log4j:WARN No appenders could be found for logger (org.apache.hive.jdbc.Utils).\n07-11-2017 16:00:51 CST run_dataimport INFO - log4j:WARN Please initialize the log4j system properly.\n07-11-2017 16:00:51 CST run_dataimport INFO - log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.\n07-11-2017 16:00:51 CST run_dataimport INFO - SLF4J: Failed to load class &quot;org.slf4j.impl.StaticLoggerBinder&quot;.\n07-11-2017 16:00:51 CST run_dataimport INFO - SLF4J: Defaulting to no-operation (NOP) logger implementation\n07-11-2017 16:00:51 CST run_dataimport INFO - SLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.\n07-11-2017 16:02:05 CST run_dataimport INFO - 1,84598500,83222300,1376200sqlmax:84598500\n07-11-2017 16:02:05 CST run_dataimport INFO - hivemax:83222300\n07-11-2017 16:02:05 CST run_dataimport INFO - counts:1376200\n07-11-2017 16:02:05 CST run_dataimport INFO - 7,176200\n07-11-2017 16:02:05 CST run_dataimport INFO - 7\n07-11-2017 16:02:05 CST run_dataimport INFO - Warning: /opt/cloudera/parcels/CDH-5.8.0-1.cdh5.8.0.p0.42/bin/../lib/sqoop/../accumulo does not exist! Accumulo imports will fail.\n07-11-2017 16:02:05 CST run_dataimport INFO - Please set $ACCUMULO_HOME to the root of your Accumulo installation.\n07-11-2017 16:02:07 CST run_dataimport INFO - 17/11/07 16:02:07 INFO sqoop.Sqoop: Running Sqoop version: 1.4.6-cdh5.8.0\n07-11-2017 16:02:07 CST run_dataimport INFO - 17/11/07 16:02:07 WARN sqoop.ConnFactory: Parameter --driver is set to an explicit driver however appropriate connection manager is not being set (via --connection-manager). Sqoop is going to fall back to org.apache.sqoop.manager.GenericJdbcManager. Please specify explicitly which connection manager should be used next time.\n07-11-2017 16:02:07 CST run_dataimport INFO - 17/11/07 16:02:07 INFO manager.SqlManager: Using default fetchSize of 1000\n07-11-2017 16:02:07 CST run_dataimport INFO - 17/11/07 16:02:07 INFO tool.CodeGenTool: Beginning code generation\n07-11-2017 16:02:08 CST run_dataimport INFO - 17/11/07 16:02:08 INFO manager.SqlManager: Executing SQL statement: select top 200000 * from jzj.dbo.jzj_test where uid&gt;=(select uid from (select top 1 uid from jzj.dbo.jzj_test where uid&gt;83222300 and uid&lt;=84598500) a) and uid &lt;=84598500 and  (1 = 0) \n07-11-2017 16:02:08 CST run_dataimport INFO - 17/11/07 16:02:08 INFO manager.SqlManager: Executing SQL statement: select top 200000 * from jzj.dbo.jzj_test where uid&gt;=(select uid from (select top 1 uid from jzj.dbo.jzj_test where uid&gt;83222300 and uid&lt;=84598500) a) and uid &lt;=84598500 and  (1 = 0) \n07-11-2017 16:02:08 CST run_dataimport INFO - 17/11/07 16:02:08 INFO orm.CompilationManager: HADOOP_MAPRED_HOME is /opt/cloudera/parcels/CDH/lib/hadoop-mapreduce\n07-11-2017 16:02:10 CST run_dataimport INFO - Note: /tmp/sqoop-root/compile/b796f2c0b0198c05bdcf8149b3f9a7fd/QueryResult.java uses or overrides a deprecated API.\n07-11-2017 16:02:10 CST run_dataimport INFO - Note: Recompile with -Xlint:deprecation for details.\n07-11-2017 16:02:10 CST run_dataimport INFO - 17/11/07 16:02:10 INFO orm.CompilationManager: Writing jar file: /tmp/sqoop-root/compile/b796f2c0b0198c05bdcf8149b3f9a7fd/QueryResult.jar\n07-11-2017 16:02:11 CST run_dataimport INFO - 17/11/07 16:02:11 INFO tool.ImportTool: Maximal id query for free form incremental import: SELECT MAX(uid) FROM (select top 200000 * from jzj.dbo.jzj_test where uid&gt;=(select uid from (select top 1 uid from jzj.dbo.jzj_test where uid&gt;83222300 and uid&lt;=84598500) a) and uid &lt;=84598500 and (1 = 1)) sqoop_import_query_alias\n07-11-2017 16:02:11 CST run_dataimport INFO - 17/11/07 16:02:11 INFO tool.ImportTool: Incremental import based on column uid\n07-11-2017 16:02:11 CST run_dataimport INFO - 17/11/07 16:02:11 INFO tool.ImportTool: Upper bound value: 83422300\n07-11-2017 16:02:11 CST run_dataimport INFO - 17/11/07 16:02:11 INFO mapreduce.ImportJobBase: Beginning query import.\n07-11-2017 16:02:11 CST run_dataimport INFO - 17/11/07 16:02:11 INFO Configuration.deprecation: mapred.jar is deprecated. Instead, use mapreduce.job.jar\n07-11-2017 16:02:11 CST run_dataimport INFO - 17/11/07 16:02:11 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n07-11-2017 16:02:11 CST run_dataimport INFO - 17/11/07 16:02:11 INFO client.RMProxy: Connecting to ResourceManager at master/172.23.27.203:8032\n07-11-2017 16:02:18 CST run_dataimport INFO - 17/11/07 16:02:18 INFO db.DBInputFormat: Using read commited transaction isolation\n07-11-2017 16:02:18 CST run_dataimport INFO - 17/11/07 16:02:18 INFO db.DataDrivenDBInputFormat: BoundingValsQuery: SELECT MIN(uid), MAX(uid) FROM (select top 200000 * from jzj.dbo.jzj_test where uid&gt;=(select uid from (select top 1 uid from jzj.dbo.jzj_test where uid&gt;83222300 and uid&lt;=84598500) a) and uid &lt;=84598500 and uid &lt;= 83422300 AND  (1 = 1) ) AS t1\n07-11-2017 16:02:18 CST run_dataimport INFO - 17/11/07 16:02:18 INFO mapreduce.JobSubmitter: number of splits:4\n07-11-2017 16:02:18 CST run_dataimport INFO - 17/11/07 16:02:18 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1508671365448_2954\n07-11-2017 16:02:19 CST run_dataimport INFO - 17/11/07 16:02:19 INFO impl.YarnClientImpl: Submitted application application_1508671365448_2954\n07-11-2017 16:02:19 CST run_dataimport INFO - 17/11/07 16:02:19 INFO mapreduce.Job: The url to track the job: http://master:8088/proxy/application_1508671365448_2954/\n07-11-2017 16:02:19 CST run_dataimport INFO - 17/11/07 16:02:19 INFO mapreduce.Job: Running job: job_1508671365448_2954\n07-11-2017 16:02:26 CST run_dataimport INFO - 17/11/07 16:02:26 INFO mapreduce.Job: Job job_1508671365448_2954 running in uber mode : false\n07-11-2017 16:02:26 CST run_dataimport INFO - 17/11/07 16:02:26 INFO mapreduce.Job:  map 0% reduce 0%\n07-11-2017 16:02:37 CST run_dataimport INFO - 17/11/07 16:02:37 INFO mapreduce.Job:  map 25% reduce 0%\n07-11-2017 16:02:40 CST run_dataimport INFO - 17/11/07 16:02:40 INFO mapreduce.Job:  map 100% reduce 0%\n07-11-2017 16:02:41 CST run_dataimport INFO - 17/11/07 16:02:41 INFO mapreduce.Job: Job job_1508671365448_2954 completed successfully\n07-11-2017 16:02:41 CST run_dataimport INFO - 17/11/07 16:02:41 INFO mapreduce.Job: Counters: 30\n07-11-2017 16:02:41 CST run_dataimport INFO - \tFile System Counters\n07-11-2017 16:02:41 CST run_dataimport INFO - \t\tFILE: Number of bytes read=0\n07-11-2017 16:02:41 CST run_dataimport INFO - \t\tFILE: Number of bytes written=587024\n07-11-2017 16:02:41 CST run_dataimport INFO - \t\tFILE: Number of read operations=0\n07-11-2017 16:02:41 CST run_dataimport INFO - \t\tFILE: Number of large read operations=0\n07-11-2017 16:02:41 CST run_dataimport INFO - \t\tFILE: Number of write operations=0\n07-11-2017 16:02:41 CST run_dataimport INFO - \t\tHDFS: Number of bytes read=462\n07-11-2017 16:02:41 CST run_dataimport INFO - \t\tHDFS: Number of bytes written=93800000\n07-11-2017 16:02:41 CST run_dataimport INFO - \t\tHDFS: Number of read operations=16\n07-11-2017 16:02:41 CST run_dataimport INFO - \t\tHDFS: Number of large read operations=0\n07-11-2017 16:02:41 CST run_dataimport INFO - \t\tHDFS: Number of write operations=8\n07-11-2017 16:02:41 CST run_dataimport INFO - \tJob Counters \n07-11-2017 16:02:41 CST run_dataimport INFO - \t\tLaunched map tasks=4\n07-11-2017 16:02:41 CST run_dataimport INFO - \t\tOther local map tasks=4\n07-11-2017 16:02:41 CST run_dataimport INFO - \t\tTotal time spent by all maps in occupied slots (ms)=44342\n07-11-2017 16:02:41 CST run_dataimport INFO - \t\tTotal time spent by all reduces in occupied slots (ms)=0\n07-11-2017 16:02:41 CST run_dataimport INFO - \t\tTotal time spent by all map tasks (ms)=44342\n07-11-2017 16:02:41 CST run_dataimport INFO - \t\tTotal vcore-seconds taken by all map tasks=44342\n07-11-2017 16:02:41 CST run_dataimport INFO - \t\tTotal megabyte-seconds taken by all map tasks=45406208\n07-11-2017 16:02:41 CST run_dataimport INFO - \tMap-Reduce Framework\n07-11-2017 16:02:41 CST run_dataimport INFO - \t\tMap input records=200000\n07-11-2017 16:02:41 CST run_dataimport INFO - \t\tMap output records=200000\n07-11-2017 16:02:41 CST run_dataimport INFO - \t\tInput split bytes=462\n07-11-2017 16:02:41 CST run_dataimport INFO - \t\tSpilled Records=0\n07-11-2017 16:02:41 CST run_dataimport INFO - \t\tFailed Shuffles=0\n07-11-2017 16:02:41 CST run_dataimport INFO - \t\tMerged Map outputs=0\n07-11-2017 16:02:41 CST run_dataimport INFO - \t\tGC time elapsed (ms)=569\n07-11-2017 16:02:41 CST run_dataimport INFO - \t\tCPU time spent (ms)=36080\n07-11-2017 16:02:41 CST run_dataimport INFO - \t\tPhysical memory (bytes) snapshot=1458102272\n07-11-2017 16:02:41 CST run_dataimport INFO - \t\tVirtual memory (bytes) snapshot=6516502528\n07-11-2017 16:02:41 CST run_dataimport INFO - \t\tTotal committed heap usage (bytes)=3231186944\n07-11-2017 16:02:41 CST run_dataimport INFO - \tFile Input Format Counters \n07-11-2017 16:02:41 CST run_dataimport INFO - \t\tBytes Read=0\n07-11-2017 16:02:41 CST run_dataimport INFO - \tFile Output Format Counters \n07-11-2017 16:02:41 CST run_dataimport INFO - \t\tBytes Written=93800000\n07-11-2017 16:02:41 CST run_dataimport INFO - 17/11/07 16:02:41 INFO mapreduce.ImportJobBase: Transferred 89.4547 MB in 29.9824 seconds (2.9836 MB/sec)\n07-11-2017 16:02:41 CST run_dataimport INFO - 17/11/07 16:02:41 INFO mapreduce.ImportJobBase: Retrieved 200000 records.\n07-11-2017 16:02:41 CST run_dataimport INFO - 17/11/07 16:02:41 INFO util.AppendUtils: Appending to directory sql_to_hive\n07-11-2017 16:02:42 CST run_dataimport INFO - 17/11/07 16:02:42 INFO util.AppendUtils: Using found partition 6800\n07-11-2017 16:02:42 CST run_dataimport INFO - 17/11/07 16:02:42 INFO tool.ImportTool: Incremental import complete! To run another incremental import of all data following this import, supply the following arguments:\n07-11-2017 16:02:42 CST run_dataimport INFO - 17/11/07 16:02:42 INFO tool.ImportTool:  --incremental append\n07-11-2017 16:02:42 CST run_dataimport INFO - 17/11/07 16:02:42 INFO tool.ImportTool:   --check-column uid\n07-11-2017 16:02:42 CST run_dataimport INFO - 17/11/07 16:02:42 INFO tool.ImportTool:   --last-value 83422300\n07-11-2017 16:02:42 CST run_dataimport INFO - 17/11/07 16:02:42 INFO tool.ImportTool: (Consider saving this with 'sqoop job --create')\n07-11-2017 16:02:42 CST run_dataimport INFO - 6\n07-11-2017 16:02:42 CST run_dataimport INFO - Warning: /opt/cloudera/parcels/CDH-5.8.0-1.cdh5.8.0.p0.42/bin/../lib/sqoop/../accumulo does not exist! Accumulo imports will fail.\n07-11-2017 16:02:42 CST run_dataimport INFO - Please set $ACCUMULO_HOME to the root of your Accumulo installation.\n07-11-2017 16:02:44 CST run_dataimport INFO - 17/11/07 16:02:44 INFO sqoop.Sqoop: Running Sqoop version: 1.4.6-cdh5.8.0\n07-11-2017 16:02:44 CST run_dataimport INFO - 17/11/07 16:02:44 WARN sqoop.ConnFactory: Parameter --driver is set to an explicit driver however appropriate connection manager is not being set (via --connection-manager). Sqoop is going to fall back to org.apache.sqoop.manager.GenericJdbcManager. Please specify explicitly which connection manager should be used next time.\n07-11-2017 16:02:44 CST run_dataimport INFO - 17/11/07 16:02:44 INFO manager.SqlManager: Using default fetchSize of 1000\n07-11-2017 16:02:44 CST run_dataimport INFO - 17/11/07 16:02:44 INFO tool.CodeGenTool: Beginning code generation\n07-11-2017 16:02:45 CST run_dataimport INFO - 17/11/07 16:02:45 INFO manager.SqlManager: Executing SQL statement: select top 200000 * from jzj.dbo.jzj_test where uid&gt;(select uid from (select top 200000 uid from jzj.dbo.jzj_test where uid&gt;83222300 and uid&lt;=84598500) a except select uid from (select top 199999 uid from jzj.dbo.jzj_test where uid&gt;83222300 and uid&lt;=84598500) a) and uid &lt;=84598500 and  (1 = 0) \n07-11-2017 16:02:45 CST run_dataimport INFO - 17/11/07 16:02:45 INFO manager.SqlManager: Executing SQL statement: select top 200000 * from jzj.dbo.jzj_test where uid&gt;(select uid from (select top 200000 uid from jzj.dbo.jzj_test where uid&gt;83222300 and uid&lt;=84598500) a except select uid from (select top 199999 uid from jzj.dbo.jzj_test where uid&gt;83222300 and uid&lt;=84598500) a) and uid &lt;=84598500 and  (1 = 0) \n07-11-2017 16:02:45 CST run_dataimport INFO - 17/11/07 16:02:45 INFO orm.CompilationManager: HADOOP_MAPRED_HOME is /opt/cloudera/parcels/CDH/lib/hadoop-mapreduce\n07-11-2017 16:02:47 CST run_dataimport INFO - Note: /tmp/sqoop-root/compile/3e3fe91d38feffbabab3df86c4ddb2c4/QueryResult.java uses or overrides a deprecated API.\n07-11-2017 16:02:47 CST run_dataimport INFO - Note: Recompile with -Xlint:deprecation for details.\n07-11-2017 16:02:47 CST run_dataimport INFO - 17/11/07 16:02:47 INFO orm.CompilationManager: Writing jar file: /tmp/sqoop-root/compile/3e3fe91d38feffbabab3df86c4ddb2c4/QueryResult.jar\n07-11-2017 16:02:48 CST run_dataimport INFO - 17/11/07 16:02:48 INFO tool.ImportTool: Maximal id query for free form incremental import: SELECT MAX(uid) FROM (select top 200000 * from jzj.dbo.jzj_test where uid&gt;(select uid from (select top 200000 uid from jzj.dbo.jzj_test where uid&gt;83222300 and uid&lt;=84598500) a except select uid from (select top 199999 uid from jzj.dbo.jzj_test where uid&gt;83222300 and uid&lt;=84598500) a) and uid &lt;=84598500 and (1 = 1)) sqoop_import_query_alias\n07-11-2017 16:42:28 CST run_dataimport INFO - 17/11/07 16:42:28 INFO tool.ImportTool: Incremental import based on column uid\n07-11-2017 16:42:28 CST run_dataimport INFO - 17/11/07 16:42:28 INFO tool.ImportTool: Upper bound value: 83622300\n07-11-2017 16:42:28 CST run_dataimport INFO - 17/11/07 16:42:28 INFO mapreduce.ImportJobBase: Beginning query import.\n07-11-2017 16:42:28 CST run_dataimport INFO - 17/11/07 16:42:28 INFO Configuration.deprecation: mapred.jar is deprecated. Instead, use mapreduce.job.jar\n07-11-2017 16:42:28 CST run_dataimport INFO - 17/11/07 16:42:28 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n07-11-2017 16:42:28 CST run_dataimport INFO - 17/11/07 16:42:28 INFO client.RMProxy: Connecting to ResourceManager at master/172.23.27.203:8032\n07-11-2017 16:42:34 CST run_dataimport INFO - 17/11/07 16:42:34 INFO db.DBInputFormat: Using read commited transaction isolation\n07-11-2017 16:42:34 CST run_dataimport INFO - 17/11/07 16:42:34 INFO db.DataDrivenDBInputFormat: BoundingValsQuery: SELECT MIN(uid), MAX(uid) FROM (select top 200000 * from jzj.dbo.jzj_test where uid&gt;(select uid from (select top 200000 uid from jzj.dbo.jzj_test where uid&gt;83222300 and uid&lt;=84598500) a except select uid from (select top 199999 uid from jzj.dbo.jzj_test where uid&gt;83222300 and uid&lt;=84598500) a) and uid &lt;=84598500 and uid &lt;= 83622300 AND  (1 = 1) ) AS t1\n07-11-2017 17:22:49 CST run_dataimport INFO - 17/11/07 17:22:49 INFO mapreduce.JobSubmitter: number of splits:4\n07-11-2017 17:22:49 CST run_dataimport INFO - 17/11/07 17:22:49 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1508671365448_2955\n07-11-2017 17:22:49 CST run_dataimport INFO - 17/11/07 17:22:49 INFO impl.YarnClientImpl: Submitted application application_1508671365448_2955\n07-11-2017 17:22:49 CST run_dataimport INFO - 17/11/07 17:22:49 INFO mapreduce.Job: The url to track the job: http://master:8088/proxy/application_1508671365448_2955/\n07-11-2017 17:22:49 CST run_dataimport INFO - 17/11/07 17:22:49 INFO mapreduce.Job: Running job: job_1508671365448_2955\n07-11-2017 17:22:57 CST run_dataimport INFO - 17/11/07 17:22:57 INFO mapreduce.Job: Job job_1508671365448_2955 running in uber mode : false\n07-11-2017 17:22:57 CST run_dataimport INFO - 17/11/07 17:22:57 INFO mapreduce.Job:  map 0% reduce 0%\n07-11-2017 18:02:56 CST run_dataimport INFO - 17/11/07 18:02:56 INFO mapreduce.Job:  map 25% reduce 0%\n07-11-2017 18:03:23 CST run_dataimport INFO - 17/11/07 18:03:23 INFO mapreduce.Job:  map 50% reduce 0%\n07-11-2017 18:03:46 CST run_dataimport INFO - 17/11/07 18:03:46 INFO mapreduce.Job:  map 75% reduce 0%\n07-11-2017 18:03:47 CST run_dataimport INFO - 17/11/07 18:03:47 INFO mapreduce.Job:  map 100% reduce 0%\n07-11-2017 18:03:48 CST run_dataimport INFO - 17/11/07 18:03:48 INFO mapreduce.Job: Job job_1508671365448_2955 completed successfully\n07-11-2017 18:03:48 CST run_dataimport INFO - 17/11/07 18:03:48 INFO mapreduce.Job: Counters: 30\n07-11-2017 18:03:48 CST run_dataimport INFO - \tFile System Counters\n07-11-2017 18:03:48 CST run_dataimport INFO - \t\tFILE: Number of bytes read=0\n07-11-2017 18:03:48 CST run_dataimport INFO - \t\tFILE: Number of bytes written=588156\n07-11-2017 18:03:48 CST run_dataimport INFO - \t\tFILE: Number of read operations=0\n07-11-2017 18:03:48 CST run_dataimport INFO - \t\tFILE: Number of large read operations=0\n07-11-2017 18:03:48 CST run_dataimport INFO - \t\tFILE: Number of write operations=0\n07-11-2017 18:03:48 CST run_dataimport INFO - \t\tHDFS: Number of bytes read=462\n07-11-2017 18:03:48 CST run_dataimport INFO - \t\tHDFS: Number of bytes written=93800000\n07-11-2017 18:03:48 CST run_dataimport INFO - \t\tHDFS: Number of read operations=16\n07-11-2017 18:03:48 CST run_dataimport INFO - \t\tHDFS: Number of large read operations=0\n07-11-2017 18:03:48 CST run_dataimport INFO - \t\tHDFS: Number of write operations=8\n07-11-2017 18:03:48 CST run_dataimport INFO - \tJob Counters \n07-11-2017 18:03:48 CST run_dataimport INFO - \t\tLaunched map tasks=4\n07-11-2017 18:03:48 CST run_dataimport INFO - \t\tOther local map tasks=4\n07-11-2017 18:03:48 CST run_dataimport INFO - \t\tTotal time spent by all maps in occupied slots (ms)=9712787\n07-11-2017 18:03:48 CST run_dataimport INFO - \t\tTotal time spent by all reduces in occupied slots (ms)=0\n07-11-2017 18:03:48 CST run_dataimport INFO - \t\tTotal time spent by all map tasks (ms)=9712787\n07-11-2017 18:03:48 CST run_dataimport INFO - \t\tTotal vcore-seconds taken by all map tasks=9712787\n07-11-2017 18:03:48 CST run_dataimport INFO - \t\tTotal megabyte-seconds taken by all map tasks=9945893888\n07-11-2017 18:03:48 CST run_dataimport INFO - \tMap-Reduce Framework\n07-11-2017 18:03:48 CST run_dataimport INFO - \t\tMap input records=200000\n07-11-2017 18:03:48 CST run_dataimport INFO - \t\tMap output records=200000\n07-11-2017 18:03:48 CST run_dataimport INFO - \t\tInput split bytes=462\n07-11-2017 18:03:48 CST run_dataimport INFO - \t\tSpilled Records=0\n07-11-2017 18:03:48 CST run_dataimport INFO - \t\tFailed Shuffles=0\n07-11-2017 18:03:48 CST run_dataimport INFO - \t\tMerged Map outputs=0\n07-11-2017 18:03:48 CST run_dataimport INFO - \t\tGC time elapsed (ms)=764\n07-11-2017 18:03:48 CST run_dataimport INFO - \t\tCPU time spent (ms)=64470\n07-11-2017 18:03:48 CST run_dataimport INFO - \t\tPhysical memory (bytes) snapshot=1473638400\n07-11-2017 18:03:48 CST run_dataimport INFO - \t\tVirtual memory (bytes) snapshot=6498091008\n07-11-2017 18:03:48 CST run_dataimport INFO - \t\tTotal committed heap usage (bytes)=3240099840\n07-11-2017 18:03:48 CST run_dataimport INFO - \tFile Input Format Counters \n07-11-2017 18:03:48 CST run_dataimport INFO - \t\tBytes Read=0\n07-11-2017 18:03:48 CST run_dataimport INFO - \tFile Output Format Counters \n07-11-2017 18:03:48 CST run_dataimport INFO - \t\tBytes Written=93800000\n07-11-2017 18:03:48 CST run_dataimport INFO - 17/11/07 18:03:48 INFO mapreduce.ImportJobBase: Transferred 89.4547 MB in 4,879.8256 seconds (18.7715 KB/sec)\n07-11-2017 18:03:48 CST run_dataimport INFO - 17/11/07 18:03:48 INFO mapreduce.ImportJobBase: Retrieved 200000 records.\n07-11-2017 18:03:48 CST run_dataimport INFO - 17/11/07 18:03:48 INFO util.AppendUtils: Appending to directory sql_to_hive\n07-11-2017 18:03:48 CST run_dataimport INFO - 17/11/07 18:03:48 INFO util.AppendUtils: Using found partition 6804\n07-11-2017 18:03:49 CST run_dataimport INFO - 17/11/07 18:03:49 INFO tool.ImportTool: Incremental import complete! To run another incremental import of all data following this import, supply the following arguments:\n07-11-2017 18:03:49 CST run_dataimport INFO - 17/11/07 18:03:49 INFO tool.ImportTool:  --incremental append\n07-11-2017 18:03:49 CST run_dataimport INFO - 17/11/07 18:03:49 INFO tool.ImportTool:   --check-column uid\n07-11-2017 18:03:49 CST run_dataimport INFO - 17/11/07 18:03:49 INFO tool.ImportTool:   --last-value 83622300\n07-11-2017 18:03:49 CST run_dataimport INFO - 17/11/07 18:03:49 INFO tool.ImportTool: (Consider saving this with 'sqoop job --create')\n07-11-2017 18:03:49 CST run_dataimport INFO - 5\n07-11-2017 18:03:49 CST run_dataimport INFO - Warning: /opt/cloudera/parcels/CDH-5.8.0-1.cdh5.8.0.p0.42/bin/../lib/sqoop/../accumulo does not exist! Accumulo imports will fail.\n07-11-2017 18:03:49 CST run_dataimport INFO - Please set $ACCUMULO_HOME to the root of your Accumulo installation.\n07-11-2017 18:03:51 CST run_dataimport INFO - 17/11/07 18:03:51 INFO sqoop.Sqoop: Running Sqoop version: 1.4.6-cdh5.8.0\n07-11-2017 18:03:51 CST run_dataimport INFO - 17/11/07 18:03:51 WARN sqoop.ConnFactory: Parameter --driver is set to an explicit driver however appropriate connection manager is not being set (via --connection-manager). Sqoop is going to fall back to org.apache.sqoop.manager.GenericJdbcManager. Please specify explicitly which connection manager should be used next time.\n07-11-2017 18:03:51 CST run_dataimport INFO - 17/11/07 18:03:51 INFO manager.SqlManager: Using default fetchSize of 1000\n07-11-2017 18:03:51 CST run_dataimport INFO - 17/11/07 18:03:51 INFO tool.CodeGenTool: Beginning code generation\n07-11-2017 18:03:52 CST run_dataimport INFO - 17/11/07 18:03:52 INFO manager.SqlManager: Executing SQL statement: select top 200000 * from jzj.dbo.jzj_test where uid&gt;(select uid from (select top 400000 uid from jzj.dbo.jzj_test where uid&gt;83222300 and uid&lt;=84598500) a except select uid from (select top 399999 uid from jzj.dbo.jzj_test where uid&gt;83222300 and uid&lt;=84598500) a) and uid &lt;=84598500 and  (1 = 0) \n07-11-2017 18:03:52 CST run_dataimport INFO - 17/11/07 18:03:52 INFO manager.SqlManager: Executing SQL statement: select top 200000 * from jzj.dbo.jzj_test where uid&gt;(select uid from (select top 400000 uid from jzj.dbo.jzj_test where uid&gt;83222300 and uid&lt;=84598500) a except select uid from (select top 399999 uid from jzj.dbo.jzj_test where uid&gt;83222300 and uid&lt;=84598500) a) and uid &lt;=84598500 and  (1 = 0) \n07-11-2017 18:03:52 CST run_dataimport INFO - 17/11/07 18:03:52 INFO orm.CompilationManager: HADOOP_MAPRED_HOME is /opt/cloudera/parcels/CDH/lib/hadoop-mapreduce\n07-11-2017 18:03:54 CST run_dataimport INFO - Note: /tmp/sqoop-root/compile/21f23a42adb9bf71452da9a3c7187b06/QueryResult.java uses or overrides a deprecated API.\n07-11-2017 18:03:54 CST run_dataimport INFO - Note: Recompile with -Xlint:deprecation for details.\n07-11-2017 18:03:54 CST run_dataimport INFO - 17/11/07 18:03:54 INFO orm.CompilationManager: Writing jar file: /tmp/sqoop-root/compile/21f23a42adb9bf71452da9a3c7187b06/QueryResult.jar\n07-11-2017 18:03:55 CST run_dataimport INFO - 17/11/07 18:03:55 INFO tool.ImportTool: Maximal id query for free form incremental import: SELECT MAX(uid) FROM (select top 200000 * from jzj.dbo.jzj_test where uid&gt;(select uid from (select top 400000 uid from jzj.dbo.jzj_test where uid&gt;83222300 and uid&lt;=84598500) a except select uid from (select top 399999 uid from jzj.dbo.jzj_test where uid&gt;83222300 and uid&lt;=84598500) a) and uid &lt;=84598500 and (1 = 1)) sqoop_import_query_alias\n07-11-2017 20:43:27 CST run_dataimport INFO - 17/11/07 20:43:27 INFO tool.ImportTool: Incremental import based on column uid\n07-11-2017 20:43:27 CST run_dataimport INFO - 17/11/07 20:43:27 INFO tool.ImportTool: Upper bound value: 83822300\n07-11-2017 20:43:27 CST run_dataimport INFO - 17/11/07 20:43:27 INFO mapreduce.ImportJobBase: Beginning query import.\n07-11-2017 20:43:27 CST run_dataimport INFO - 17/11/07 20:43:27 INFO Configuration.deprecation: mapred.jar is deprecated. Instead, use mapreduce.job.jar\n07-11-2017 20:43:27 CST run_dataimport INFO - 17/11/07 20:43:27 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n07-11-2017 20:43:28 CST run_dataimport INFO - 17/11/07 20:43:28 INFO client.RMProxy: Connecting to ResourceManager at master/172.23.27.203:8032\n07-11-2017 20:43:34 CST run_dataimport INFO - 17/11/07 20:43:34 INFO db.DBInputFormat: Using read commited transaction isolation\n07-11-2017 20:43:34 CST run_dataimport INFO - 17/11/07 20:43:34 INFO db.DataDrivenDBInputFormat: BoundingValsQuery: SELECT MIN(uid), MAX(uid) FROM (select top 200000 * from jzj.dbo.jzj_test where uid&gt;(select uid from (select top 400000 uid from jzj.dbo.jzj_test where uid&gt;83222300 and uid&lt;=84598500) a except select uid from (select top 399999 uid from jzj.dbo.jzj_test where uid&gt;83222300 and uid&lt;=84598500) a) and uid &lt;=84598500 and uid &lt;= 83822300 AND  (1 = 1) ) AS t1\n07-11-2017 23:22:41 CST run_dataimport INFO - 17/11/07 23:22:41 INFO mapreduce.JobSubmitter: number of splits:4\n07-11-2017 23:22:41 CST run_dataimport INFO - 17/11/07 23:22:41 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1508671365448_2956\n07-11-2017 23:22:42 CST run_dataimport INFO - 17/11/07 23:22:42 INFO impl.YarnClientImpl: Submitted application application_1508671365448_2956\n07-11-2017 23:22:42 CST run_dataimport INFO - 17/11/07 23:22:42 INFO mapreduce.Job: The url to track the job: http://master:8088/proxy/application_1508671365448_2956/\n07-11-2017 23:22:42 CST run_dataimport INFO - 17/11/07 23:22:42 INFO mapreduce.Job: Running job: job_1508671365448_2956\n07-11-2017 23:22:49 CST run_dataimport INFO - 17/11/07 23:22:49 INFO mapreduce.Job: Job job_1508671365448_2956 running in uber mode : false\n07-11-2017 23:22:49 CST run_dataimport INFO - 17/11/07 23:22:49 INFO mapreduce.Job:  map 0% reduce 0%\n08-11-2017 02:00:13 CST run_dataimport INFO - 17/11/08 02:00:13 INFO mapreduce.Job:  map 25% reduce 0%\n08-11-2017 02:01:28 CST run_dataimport INFO - 17/11/08 02:01:28 INFO mapreduce.Job:  map 50% reduce 0%\n08-11-2017 02:03:21 CST run_dataimport INFO - 17/11/08 02:03:21 INFO mapreduce.Job:  map 75% reduce 0%\n08-11-2017 02:03:22 CST run_dataimport INFO - 17/11/08 02:03:22 INFO mapreduce.Job:  map 100% reduce 0%\n08-11-2017 02:03:23 CST run_dataimport INFO - 17/11/08 02:03:23 INFO mapreduce.Job: Job job_1508671365448_2956 completed successfully\n08-11-2017 02:03:23 CST run_dataimport INFO - 17/11/08 02:03:23 INFO mapreduce.Job: Counters: 30\n08-11-2017 02:03:23 CST run_dataimport INFO - \tFile System Counters\n08-11-2017 02:03:23 CST run_dataimport INFO - \t\tFILE: Number of bytes read=0\n08-11-2017 02:03:23 CST run_dataimport INFO - \t\tFILE: Number of bytes written=588300\n08-11-2017 02:03:23 CST run_dataimport INFO - \t\tFILE: Number of read operations=0\n08-11-2017 02:03:23 CST run_dataimport INFO - \t\tFILE: Number of large read operations=0\n08-11-2017 02:03:23 CST run_dataimport INFO - \t\tFILE: Number of write operations=0\n08-11-2017 02:03:23 CST run_dataimport INFO - \t\tHDFS: Number of bytes read=462\n08-11-2017 02:03:23 CST run_dataimport INFO - \t\tHDFS: Number of bytes written=93800000\n08-11-2017 02:03:23 CST run_dataimport INFO - \t\tHDFS: Number of read operations=16\n08-11-2017 02:03:23 CST run_dataimport INFO - \t\tHDFS: Number of large read operations=0\n08-11-2017 02:03:23 CST run_dataimport INFO - \t\tHDFS: Number of write operations=8\n08-11-2017 02:03:23 CST run_dataimport INFO - \tJob Counters \n08-11-2017 02:03:23 CST run_dataimport INFO - \t\tLaunched map tasks=4\n08-11-2017 02:03:23 CST run_dataimport INFO - \t\tOther local map tasks=4\n08-11-2017 02:03:23 CST run_dataimport INFO - \t\tTotal time spent by all maps in occupied slots (ms)=38216711\n08-11-2017 02:03:23 CST run_dataimport INFO - \t\tTotal time spent by all reduces in occupied slots (ms)=0\n08-11-2017 02:03:23 CST run_dataimport INFO - \t\tTotal time spent by all map tasks (ms)=38216711\n08-11-2017 02:03:23 CST run_dataimport INFO - \t\tTotal vcore-seconds taken by all map tasks=38216711\n08-11-2017 02:03:23 CST run_dataimport INFO - \t\tTotal megabyte-seconds taken by all map tasks=39133912064\n08-11-2017 02:03:23 CST run_dataimport INFO - \tMap-Reduce Framework\n08-11-2017 02:03:23 CST run_dataimport INFO - \t\tMap input records=200000\n08-11-2017 02:03:23 CST run_dataimport INFO - \t\tMap output records=200000\n08-11-2017 02:03:23 CST run_dataimport INFO - \t\tInput split bytes=462\n08-11-2017 02:03:23 CST run_dataimport INFO - \t\tSpilled Records=0\n08-11-2017 02:03:23 CST run_dataimport INFO - \t\tFailed Shuffles=0\n08-11-2017 02:03:23 CST run_dataimport INFO - \t\tMerged Map outputs=0\n08-11-2017 02:03:23 CST run_dataimport INFO - \t\tGC time elapsed (ms)=1588\n08-11-2017 02:03:23 CST run_dataimport INFO - \t\tCPU time spent (ms)=134830\n08-11-2017 02:03:23 CST run_dataimport INFO - \t\tPhysical memory (bytes) snapshot=1547542528\n08-11-2017 02:03:23 CST run_dataimport INFO - \t\tVirtual memory (bytes) snapshot=6530818048\n08-11-2017 02:03:23 CST run_dataimport INFO - \t\tTotal committed heap usage (bytes)=3172990976\n08-11-2017 02:03:23 CST run_dataimport INFO - \tFile Input Format Counters \n08-11-2017 02:03:23 CST run_dataimport INFO - \t\tBytes Read=0\n08-11-2017 02:03:23 CST run_dataimport INFO - \tFile Output Format Counters \n08-11-2017 02:03:23 CST run_dataimport INFO - \t\tBytes Written=93800000\n08-11-2017 02:03:23 CST run_dataimport INFO - 17/11/08 02:03:23 INFO mapreduce.ImportJobBase: Transferred 89.4547 MB in 19,195.7726 seconds (4.772 KB/sec)\n08-11-2017 02:03:23 CST run_dataimport INFO - 17/11/08 02:03:23 INFO mapreduce.ImportJobBase: Retrieved 200000 records.\n08-11-2017 02:03:23 CST run_dataimport INFO - 17/11/08 02:03:23 INFO util.AppendUtils: Appending to directory sql_to_hive\n08-11-2017 02:03:24 CST run_dataimport INFO - 17/11/08 02:03:24 INFO util.AppendUtils: Using found partition 6808\n08-11-2017 02:03:24 CST run_dataimport INFO - 17/11/08 02:03:24 INFO tool.ImportTool: Incremental import complete! To run another incremental import of all data following this import, supply the following arguments:\n08-11-2017 02:03:24 CST run_dataimport INFO - 17/11/08 02:03:24 INFO tool.ImportTool:  --incremental append\n08-11-2017 02:03:24 CST run_dataimport INFO - 17/11/08 02:03:24 INFO tool.ImportTool:   --check-column uid\n08-11-2017 02:03:24 CST run_dataimport INFO - 17/11/08 02:03:24 INFO tool.ImportTool:   --last-value 83822300\n08-11-2017 02:03:24 CST run_dataimport INFO - 17/11/08 02:03:24 INFO tool.ImportTool: (Consider saving this with 'sqoop job --create')\n08-11-2017 02:03:24 CST run_dataimport INFO - 4\n08-11-2017 02:03:24 CST run_dataimport INFO - Warning: /opt/cloudera/parcels/CDH-5.8.0-1.cdh5.8.0.p0.42/bin/../lib/sqoop/../accumulo does not exist! Accumulo imports will fail.\n08-11-2017 02:03:24 CST run_dataimport INFO - Please set $ACCUMULO_HOME to the root of your Accumulo installation.\n08-11-2017 02:03:26 CST run_dataimport INFO - 17/11/08 02:03:26 INFO sqoop.Sqoop: Running Sqoop version: 1.4.6-cdh5.8.0\n08-11-2017 02:03:26 CST run_dataimport INFO - 17/11/08 02:03:26 WARN sqoop.ConnFactory: Parameter --driver is set to an explicit driver however appropriate connection manager is not being set (via --connection-manager). Sqoop is going to fall back to org.apache.sqoop.manager.GenericJdbcManager. Please specify explicitly which connection manager should be used next time.\n08-11-2017 02:03:26 CST run_dataimport INFO - 17/11/08 02:03:26 INFO manager.SqlManager: Using default fetchSize of 1000\n08-11-2017 02:03:26 CST run_dataimport INFO - 17/11/08 02:03:26 INFO tool.CodeGenTool: Beginning code generation\n08-11-2017 02:03:27 CST run_dataimport INFO - 17/11/08 02:03:27 INFO manager.SqlManager: Executing SQL statement: select top 200000 * from jzj.dbo.jzj_test where uid&gt;(select uid from (select top 600000 uid from jzj.dbo.jzj_test where uid&gt;83222300 and uid&lt;=84598500) a except select uid from (select top 599999 uid from jzj.dbo.jzj_test where uid&gt;83222300 and uid&lt;=84598500) a) and uid &lt;=84598500 and  (1 = 0) \n08-11-2017 02:03:27 CST run_dataimport INFO - 17/11/08 02:03:27 INFO manager.SqlManager: Executing SQL statement: select top 200000 * from jzj.dbo.jzj_test where uid&gt;(select uid from (select top 600000 uid from jzj.dbo.jzj_test where uid&gt;83222300 and uid&lt;=84598500) a except select uid from (select top 599999 uid from jzj.dbo.jzj_test where uid&gt;83222300 and uid&lt;=84598500) a) and uid &lt;=84598500 and  (1 = 0) \n08-11-2017 02:03:27 CST run_dataimport INFO - 17/11/08 02:03:27 INFO orm.CompilationManager: HADOOP_MAPRED_HOME is /opt/cloudera/parcels/CDH/lib/hadoop-mapreduce\n08-11-2017 02:03:29 CST run_dataimport INFO - Note: /tmp/sqoop-root/compile/e8acb4e9422617ba257a228b6dfa0561/QueryResult.java uses or overrides a deprecated API.\n08-11-2017 02:03:29 CST run_dataimport INFO - Note: Recompile with -Xlint:deprecation for details.\n08-11-2017 02:03:29 CST run_dataimport INFO - 17/11/08 02:03:29 INFO orm.CompilationManager: Writing jar file: /tmp/sqoop-root/compile/e8acb4e9422617ba257a228b6dfa0561/QueryResult.jar\n08-11-2017 02:03:30 CST run_dataimport INFO - 17/11/08 02:03:30 INFO tool.ImportTool: Maximal id query for free form incremental import: SELECT MAX(uid) FROM (select top 200000 * from jzj.dbo.jzj_test where uid&gt;(select uid from (select top 600000 uid from jzj.dbo.jzj_test where uid&gt;83222300 and uid&lt;=84598500) a except select uid from (select top 599999 uid from jzj.dbo.jzj_test where uid&gt;83222300 and uid&lt;=84598500) a) and uid &lt;=84598500 and (1 = 1)) sqoop_import_query_alias\n08-11-2017 07:55:53 CST run_dataimport INFO - 17/11/08 07:55:53 INFO tool.ImportTool: Incremental import based on column uid\n08-11-2017 07:55:53 CST run_dataimport INFO - 17/11/08 07:55:53 INFO tool.ImportTool: Upper bound value: 84022300\n08-11-2017 07:55:53 CST run_dataimport INFO - 17/11/08 07:55:53 INFO mapreduce.ImportJobBase: Beginning query import.\n08-11-2017 07:55:53 CST run_dataimport INFO - 17/11/08 07:55:53 INFO Configuration.deprecation: mapred.jar is deprecated. Instead, use mapreduce.job.jar\n08-11-2017 07:55:53 CST run_dataimport INFO - 17/11/08 07:55:53 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n08-11-2017 07:55:53 CST run_dataimport INFO - 17/11/08 07:55:53 INFO client.RMProxy: Connecting to ResourceManager at master/172.23.27.203:8032\n08-11-2017 07:56:00 CST run_dataimport INFO - 17/11/08 07:56:00 INFO db.DBInputFormat: Using read commited transaction isolation\n08-11-2017 07:56:00 CST run_dataimport INFO - 17/11/08 07:56:00 INFO db.DataDrivenDBInputFormat: BoundingValsQuery: SELECT MIN(uid), MAX(uid) FROM (select top 200000 * from jzj.dbo.jzj_test where uid&gt;(select uid from (select top 600000 uid from jzj.dbo.jzj_test where uid&gt;83222300 and uid&lt;=84598500) a except select uid from (select top 599999 uid from jzj.dbo.jzj_test where uid&gt;83222300 and uid&lt;=84598500) a) and uid &lt;=84598500 and uid &lt;= 84022300 AND  (1 = 1) ) AS t1\n08-11-2017 12:11:01 CST run_dataimport ERROR - Kill has been called.\n08-11-2017 12:11:06 CST run_dataimport INFO - Process completed unsuccessfully in 72615 seconds.\n08-11-2017 12:11:06 CST run_dataimport ERROR - Job run killed!\njava.lang.RuntimeException: azkaban.jobExecutor.utils.process.ProcessFailureException\n\tat azkaban.jobExecutor.ProcessJob.run(ProcessJob.java:297)\n\tat azkaban.execapp.JobRunner.runJob(JobRunner.java:748)\n\tat azkaban.execapp.JobRunner.doRun(JobRunner.java:591)\n\tat azkaban.execapp.JobRunner.run(JobRunner.java:552)\n\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: azkaban.jobExecutor.utils.process.ProcessFailureException\n\tat azkaban.jobExecutor.utils.process.AzkabanProcess.run(AzkabanProcess.java:130)\n\tat azkaban.jobExecutor.ProcessJob.run(ProcessJob.java:289)\n\t... 8 more\n08-11-2017 12:11:06 CST run_dataimport ERROR - azkaban.jobExecutor.utils.process.ProcessFailureException cause: azkaban.jobExecutor.utils.process.ProcessFailureException\n08-11-2017 12:11:06 CST run_dataimport INFO - Finishing job run_dataimport at 1510114266951 with status KILLED\n", 'length': 37422}
[2017-11-20 20:52:30,772] app.hive_mysql.AzkabanMonitor [INFO] URL: https://172.23.27.203:8443
[2017-11-20 20:52:30,960] app.hive_mysql.AzkabanMonitor [INFO] Success: Login https://172.23.27.203:8443 with user azkaban
[2017-11-20 20:52:30,960] app.hive_mysql.AzkabanMonitor [DEBUG] {'session.id': '0bc68cba-6b16-4b00-afcf-3863b3645635', 'status': 'success'}
[2017-11-20 20:52:30,971] app.hive_mysql.AzkabanMonitor [INFO] Success: Fetch Executions of a Flow - run_dataimport
[2017-11-20 20:52:30,971] app.hive_mysql.AzkabanMonitor [DEBUG] {'total': 16, 'executions': [{'submitTime': 1510041651107, 'submitUser': 'azkaban', 'startTime': 1510041651277, 'endTime': 1510114267057, 'flowId': 'run_dataimport', 'projectId': 68, 'execId': 1670, 'status': 'KILLED'}, {'submitTime': 1510027251078, 'submitUser': 'azkaban', 'startTime': 1510027251395, 'endTime': 1510036906931, 'flowId': 'run_dataimport', 'projectId': 68, 'execId': 1669, 'status': 'SUCCEEDED'}, {'submitTime': 1509897650826, 'submitUser': 'azkaban', 'startTime': 1509897651116, 'endTime': 1510026870568, 'flowId': 'run_dataimport', 'projectId': 68, 'execId': 1668, 'status': 'SUCCEEDED'}, {'submitTime': 1509883250800, 'submitUser': 'azkaban', 'startTime': 1509883251056, 'endTime': 1509892870422, 'flowId': 'run_dataimport', 'projectId': 68, 'execId': 1667, 'status': 'SUCCEEDED'}, {'submitTime': 1509753650545, 'submitUser': 'azkaban', 'startTime': 1509753650705, 'endTime': 1509881095432, 'flowId': 'run_dataimport', 'projectId': 68, 'execId': 1666, 'status': 'SUCCEEDED'}, {'submitTime': 1509746450529, 'submitUser': 'azkaban', 'startTime': 1509746450965, 'endTime': 1509753076918, 'flowId': 'run_dataimport', 'projectId': 68, 'execId': 1665, 'status': 'SUCCEEDED'}, {'submitTime': 1509652850362, 'submitUser': 'azkaban', 'startTime': 1509652850560, 'endTime': 1509744409191, 'flowId': 'run_dataimport', 'projectId': 68, 'execId': 1664, 'status': 'SUCCEEDED'}, {'submitTime': 1509645650349, 'submitUser': 'azkaban', 'startTime': 1509645650498, 'endTime': 1509651159750, 'flowId': 'run_dataimport', 'projectId': 68, 'execId': 1663, 'status': 'SUCCEEDED'}, {'submitTime': 1509566450178, 'submitUser': 'azkaban', 'startTime': 1509566450488, 'endTime': 1509642929867, 'flowId': 'run_dataimport', 'projectId': 68, 'execId': 1661, 'status': 'SUCCEEDED'}, {'submitTime': 1509559250163, 'submitUser': 'azkaban', 'startTime': 1509559250321, 'endTime': 1509563186239, 'flowId': 'run_dataimport', 'projectId': 68, 'execId': 1660, 'status': 'SUCCEEDED'}], 'length': 10, 'project': 'sql_to_hive', 'from': 0, 'projectId': 68, 'flow': 'run_dataimport'}
[2017-11-20 20:52:30,984] app.hive_mysql.AzkabanMonitor [INFO] Success: Fetch Execution Job Logs - 1670-run_dataimport
[2017-11-20 20:52:30,984] app.hive_mysql.AzkabanMonitor [DEBUG] {'offset': 0, 'data': "07-11-2017 16:00:51 CST run_dataimport INFO - Starting job run_dataimport at 1510041651281\n07-11-2017 16:00:51 CST run_dataimport INFO - azkaban.webserver.url property was not set\n07-11-2017 16:00:51 CST run_dataimport INFO - job JVM args: -Dazkaban.flowid=run_dataimport -Dazkaban.execid=1670 -Dazkaban.jobid=run_dataimport\n07-11-2017 16:00:51 CST run_dataimport INFO - Building command job executor. \n07-11-2017 16:00:51 CST run_dataimport INFO - Memory granted for job run_dataimport\n07-11-2017 16:00:51 CST run_dataimport INFO - 1 commands to execute.\n07-11-2017 16:00:51 CST run_dataimport INFO - cwd=/home/azkaban/azkaban_bk/azkaban-exec-server-3.35.0-22-g5517d50/executions/1670\n07-11-2017 16:00:51 CST run_dataimport INFO - effective user is: azkaban\n07-11-2017 16:00:51 CST run_dataimport INFO - Command: sh /home/sql_to_hive/dataimport.sh\n07-11-2017 16:00:51 CST run_dataimport INFO - Environment variables: {JOB_OUTPUT_PROP_FILE=/home/azkaban/azkaban_bk/azkaban-exec-server-3.35.0-22-g5517d50/executions/1670/run_dataimport_output_8807970133616151260_tmp, JOB_PROP_FILE=/home/azkaban/azkaban_bk/azkaban-exec-server-3.35.0-22-g5517d50/executions/1670/run_dataimport_props_6140684690151138768_tmp, KRB5CCNAME=/tmp/krb5cc__sql_to_hive__run_dataimport__run_dataimport__1670__azkaban, JOB_NAME=run_dataimport}\n07-11-2017 16:00:51 CST run_dataimport INFO - Working directory: /home/azkaban/azkaban_bk/azkaban-exec-server-3.35.0-22-g5517d50/executions/1670\n07-11-2017 16:00:51 CST run_dataimport INFO - log4j:WARN No appenders could be found for logger (org.apache.hive.jdbc.Utils).\n07-11-2017 16:00:51 CST run_dataimport INFO - log4j:WARN Please initialize the log4j system properly.\n07-11-2017 16:00:51 CST run_dataimport INFO - log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.\n07-11-2017 16:00:51 CST run_dataimport INFO - SLF4J: Failed to load class &quot;org.slf4j.impl.StaticLoggerBinder&quot;.\n07-11-2017 16:00:51 CST run_dataimport INFO - SLF4J: Defaulting to no-operation (NOP) logger implementation\n07-11-2017 16:00:51 CST run_dataimport INFO - SLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.\n07-11-2017 16:02:05 CST run_dataimport INFO - 1,84598500,83222300,1376200sqlmax:84598500\n07-11-2017 16:02:05 CST run_dataimport INFO - hivemax:83222300\n07-11-2017 16:02:05 CST run_dataimport INFO - counts:1376200\n07-11-2017 16:02:05 CST run_dataimport INFO - 7,176200\n07-11-2017 16:02:05 CST run_dataimport INFO - 7\n07-11-2017 16:02:05 CST run_dataimport INFO - Warning: /opt/cloudera/parcels/CDH-5.8.0-1.cdh5.8.0.p0.42/bin/../lib/sqoop/../accumulo does not exist! Accumulo imports will fail.\n07-11-2017 16:02:05 CST run_dataimport INFO - Please set $ACCUMULO_HOME to the root of your Accumulo installation.\n07-11-2017 16:02:07 CST run_dataimport INFO - 17/11/07 16:02:07 INFO sqoop.Sqoop: Running Sqoop version: 1.4.6-cdh5.8.0\n07-11-2017 16:02:07 CST run_dataimport INFO - 17/11/07 16:02:07 WARN sqoop.ConnFactory: Parameter --driver is set to an explicit driver however appropriate connection manager is not being set (via --connection-manager). Sqoop is going to fall back to org.apache.sqoop.manager.GenericJdbcManager. Please specify explicitly which connection manager should be used next time.\n07-11-2017 16:02:07 CST run_dataimport INFO - 17/11/07 16:02:07 INFO manager.SqlManager: Using default fetchSize of 1000\n07-11-2017 16:02:07 CST run_dataimport INFO - 17/11/07 16:02:07 INFO tool.CodeGenTool: Beginning code generation\n07-11-2017 16:02:08 CST run_dataimport INFO - 17/11/07 16:02:08 INFO manager.SqlManager: Executing SQL statement: select top 200000 * from jzj.dbo.jzj_test where uid&gt;=(select uid from (select top 1 uid from jzj.dbo.jzj_test where uid&gt;83222300 and uid&lt;=84598500) a) and uid &lt;=84598500 and  (1 = 0) \n07-11-2017 16:02:08 CST run_dataimport INFO - 17/11/07 16:02:08 INFO manager.SqlManager: Executing SQL statement: select top 200000 * from jzj.dbo.jzj_test where uid&gt;=(select uid from (select top 1 uid from jzj.dbo.jzj_test where uid&gt;83222300 and uid&lt;=84598500) a) and uid &lt;=84598500 and  (1 = 0) \n07-11-2017 16:02:08 CST run_dataimport INFO - 17/11/07 16:02:08 INFO orm.CompilationManager: HADOOP_MAPRED_HOME is /opt/cloudera/parcels/CDH/lib/hadoop-mapreduce\n07-11-2017 16:02:10 CST run_dataimport INFO - Note: /tmp/sqoop-root/compile/b796f2c0b0198c05bdcf8149b3f9a7fd/QueryResult.java uses or overrides a deprecated API.\n07-11-2017 16:02:10 CST run_dataimport INFO - Note: Recompile with -Xlint:deprecation for details.\n07-11-2017 16:02:10 CST run_dataimport INFO - 17/11/07 16:02:10 INFO orm.CompilationManager: Writing jar file: /tmp/sqoop-root/compile/b796f2c0b0198c05bdcf8149b3f9a7fd/QueryResult.jar\n07-11-2017 16:02:11 CST run_dataimport INFO - 17/11/07 16:02:11 INFO tool.ImportTool: Maximal id query for free form incremental import: SELECT MAX(uid) FROM (select top 200000 * from jzj.dbo.jzj_test where uid&gt;=(select uid from (select top 1 uid from jzj.dbo.jzj_test where uid&gt;83222300 and uid&lt;=84598500) a) and uid &lt;=84598500 and (1 = 1)) sqoop_import_query_alias\n07-11-2017 16:02:11 CST run_dataimport INFO - 17/11/07 16:02:11 INFO tool.ImportTool: Incremental import based on column uid\n07-11-2017 16:02:11 CST run_dataimport INFO - 17/11/07 16:02:11 INFO tool.ImportTool: Upper bound value: 83422300\n07-11-2017 16:02:11 CST run_dataimport INFO - 17/11/07 16:02:11 INFO mapreduce.ImportJobBase: Beginning query import.\n07-11-2017 16:02:11 CST run_dataimport INFO - 17/11/07 16:02:11 INFO Configuration.deprecation: mapred.jar is deprecated. Instead, use mapreduce.job.jar\n07-11-2017 16:02:11 CST run_dataimport INFO - 17/11/07 16:02:11 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n07-11-2017 16:02:11 CST run_dataimport INFO - 17/11/07 16:02:11 INFO client.RMProxy: Connecting to ResourceManager at master/172.23.27.203:8032\n07-11-2017 16:02:18 CST run_dataimport INFO - 17/11/07 16:02:18 INFO db.DBInputFormat: Using read commited transaction isolation\n07-11-2017 16:02:18 CST run_dataimport INFO - 17/11/07 16:02:18 INFO db.DataDrivenDBInputFormat: BoundingValsQuery: SELECT MIN(uid), MAX(uid) FROM (select top 200000 * from jzj.dbo.jzj_test where uid&gt;=(select uid from (select top 1 uid from jzj.dbo.jzj_test where uid&gt;83222300 and uid&lt;=84598500) a) and uid &lt;=84598500 and uid &lt;= 83422300 AND  (1 = 1) ) AS t1\n07-11-2017 16:02:18 CST run_dataimport INFO - 17/11/07 16:02:18 INFO mapreduce.JobSubmitter: number of splits:4\n07-11-2017 16:02:18 CST run_dataimport INFO - 17/11/07 16:02:18 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1508671365448_2954\n07-11-2017 16:02:19 CST run_dataimport INFO - 17/11/07 16:02:19 INFO impl.YarnClientImpl: Submitted application application_1508671365448_2954\n07-11-2017 16:02:19 CST run_dataimport INFO - 17/11/07 16:02:19 INFO mapreduce.Job: The url to track the job: http://master:8088/proxy/application_1508671365448_2954/\n07-11-2017 16:02:19 CST run_dataimport INFO - 17/11/07 16:02:19 INFO mapreduce.Job: Running job: job_1508671365448_2954\n07-11-2017 16:02:26 CST run_dataimport INFO - 17/11/07 16:02:26 INFO mapreduce.Job: Job job_1508671365448_2954 running in uber mode : false\n07-11-2017 16:02:26 CST run_dataimport INFO - 17/11/07 16:02:26 INFO mapreduce.Job:  map 0% reduce 0%\n07-11-2017 16:02:37 CST run_dataimport INFO - 17/11/07 16:02:37 INFO mapreduce.Job:  map 25% reduce 0%\n07-11-2017 16:02:40 CST run_dataimport INFO - 17/11/07 16:02:40 INFO mapreduce.Job:  map 100% reduce 0%\n07-11-2017 16:02:41 CST run_dataimport INFO - 17/11/07 16:02:41 INFO mapreduce.Job: Job job_1508671365448_2954 completed successfully\n07-11-2017 16:02:41 CST run_dataimport INFO - 17/11/07 16:02:41 INFO mapreduce.Job: Counters: 30\n07-11-2017 16:02:41 CST run_dataimport INFO - \tFile System Counters\n07-11-2017 16:02:41 CST run_dataimport INFO - \t\tFILE: Number of bytes read=0\n07-11-2017 16:02:41 CST run_dataimport INFO - \t\tFILE: Number of bytes written=587024\n07-11-2017 16:02:41 CST run_dataimport INFO - \t\tFILE: Number of read operations=0\n07-11-2017 16:02:41 CST run_dataimport INFO - \t\tFILE: Number of large read operations=0\n07-11-2017 16:02:41 CST run_dataimport INFO - \t\tFILE: Number of write operations=0\n07-11-2017 16:02:41 CST run_dataimport INFO - \t\tHDFS: Number of bytes read=462\n07-11-2017 16:02:41 CST run_dataimport INFO - \t\tHDFS: Number of bytes written=93800000\n07-11-2017 16:02:41 CST run_dataimport INFO - \t\tHDFS: Number of read operations=16\n07-11-2017 16:02:41 CST run_dataimport INFO - \t\tHDFS: Number of large read operations=0\n07-11-2017 16:02:41 CST run_dataimport INFO - \t\tHDFS: Number of write operations=8\n07-11-2017 16:02:41 CST run_dataimport INFO - \tJob Counters \n07-11-2017 16:02:41 CST run_dataimport INFO - \t\tLaunched map tasks=4\n07-11-2017 16:02:41 CST run_dataimport INFO - \t\tOther local map tasks=4\n07-11-2017 16:02:41 CST run_dataimport INFO - \t\tTotal time spent by all maps in occupied slots (ms)=44342\n07-11-2017 16:02:41 CST run_dataimport INFO - \t\tTotal time spent by all reduces in occupied slots (ms)=0\n07-11-2017 16:02:41 CST run_dataimport INFO - \t\tTotal time spent by all map tasks (ms)=44342\n07-11-2017 16:02:41 CST run_dataimport INFO - \t\tTotal vcore-seconds taken by all map tasks=44342\n07-11-2017 16:02:41 CST run_dataimport INFO - \t\tTotal megabyte-seconds taken by all map tasks=45406208\n07-11-2017 16:02:41 CST run_dataimport INFO - \tMap-Reduce Framework\n07-11-2017 16:02:41 CST run_dataimport INFO - \t\tMap input records=200000\n07-11-2017 16:02:41 CST run_dataimport INFO - \t\tMap output records=200000\n07-11-2017 16:02:41 CST run_dataimport INFO - \t\tInput split bytes=462\n07-11-2017 16:02:41 CST run_dataimport INFO - \t\tSpilled Records=0\n07-11-2017 16:02:41 CST run_dataimport INFO - \t\tFailed Shuffles=0\n07-11-2017 16:02:41 CST run_dataimport INFO - \t\tMerged Map outputs=0\n07-11-2017 16:02:41 CST run_dataimport INFO - \t\tGC time elapsed (ms)=569\n07-11-2017 16:02:41 CST run_dataimport INFO - \t\tCPU time spent (ms)=36080\n07-11-2017 16:02:41 CST run_dataimport INFO - \t\tPhysical memory (bytes) snapshot=1458102272\n07-11-2017 16:02:41 CST run_dataimport INFO - \t\tVirtual memory (bytes) snapshot=6516502528\n07-11-2017 16:02:41 CST run_dataimport INFO - \t\tTotal committed heap usage (bytes)=3231186944\n07-11-2017 16:02:41 CST run_dataimport INFO - \tFile Input Format Counters \n07-11-2017 16:02:41 CST run_dataimport INFO - \t\tBytes Read=0\n07-11-2017 16:02:41 CST run_dataimport INFO - \tFile Output Format Counters \n07-11-2017 16:02:41 CST run_dataimport INFO - \t\tBytes Written=93800000\n07-11-2017 16:02:41 CST run_dataimport INFO - 17/11/07 16:02:41 INFO mapreduce.ImportJobBase: Transferred 89.4547 MB in 29.9824 seconds (2.9836 MB/sec)\n07-11-2017 16:02:41 CST run_dataimport INFO - 17/11/07 16:02:41 INFO mapreduce.ImportJobBase: Retrieved 200000 records.\n07-11-2017 16:02:41 CST run_dataimport INFO - 17/11/07 16:02:41 INFO util.AppendUtils: Appending to directory sql_to_hive\n07-11-2017 16:02:42 CST run_dataimport INFO - 17/11/07 16:02:42 INFO util.AppendUtils: Using found partition 6800\n07-11-2017 16:02:42 CST run_dataimport INFO - 17/11/07 16:02:42 INFO tool.ImportTool: Incremental import complete! To run another incremental import of all data following this import, supply the following arguments:\n07-11-2017 16:02:42 CST run_dataimport INFO - 17/11/07 16:02:42 INFO tool.ImportTool:  --incremental append\n07-11-2017 16:02:42 CST run_dataimport INFO - 17/11/07 16:02:42 INFO tool.ImportTool:   --check-column uid\n07-11-2017 16:02:42 CST run_dataimport INFO - 17/11/07 16:02:42 INFO tool.ImportTool:   --last-value 83422300\n07-11-2017 16:02:42 CST run_dataimport INFO - 17/11/07 16:02:42 INFO tool.ImportTool: (Consider saving this with 'sqoop job --create')\n07-11-2017 16:02:42 CST run_dataimport INFO - 6\n07-11-2017 16:02:42 CST run_dataimport INFO - Warning: /opt/cloudera/parcels/CDH-5.8.0-1.cdh5.8.0.p0.42/bin/../lib/sqoop/../accumulo does not exist! Accumulo imports will fail.\n07-11-2017 16:02:42 CST run_dataimport INFO - Please set $ACCUMULO_HOME to the root of your Accumulo installation.\n07-11-2017 16:02:44 CST run_dataimport INFO - 17/11/07 16:02:44 INFO sqoop.Sqoop: Running Sqoop version: 1.4.6-cdh5.8.0\n07-11-2017 16:02:44 CST run_dataimport INFO - 17/11/07 16:02:44 WARN sqoop.ConnFactory: Parameter --driver is set to an explicit driver however appropriate connection manager is not being set (via --connection-manager). Sqoop is going to fall back to org.apache.sqoop.manager.GenericJdbcManager. Please specify explicitly which connection manager should be used next time.\n07-11-2017 16:02:44 CST run_dataimport INFO - 17/11/07 16:02:44 INFO manager.SqlManager: Using default fetchSize of 1000\n07-11-2017 16:02:44 CST run_dataimport INFO - 17/11/07 16:02:44 INFO tool.CodeGenTool: Beginning code generation\n07-11-2017 16:02:45 CST run_dataimport INFO - 17/11/07 16:02:45 INFO manager.SqlManager: Executing SQL statement: select top 200000 * from jzj.dbo.jzj_test where uid&gt;(select uid from (select top 200000 uid from jzj.dbo.jzj_test where uid&gt;83222300 and uid&lt;=84598500) a except select uid from (select top 199999 uid from jzj.dbo.jzj_test where uid&gt;83222300 and uid&lt;=84598500) a) and uid &lt;=84598500 and  (1 = 0) \n07-11-2017 16:02:45 CST run_dataimport INFO - 17/11/07 16:02:45 INFO manager.SqlManager: Executing SQL statement: select top 200000 * from jzj.dbo.jzj_test where uid&gt;(select uid from (select top 200000 uid from jzj.dbo.jzj_test where uid&gt;83222300 and uid&lt;=84598500) a except select uid from (select top 199999 uid from jzj.dbo.jzj_test where uid&gt;83222300 and uid&lt;=84598500) a) and uid &lt;=84598500 and  (1 = 0) \n07-11-2017 16:02:45 CST run_dataimport INFO - 17/11/07 16:02:45 INFO orm.CompilationManager: HADOOP_MAPRED_HOME is /opt/cloudera/parcels/CDH/lib/hadoop-mapreduce\n07-11-2017 16:02:47 CST run_dataimport INFO - Note: /tmp/sqoop-root/compile/3e3fe91d38feffbabab3df86c4ddb2c4/QueryResult.java uses or overrides a deprecated API.\n07-11-2017 16:02:47 CST run_dataimport INFO - Note: Recompile with -Xlint:deprecation for details.\n07-11-2017 16:02:47 CST run_dataimport INFO - 17/11/07 16:02:47 INFO orm.CompilationManager: Writing jar file: /tmp/sqoop-root/compile/3e3fe91d38feffbabab3df86c4ddb2c4/QueryResult.jar\n07-11-2017 16:02:48 CST run_dataimport INFO - 17/11/07 16:02:48 INFO tool.ImportTool: Maximal id query for free form incremental import: SELECT MAX(uid) FROM (select top 200000 * from jzj.dbo.jzj_test where uid&gt;(select uid from (select top 200000 uid from jzj.dbo.jzj_test where uid&gt;83222300 and uid&lt;=84598500) a except select uid from (select top 199999 uid from jzj.dbo.jzj_test where uid&gt;83222300 and uid&lt;=84598500) a) and uid &lt;=84598500 and (1 = 1)) sqoop_import_query_alias\n07-11-2017 16:42:28 CST run_dataimport INFO - 17/11/07 16:42:28 INFO tool.ImportTool: Incremental import based on column uid\n07-11-2017 16:42:28 CST run_dataimport INFO - 17/11/07 16:42:28 INFO tool.ImportTool: Upper bound value: 83622300\n07-11-2017 16:42:28 CST run_dataimport INFO - 17/11/07 16:42:28 INFO mapreduce.ImportJobBase: Beginning query import.\n07-11-2017 16:42:28 CST run_dataimport INFO - 17/11/07 16:42:28 INFO Configuration.deprecation: mapred.jar is deprecated. Instead, use mapreduce.job.jar\n07-11-2017 16:42:28 CST run_dataimport INFO - 17/11/07 16:42:28 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n07-11-2017 16:42:28 CST run_dataimport INFO - 17/11/07 16:42:28 INFO client.RMProxy: Connecting to ResourceManager at master/172.23.27.203:8032\n07-11-2017 16:42:34 CST run_dataimport INFO - 17/11/07 16:42:34 INFO db.DBInputFormat: Using read commited transaction isolation\n07-11-2017 16:42:34 CST run_dataimport INFO - 17/11/07 16:42:34 INFO db.DataDrivenDBInputFormat: BoundingValsQuery: SELECT MIN(uid), MAX(uid) FROM (select top 200000 * from jzj.dbo.jzj_test where uid&gt;(select uid from (select top 200000 uid from jzj.dbo.jzj_test where uid&gt;83222300 and uid&lt;=84598500) a except select uid from (select top 199999 uid from jzj.dbo.jzj_test where uid&gt;83222300 and uid&lt;=84598500) a) and uid &lt;=84598500 and uid &lt;= 83622300 AND  (1 = 1) ) AS t1\n07-11-2017 17:22:49 CST run_dataimport INFO - 17/11/07 17:22:49 INFO mapreduce.JobSubmitter: number of splits:4\n07-11-2017 17:22:49 CST run_dataimport INFO - 17/11/07 17:22:49 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1508671365448_2955\n07-11-2017 17:22:49 CST run_dataimport INFO - 17/11/07 17:22:49 INFO impl.YarnClientImpl: Submitted application application_1508671365448_2955\n07-11-2017 17:22:49 CST run_dataimport INFO - 17/11/07 17:22:49 INFO mapreduce.Job: The url to track the job: http://master:8088/proxy/application_1508671365448_2955/\n07-11-2017 17:22:49 CST run_dataimport INFO - 17/11/07 17:22:49 INFO mapreduce.Job: Running job: job_1508671365448_2955\n07-11-2017 17:22:57 CST run_dataimport INFO - 17/11/07 17:22:57 INFO mapreduce.Job: Job job_1508671365448_2955 running in uber mode : false\n07-11-2017 17:22:57 CST run_dataimport INFO - 17/11/07 17:22:57 INFO mapreduce.Job:  map 0% reduce 0%\n07-11-2017 18:02:56 CST run_dataimport INFO - 17/11/07 18:02:56 INFO mapreduce.Job:  map 25% reduce 0%\n07-11-2017 18:03:23 CST run_dataimport INFO - 17/11/07 18:03:23 INFO mapreduce.Job:  map 50% reduce 0%\n07-11-2017 18:03:46 CST run_dataimport INFO - 17/11/07 18:03:46 INFO mapreduce.Job:  map 75% reduce 0%\n07-11-2017 18:03:47 CST run_dataimport INFO - 17/11/07 18:03:47 INFO mapreduce.Job:  map 100% reduce 0%\n07-11-2017 18:03:48 CST run_dataimport INFO - 17/11/07 18:03:48 INFO mapreduce.Job: Job job_1508671365448_2955 completed successfully\n07-11-2017 18:03:48 CST run_dataimport INFO - 17/11/07 18:03:48 INFO mapreduce.Job: Counters: 30\n07-11-2017 18:03:48 CST run_dataimport INFO - \tFile System Counters\n07-11-2017 18:03:48 CST run_dataimport INFO - \t\tFILE: Number of bytes read=0\n07-11-2017 18:03:48 CST run_dataimport INFO - \t\tFILE: Number of bytes written=588156\n07-11-2017 18:03:48 CST run_dataimport INFO - \t\tFILE: Number of read operations=0\n07-11-2017 18:03:48 CST run_dataimport INFO - \t\tFILE: Number of large read operations=0\n07-11-2017 18:03:48 CST run_dataimport INFO - \t\tFILE: Number of write operations=0\n07-11-2017 18:03:48 CST run_dataimport INFO - \t\tHDFS: Number of bytes read=462\n07-11-2017 18:03:48 CST run_dataimport INFO - \t\tHDFS: Number of bytes written=93800000\n07-11-2017 18:03:48 CST run_dataimport INFO - \t\tHDFS: Number of read operations=16\n07-11-2017 18:03:48 CST run_dataimport INFO - \t\tHDFS: Number of large read operations=0\n07-11-2017 18:03:48 CST run_dataimport INFO - \t\tHDFS: Number of write operations=8\n07-11-2017 18:03:48 CST run_dataimport INFO - \tJob Counters \n07-11-2017 18:03:48 CST run_dataimport INFO - \t\tLaunched map tasks=4\n07-11-2017 18:03:48 CST run_dataimport INFO - \t\tOther local map tasks=4\n07-11-2017 18:03:48 CST run_dataimport INFO - \t\tTotal time spent by all maps in occupied slots (ms)=9712787\n07-11-2017 18:03:48 CST run_dataimport INFO - \t\tTotal time spent by all reduces in occupied slots (ms)=0\n07-11-2017 18:03:48 CST run_dataimport INFO - \t\tTotal time spent by all map tasks (ms)=9712787\n07-11-2017 18:03:48 CST run_dataimport INFO - \t\tTotal vcore-seconds taken by all map tasks=9712787\n07-11-2017 18:03:48 CST run_dataimport INFO - \t\tTotal megabyte-seconds taken by all map tasks=9945893888\n07-11-2017 18:03:48 CST run_dataimport INFO - \tMap-Reduce Framework\n07-11-2017 18:03:48 CST run_dataimport INFO - \t\tMap input records=200000\n07-11-2017 18:03:48 CST run_dataimport INFO - \t\tMap output records=200000\n07-11-2017 18:03:48 CST run_dataimport INFO - \t\tInput split bytes=462\n07-11-2017 18:03:48 CST run_dataimport INFO - \t\tSpilled Records=0\n07-11-2017 18:03:48 CST run_dataimport INFO - \t\tFailed Shuffles=0\n07-11-2017 18:03:48 CST run_dataimport INFO - \t\tMerged Map outputs=0\n07-11-2017 18:03:48 CST run_dataimport INFO - \t\tGC time elapsed (ms)=764\n07-11-2017 18:03:48 CST run_dataimport INFO - \t\tCPU time spent (ms)=64470\n07-11-2017 18:03:48 CST run_dataimport INFO - \t\tPhysical memory (bytes) snapshot=1473638400\n07-11-2017 18:03:48 CST run_dataimport INFO - \t\tVirtual memory (bytes) snapshot=6498091008\n07-11-2017 18:03:48 CST run_dataimport INFO - \t\tTotal committed heap usage (bytes)=3240099840\n07-11-2017 18:03:48 CST run_dataimport INFO - \tFile Input Format Counters \n07-11-2017 18:03:48 CST run_dataimport INFO - \t\tBytes Read=0\n07-11-2017 18:03:48 CST run_dataimport INFO - \tFile Output Format Counters \n07-11-2017 18:03:48 CST run_dataimport INFO - \t\tBytes Written=93800000\n07-11-2017 18:03:48 CST run_dataimport INFO - 17/11/07 18:03:48 INFO mapreduce.ImportJobBase: Transferred 89.4547 MB in 4,879.8256 seconds (18.7715 KB/sec)\n07-11-2017 18:03:48 CST run_dataimport INFO - 17/11/07 18:03:48 INFO mapreduce.ImportJobBase: Retrieved 200000 records.\n07-11-2017 18:03:48 CST run_dataimport INFO - 17/11/07 18:03:48 INFO util.AppendUtils: Appending to directory sql_to_hive\n07-11-2017 18:03:48 CST run_dataimport INFO - 17/11/07 18:03:48 INFO util.AppendUtils: Using found partition 6804\n07-11-2017 18:03:49 CST run_dataimport INFO - 17/11/07 18:03:49 INFO tool.ImportTool: Incremental import complete! To run another incremental import of all data following this import, supply the following arguments:\n07-11-2017 18:03:49 CST run_dataimport INFO - 17/11/07 18:03:49 INFO tool.ImportTool:  --incremental append\n07-11-2017 18:03:49 CST run_dataimport INFO - 17/11/07 18:03:49 INFO tool.ImportTool:   --check-column uid\n07-11-2017 18:03:49 CST run_dataimport INFO - 17/11/07 18:03:49 INFO tool.ImportTool:   --last-value 83622300\n07-11-2017 18:03:49 CST run_dataimport INFO - 17/11/07 18:03:49 INFO tool.ImportTool: (Consider saving this with 'sqoop job --create')\n07-11-2017 18:03:49 CST run_dataimport INFO - 5\n07-11-2017 18:03:49 CST run_dataimport INFO - Warning: /opt/cloudera/parcels/CDH-5.8.0-1.cdh5.8.0.p0.42/bin/../lib/sqoop/../accumulo does not exist! Accumulo imports will fail.\n07-11-2017 18:03:49 CST run_dataimport INFO - Please set $ACCUMULO_HOME to the root of your Accumulo installation.\n07-11-2017 18:03:51 CST run_dataimport INFO - 17/11/07 18:03:51 INFO sqoop.Sqoop: Running Sqoop version: 1.4.6-cdh5.8.0\n07-11-2017 18:03:51 CST run_dataimport INFO - 17/11/07 18:03:51 WARN sqoop.ConnFactory: Parameter --driver is set to an explicit driver however appropriate connection manager is not being set (via --connection-manager). Sqoop is going to fall back to org.apache.sqoop.manager.GenericJdbcManager. Please specify explicitly which connection manager should be used next time.\n07-11-2017 18:03:51 CST run_dataimport INFO - 17/11/07 18:03:51 INFO manager.SqlManager: Using default fetchSize of 1000\n07-11-2017 18:03:51 CST run_dataimport INFO - 17/11/07 18:03:51 INFO tool.CodeGenTool: Beginning code generation\n07-11-2017 18:03:52 CST run_dataimport INFO - 17/11/07 18:03:52 INFO manager.SqlManager: Executing SQL statement: select top 200000 * from jzj.dbo.jzj_test where uid&gt;(select uid from (select top 400000 uid from jzj.dbo.jzj_test where uid&gt;83222300 and uid&lt;=84598500) a except select uid from (select top 399999 uid from jzj.dbo.jzj_test where uid&gt;83222300 and uid&lt;=84598500) a) and uid &lt;=84598500 and  (1 = 0) \n07-11-2017 18:03:52 CST run_dataimport INFO - 17/11/07 18:03:52 INFO manager.SqlManager: Executing SQL statement: select top 200000 * from jzj.dbo.jzj_test where uid&gt;(select uid from (select top 400000 uid from jzj.dbo.jzj_test where uid&gt;83222300 and uid&lt;=84598500) a except select uid from (select top 399999 uid from jzj.dbo.jzj_test where uid&gt;83222300 and uid&lt;=84598500) a) and uid &lt;=84598500 and  (1 = 0) \n07-11-2017 18:03:52 CST run_dataimport INFO - 17/11/07 18:03:52 INFO orm.CompilationManager: HADOOP_MAPRED_HOME is /opt/cloudera/parcels/CDH/lib/hadoop-mapreduce\n07-11-2017 18:03:54 CST run_dataimport INFO - Note: /tmp/sqoop-root/compile/21f23a42adb9bf71452da9a3c7187b06/QueryResult.java uses or overrides a deprecated API.\n07-11-2017 18:03:54 CST run_dataimport INFO - Note: Recompile with -Xlint:deprecation for details.\n07-11-2017 18:03:54 CST run_dataimport INFO - 17/11/07 18:03:54 INFO orm.CompilationManager: Writing jar file: /tmp/sqoop-root/compile/21f23a42adb9bf71452da9a3c7187b06/QueryResult.jar\n07-11-2017 18:03:55 CST run_dataimport INFO - 17/11/07 18:03:55 INFO tool.ImportTool: Maximal id query for free form incremental import: SELECT MAX(uid) FROM (select top 200000 * from jzj.dbo.jzj_test where uid&gt;(select uid from (select top 400000 uid from jzj.dbo.jzj_test where uid&gt;83222300 and uid&lt;=84598500) a except select uid from (select top 399999 uid from jzj.dbo.jzj_test where uid&gt;83222300 and uid&lt;=84598500) a) and uid &lt;=84598500 and (1 = 1)) sqoop_import_query_alias\n07-11-2017 20:43:27 CST run_dataimport INFO - 17/11/07 20:43:27 INFO tool.ImportTool: Incremental import based on column uid\n07-11-2017 20:43:27 CST run_dataimport INFO - 17/11/07 20:43:27 INFO tool.ImportTool: Upper bound value: 83822300\n07-11-2017 20:43:27 CST run_dataimport INFO - 17/11/07 20:43:27 INFO mapreduce.ImportJobBase: Beginning query import.\n07-11-2017 20:43:27 CST run_dataimport INFO - 17/11/07 20:43:27 INFO Configuration.deprecation: mapred.jar is deprecated. Instead, use mapreduce.job.jar\n07-11-2017 20:43:27 CST run_dataimport INFO - 17/11/07 20:43:27 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n07-11-2017 20:43:28 CST run_dataimport INFO - 17/11/07 20:43:28 INFO client.RMProxy: Connecting to ResourceManager at master/172.23.27.203:8032\n07-11-2017 20:43:34 CST run_dataimport INFO - 17/11/07 20:43:34 INFO db.DBInputFormat: Using read commited transaction isolation\n07-11-2017 20:43:34 CST run_dataimport INFO - 17/11/07 20:43:34 INFO db.DataDrivenDBInputFormat: BoundingValsQuery: SELECT MIN(uid), MAX(uid) FROM (select top 200000 * from jzj.dbo.jzj_test where uid&gt;(select uid from (select top 400000 uid from jzj.dbo.jzj_test where uid&gt;83222300 and uid&lt;=84598500) a except select uid from (select top 399999 uid from jzj.dbo.jzj_test where uid&gt;83222300 and uid&lt;=84598500) a) and uid &lt;=84598500 and uid &lt;= 83822300 AND  (1 = 1) ) AS t1\n07-11-2017 23:22:41 CST run_dataimport INFO - 17/11/07 23:22:41 INFO mapreduce.JobSubmitter: number of splits:4\n07-11-2017 23:22:41 CST run_dataimport INFO - 17/11/07 23:22:41 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1508671365448_2956\n07-11-2017 23:22:42 CST run_dataimport INFO - 17/11/07 23:22:42 INFO impl.YarnClientImpl: Submitted application application_1508671365448_2956\n07-11-2017 23:22:42 CST run_dataimport INFO - 17/11/07 23:22:42 INFO mapreduce.Job: The url to track the job: http://master:8088/proxy/application_1508671365448_2956/\n07-11-2017 23:22:42 CST run_dataimport INFO - 17/11/07 23:22:42 INFO mapreduce.Job: Running job: job_1508671365448_2956\n07-11-2017 23:22:49 CST run_dataimport INFO - 17/11/07 23:22:49 INFO mapreduce.Job: Job job_1508671365448_2956 running in uber mode : false\n07-11-2017 23:22:49 CST run_dataimport INFO - 17/11/07 23:22:49 INFO mapreduce.Job:  map 0% reduce 0%\n08-11-2017 02:00:13 CST run_dataimport INFO - 17/11/08 02:00:13 INFO mapreduce.Job:  map 25% reduce 0%\n08-11-2017 02:01:28 CST run_dataimport INFO - 17/11/08 02:01:28 INFO mapreduce.Job:  map 50% reduce 0%\n08-11-2017 02:03:21 CST run_dataimport INFO - 17/11/08 02:03:21 INFO mapreduce.Job:  map 75% reduce 0%\n08-11-2017 02:03:22 CST run_dataimport INFO - 17/11/08 02:03:22 INFO mapreduce.Job:  map 100% reduce 0%\n08-11-2017 02:03:23 CST run_dataimport INFO - 17/11/08 02:03:23 INFO mapreduce.Job: Job job_1508671365448_2956 completed successfully\n08-11-2017 02:03:23 CST run_dataimport INFO - 17/11/08 02:03:23 INFO mapreduce.Job: Counters: 30\n08-11-2017 02:03:23 CST run_dataimport INFO - \tFile System Counters\n08-11-2017 02:03:23 CST run_dataimport INFO - \t\tFILE: Number of bytes read=0\n08-11-2017 02:03:23 CST run_dataimport INFO - \t\tFILE: Number of bytes written=588300\n08-11-2017 02:03:23 CST run_dataimport INFO - \t\tFILE: Number of read operations=0\n08-11-2017 02:03:23 CST run_dataimport INFO - \t\tFILE: Number of large read operations=0\n08-11-2017 02:03:23 CST run_dataimport INFO - \t\tFILE: Number of write operations=0\n08-11-2017 02:03:23 CST run_dataimport INFO - \t\tHDFS: Number of bytes read=462\n08-11-2017 02:03:23 CST run_dataimport INFO - \t\tHDFS: Number of bytes written=93800000\n08-11-2017 02:03:23 CST run_dataimport INFO - \t\tHDFS: Number of read operations=16\n08-11-2017 02:03:23 CST run_dataimport INFO - \t\tHDFS: Number of large read operations=0\n08-11-2017 02:03:23 CST run_dataimport INFO - \t\tHDFS: Number of write operations=8\n08-11-2017 02:03:23 CST run_dataimport INFO - \tJob Counters \n08-11-2017 02:03:23 CST run_dataimport INFO - \t\tLaunched map tasks=4\n08-11-2017 02:03:23 CST run_dataimport INFO - \t\tOther local map tasks=4\n08-11-2017 02:03:23 CST run_dataimport INFO - \t\tTotal time spent by all maps in occupied slots (ms)=38216711\n08-11-2017 02:03:23 CST run_dataimport INFO - \t\tTotal time spent by all reduces in occupied slots (ms)=0\n08-11-2017 02:03:23 CST run_dataimport INFO - \t\tTotal time spent by all map tasks (ms)=38216711\n08-11-2017 02:03:23 CST run_dataimport INFO - \t\tTotal vcore-seconds taken by all map tasks=38216711\n08-11-2017 02:03:23 CST run_dataimport INFO - \t\tTotal megabyte-seconds taken by all map tasks=39133912064\n08-11-2017 02:03:23 CST run_dataimport INFO - \tMap-Reduce Framework\n08-11-2017 02:03:23 CST run_dataimport INFO - \t\tMap input records=200000\n08-11-2017 02:03:23 CST run_dataimport INFO - \t\tMap output records=200000\n08-11-2017 02:03:23 CST run_dataimport INFO - \t\tInput split bytes=462\n08-11-2017 02:03:23 CST run_dataimport INFO - \t\tSpilled Records=0\n08-11-2017 02:03:23 CST run_dataimport INFO - \t\tFailed Shuffles=0\n08-11-2017 02:03:23 CST run_dataimport INFO - \t\tMerged Map outputs=0\n08-11-2017 02:03:23 CST run_dataimport INFO - \t\tGC time elapsed (ms)=1588\n08-11-2017 02:03:23 CST run_dataimport INFO - \t\tCPU time spent (ms)=134830\n08-11-2017 02:03:23 CST run_dataimport INFO - \t\tPhysical memory (bytes) snapshot=1547542528\n08-11-2017 02:03:23 CST run_dataimport INFO - \t\tVirtual memory (bytes) snapshot=6530818048\n08-11-2017 02:03:23 CST run_dataimport INFO - \t\tTotal committed heap usage (bytes)=3172990976\n08-11-2017 02:03:23 CST run_dataimport INFO - \tFile Input Format Counters \n08-11-2017 02:03:23 CST run_dataimport INFO - \t\tBytes Read=0\n08-11-2017 02:03:23 CST run_dataimport INFO - \tFile Output Format Counters \n08-11-2017 02:03:23 CST run_dataimport INFO - \t\tBytes Written=93800000\n08-11-2017 02:03:23 CST run_dataimport INFO - 17/11/08 02:03:23 INFO mapreduce.ImportJobBase: Transferred 89.4547 MB in 19,195.7726 seconds (4.772 KB/sec)\n08-11-2017 02:03:23 CST run_dataimport INFO - 17/11/08 02:03:23 INFO mapreduce.ImportJobBase: Retrieved 200000 records.\n08-11-2017 02:03:23 CST run_dataimport INFO - 17/11/08 02:03:23 INFO util.AppendUtils: Appending to directory sql_to_hive\n08-11-2017 02:03:24 CST run_dataimport INFO - 17/11/08 02:03:24 INFO util.AppendUtils: Using found partition 6808\n08-11-2017 02:03:24 CST run_dataimport INFO - 17/11/08 02:03:24 INFO tool.ImportTool: Incremental import complete! To run another incremental import of all data following this import, supply the following arguments:\n08-11-2017 02:03:24 CST run_dataimport INFO - 17/11/08 02:03:24 INFO tool.ImportTool:  --incremental append\n08-11-2017 02:03:24 CST run_dataimport INFO - 17/11/08 02:03:24 INFO tool.ImportTool:   --check-column uid\n08-11-2017 02:03:24 CST run_dataimport INFO - 17/11/08 02:03:24 INFO tool.ImportTool:   --last-value 83822300\n08-11-2017 02:03:24 CST run_dataimport INFO - 17/11/08 02:03:24 INFO tool.ImportTool: (Consider saving this with 'sqoop job --create')\n08-11-2017 02:03:24 CST run_dataimport INFO - 4\n08-11-2017 02:03:24 CST run_dataimport INFO - Warning: /opt/cloudera/parcels/CDH-5.8.0-1.cdh5.8.0.p0.42/bin/../lib/sqoop/../accumulo does not exist! Accumulo imports will fail.\n08-11-2017 02:03:24 CST run_dataimport INFO - Please set $ACCUMULO_HOME to the root of your Accumulo installation.\n08-11-2017 02:03:26 CST run_dataimport INFO - 17/11/08 02:03:26 INFO sqoop.Sqoop: Running Sqoop version: 1.4.6-cdh5.8.0\n08-11-2017 02:03:26 CST run_dataimport INFO - 17/11/08 02:03:26 WARN sqoop.ConnFactory: Parameter --driver is set to an explicit driver however appropriate connection manager is not being set (via --connection-manager). Sqoop is going to fall back to org.apache.sqoop.manager.GenericJdbcManager. Please specify explicitly which connection manager should be used next time.\n08-11-2017 02:03:26 CST run_dataimport INFO - 17/11/08 02:03:26 INFO manager.SqlManager: Using default fetchSize of 1000\n08-11-2017 02:03:26 CST run_dataimport INFO - 17/11/08 02:03:26 INFO tool.CodeGenTool: Beginning code generation\n08-11-2017 02:03:27 CST run_dataimport INFO - 17/11/08 02:03:27 INFO manager.SqlManager: Executing SQL statement: select top 200000 * from jzj.dbo.jzj_test where uid&gt;(select uid from (select top 600000 uid from jzj.dbo.jzj_test where uid&gt;83222300 and uid&lt;=84598500) a except select uid from (select top 599999 uid from jzj.dbo.jzj_test where uid&gt;83222300 and uid&lt;=84598500) a) and uid &lt;=84598500 and  (1 = 0) \n08-11-2017 02:03:27 CST run_dataimport INFO - 17/11/08 02:03:27 INFO manager.SqlManager: Executing SQL statement: select top 200000 * from jzj.dbo.jzj_test where uid&gt;(select uid from (select top 600000 uid from jzj.dbo.jzj_test where uid&gt;83222300 and uid&lt;=84598500) a except select uid from (select top 599999 uid from jzj.dbo.jzj_test where uid&gt;83222300 and uid&lt;=84598500) a) and uid &lt;=84598500 and  (1 = 0) \n08-11-2017 02:03:27 CST run_dataimport INFO - 17/11/08 02:03:27 INFO orm.CompilationManager: HADOOP_MAPRED_HOME is /opt/cloudera/parcels/CDH/lib/hadoop-mapreduce\n08-11-2017 02:03:29 CST run_dataimport INFO - Note: /tmp/sqoop-root/compile/e8acb4e9422617ba257a228b6dfa0561/QueryResult.java uses or overrides a deprecated API.\n08-11-2017 02:03:29 CST run_dataimport INFO - Note: Recompile with -Xlint:deprecation for details.\n08-11-2017 02:03:29 CST run_dataimport INFO - 17/11/08 02:03:29 INFO orm.CompilationManager: Writing jar file: /tmp/sqoop-root/compile/e8acb4e9422617ba257a228b6dfa0561/QueryResult.jar\n08-11-2017 02:03:30 CST run_dataimport INFO - 17/11/08 02:03:30 INFO tool.ImportTool: Maximal id query for free form incremental import: SELECT MAX(uid) FROM (select top 200000 * from jzj.dbo.jzj_test where uid&gt;(select uid from (select top 600000 uid from jzj.dbo.jzj_test where uid&gt;83222300 and uid&lt;=84598500) a except select uid from (select top 599999 uid from jzj.dbo.jzj_test where uid&gt;83222300 and uid&lt;=84598500) a) and uid &lt;=84598500 and (1 = 1)) sqoop_import_query_alias\n08-11-2017 07:55:53 CST run_dataimport INFO - 17/11/08 07:55:53 INFO tool.ImportTool: Incremental import based on column uid\n08-11-2017 07:55:53 CST run_dataimport INFO - 17/11/08 07:55:53 INFO tool.ImportTool: Upper bound value: 84022300\n08-11-2017 07:55:53 CST run_dataimport INFO - 17/11/08 07:55:53 INFO mapreduce.ImportJobBase: Beginning query import.\n08-11-2017 07:55:53 CST run_dataimport INFO - 17/11/08 07:55:53 INFO Configuration.deprecation: mapred.jar is deprecated. Instead, use mapreduce.job.jar\n08-11-2017 07:55:53 CST run_dataimport INFO - 17/11/08 07:55:53 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n08-11-2017 07:55:53 CST run_dataimport INFO - 17/11/08 07:55:53 INFO client.RMProxy: Connecting to ResourceManager at master/172.23.27.203:8032\n08-11-2017 07:56:00 CST run_dataimport INFO - 17/11/08 07:56:00 INFO db.DBInputFormat: Using read commited transaction isolation\n08-11-2017 07:56:00 CST run_dataimport INFO - 17/11/08 07:56:00 INFO db.DataDrivenDBInputFormat: BoundingValsQuery: SELECT MIN(uid), MAX(uid) FROM (select top 200000 * from jzj.dbo.jzj_test where uid&gt;(select uid from (select top 600000 uid from jzj.dbo.jzj_test where uid&gt;83222300 and uid&lt;=84598500) a except select uid from (select top 599999 uid from jzj.dbo.jzj_test where uid&gt;83222300 and uid&lt;=84598500) a) and uid &lt;=84598500 and uid &lt;= 84022300 AND  (1 = 1) ) AS t1\n08-11-2017 12:11:01 CST run_dataimport ERROR - Kill has been called.\n08-11-2017 12:11:06 CST run_dataimport INFO - Process completed unsuccessfully in 72615 seconds.\n08-11-2017 12:11:06 CST run_dataimport ERROR - Job run killed!\njava.lang.RuntimeException: azkaban.jobExecutor.utils.process.ProcessFailureException\n\tat azkaban.jobExecutor.ProcessJob.run(ProcessJob.java:297)\n\tat azkaban.execapp.JobRunner.runJob(JobRunner.java:748)\n\tat azkaban.execapp.JobRunner.doRun(JobRunner.java:591)\n\tat azkaban.execapp.JobRunner.run(JobRunner.java:552)\n\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: azkaban.jobExecutor.utils.process.ProcessFailureException\n\tat azkaban.jobExecutor.utils.process.AzkabanProcess.run(AzkabanProcess.java:130)\n\tat azkaban.jobExecutor.ProcessJob.run(ProcessJob.java:289)\n\t... 8 more\n08-11-2017 12:11:06 CST run_dataimport ERROR - azkaban.jobExecutor.utils.process.ProcessFailureException cause: azkaban.jobExecutor.utils.process.ProcessFailureException\n08-11-2017 12:11:06 CST run_dataimport INFO - Finishing job run_dataimport at 1510114266951 with status KILLED\n", 'length': 37422}
[2017-11-20 20:54:04,925] app.hive_mysql.AzkabanMonitor [INFO] URL: https://172.23.27.203:8443
[2017-11-20 20:54:05,102] app.hive_mysql.AzkabanMonitor [INFO] Success: Login https://172.23.27.203:8443 with user azkaban
[2017-11-20 20:54:05,103] app.hive_mysql.AzkabanMonitor [DEBUG] {'session.id': '4e5a6b1a-95ca-4f09-8ce2-a6d43220e31d', 'status': 'success'}
[2017-11-20 20:54:05,126] app.hive_mysql.AzkabanMonitor [INFO] Success: Fetch Executions of a Flow - run_dataimport
[2017-11-20 20:54:05,127] app.hive_mysql.AzkabanMonitor [DEBUG] {'total': 16, 'executions': [{'submitTime': 1510041651107, 'submitUser': 'azkaban', 'startTime': 1510041651277, 'endTime': 1510114267057, 'flowId': 'run_dataimport', 'projectId': 68, 'execId': 1670, 'status': 'KILLED'}, {'submitTime': 1510027251078, 'submitUser': 'azkaban', 'startTime': 1510027251395, 'endTime': 1510036906931, 'flowId': 'run_dataimport', 'projectId': 68, 'execId': 1669, 'status': 'SUCCEEDED'}, {'submitTime': 1509897650826, 'submitUser': 'azkaban', 'startTime': 1509897651116, 'endTime': 1510026870568, 'flowId': 'run_dataimport', 'projectId': 68, 'execId': 1668, 'status': 'SUCCEEDED'}, {'submitTime': 1509883250800, 'submitUser': 'azkaban', 'startTime': 1509883251056, 'endTime': 1509892870422, 'flowId': 'run_dataimport', 'projectId': 68, 'execId': 1667, 'status': 'SUCCEEDED'}, {'submitTime': 1509753650545, 'submitUser': 'azkaban', 'startTime': 1509753650705, 'endTime': 1509881095432, 'flowId': 'run_dataimport', 'projectId': 68, 'execId': 1666, 'status': 'SUCCEEDED'}, {'submitTime': 1509746450529, 'submitUser': 'azkaban', 'startTime': 1509746450965, 'endTime': 1509753076918, 'flowId': 'run_dataimport', 'projectId': 68, 'execId': 1665, 'status': 'SUCCEEDED'}, {'submitTime': 1509652850362, 'submitUser': 'azkaban', 'startTime': 1509652850560, 'endTime': 1509744409191, 'flowId': 'run_dataimport', 'projectId': 68, 'execId': 1664, 'status': 'SUCCEEDED'}, {'submitTime': 1509645650349, 'submitUser': 'azkaban', 'startTime': 1509645650498, 'endTime': 1509651159750, 'flowId': 'run_dataimport', 'projectId': 68, 'execId': 1663, 'status': 'SUCCEEDED'}, {'submitTime': 1509566450178, 'submitUser': 'azkaban', 'startTime': 1509566450488, 'endTime': 1509642929867, 'flowId': 'run_dataimport', 'projectId': 68, 'execId': 1661, 'status': 'SUCCEEDED'}, {'submitTime': 1509559250163, 'submitUser': 'azkaban', 'startTime': 1509559250321, 'endTime': 1509563186239, 'flowId': 'run_dataimport', 'projectId': 68, 'execId': 1660, 'status': 'SUCCEEDED'}], 'length': 10, 'project': 'sql_to_hive', 'from': 0, 'projectId': 68, 'flow': 'run_dataimport'}
[2017-11-20 20:54:05,141] app.hive_mysql.AzkabanMonitor [INFO] Success: Fetch Execution Job Logs - 1670-run_dataimport
[2017-11-20 20:54:05,141] app.hive_mysql.AzkabanMonitor [DEBUG] {'offset': 0, 'data': "07-11-2017 16:00:51 CST run_dataimport INFO - Starting job run_dataimport at 1510041651281\n07-11-2017 16:00:51 CST run_dataimport INFO - azkaban.webserver.url property was not set\n07-11-2017 16:00:51 CST run_dataimport INFO - job JVM args: -Dazkaban.flowid=run_dataimport -Dazkaban.execid=1670 -Dazkaban.jobid=run_dataimport\n07-11-2017 16:00:51 CST run_dataimport INFO - Building command job executor. \n07-11-2017 16:00:51 CST run_dataimport INFO - Memory granted for job run_dataimport\n07-11-2017 16:00:51 CST run_dataimport INFO - 1 commands to execute.\n07-11-2017 16:00:51 CST run_dataimport INFO - cwd=/home/azkaban/azkaban_bk/azkaban-exec-server-3.35.0-22-g5517d50/executions/1670\n07-11-2017 16:00:51 CST run_dataimport INFO - effective user is: azkaban\n07-11-2017 16:00:51 CST run_dataimport INFO - Command: sh /home/sql_to_hive/dataimport.sh\n07-11-2017 16:00:51 CST run_dataimport INFO - Environment variables: {JOB_OUTPUT_PROP_FILE=/home/azkaban/azkaban_bk/azkaban-exec-server-3.35.0-22-g5517d50/executions/1670/run_dataimport_output_8807970133616151260_tmp, JOB_PROP_FILE=/home/azkaban/azkaban_bk/azkaban-exec-server-3.35.0-22-g5517d50/executions/1670/run_dataimport_props_6140684690151138768_tmp, KRB5CCNAME=/tmp/krb5cc__sql_to_hive__run_dataimport__run_dataimport__1670__azkaban, JOB_NAME=run_dataimport}\n07-11-2017 16:00:51 CST run_dataimport INFO - Working directory: /home/azkaban/azkaban_bk/azkaban-exec-server-3.35.0-22-g5517d50/executions/1670\n07-11-2017 16:00:51 CST run_dataimport INFO - log4j:WARN No appenders could be found for logger (org.apache.hive.jdbc.Utils).\n07-11-2017 16:00:51 CST run_dataimport INFO - log4j:WARN Please initialize the log4j system properly.\n07-11-2017 16:00:51 CST run_dataimport INFO - log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.\n07-11-2017 16:00:51 CST run_dataimport INFO - SLF4J: Failed to load class &quot;org.slf4j.impl.StaticLoggerBinder&quot;.\n07-11-2017 16:00:51 CST run_dataimport INFO - SLF4J: Defaulting to no-operation (NOP) logger implementation\n07-11-2017 16:00:51 CST run_dataimport INFO - SLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.\n07-11-2017 16:02:05 CST run_dataimport INFO - 1,84598500,83222300,1376200sqlmax:84598500\n07-11-2017 16:02:05 CST run_dataimport INFO - hivemax:83222300\n07-11-2017 16:02:05 CST run_dataimport INFO - counts:1376200\n07-11-2017 16:02:05 CST run_dataimport INFO - 7,176200\n07-11-2017 16:02:05 CST run_dataimport INFO - 7\n07-11-2017 16:02:05 CST run_dataimport INFO - Warning: /opt/cloudera/parcels/CDH-5.8.0-1.cdh5.8.0.p0.42/bin/../lib/sqoop/../accumulo does not exist! Accumulo imports will fail.\n07-11-2017 16:02:05 CST run_dataimport INFO - Please set $ACCUMULO_HOME to the root of your Accumulo installation.\n07-11-2017 16:02:07 CST run_dataimport INFO - 17/11/07 16:02:07 INFO sqoop.Sqoop: Running Sqoop version: 1.4.6-cdh5.8.0\n07-11-2017 16:02:07 CST run_dataimport INFO - 17/11/07 16:02:07 WARN sqoop.ConnFactory: Parameter --driver is set to an explicit driver however appropriate connection manager is not being set (via --connection-manager). Sqoop is going to fall back to org.apache.sqoop.manager.GenericJdbcManager. Please specify explicitly which connection manager should be used next time.\n07-11-2017 16:02:07 CST run_dataimport INFO - 17/11/07 16:02:07 INFO manager.SqlManager: Using default fetchSize of 1000\n07-11-2017 16:02:07 CST run_dataimport INFO - 17/11/07 16:02:07 INFO tool.CodeGenTool: Beginning code generation\n07-11-2017 16:02:08 CST run_dataimport INFO - 17/11/07 16:02:08 INFO manager.SqlManager: Executing SQL statement: select top 200000 * from jzj.dbo.jzj_test where uid&gt;=(select uid from (select top 1 uid from jzj.dbo.jzj_test where uid&gt;83222300 and uid&lt;=84598500) a) and uid &lt;=84598500 and  (1 = 0) \n07-11-2017 16:02:08 CST run_dataimport INFO - 17/11/07 16:02:08 INFO manager.SqlManager: Executing SQL statement: select top 200000 * from jzj.dbo.jzj_test where uid&gt;=(select uid from (select top 1 uid from jzj.dbo.jzj_test where uid&gt;83222300 and uid&lt;=84598500) a) and uid &lt;=84598500 and  (1 = 0) \n07-11-2017 16:02:08 CST run_dataimport INFO - 17/11/07 16:02:08 INFO orm.CompilationManager: HADOOP_MAPRED_HOME is /opt/cloudera/parcels/CDH/lib/hadoop-mapreduce\n07-11-2017 16:02:10 CST run_dataimport INFO - Note: /tmp/sqoop-root/compile/b796f2c0b0198c05bdcf8149b3f9a7fd/QueryResult.java uses or overrides a deprecated API.\n07-11-2017 16:02:10 CST run_dataimport INFO - Note: Recompile with -Xlint:deprecation for details.\n07-11-2017 16:02:10 CST run_dataimport INFO - 17/11/07 16:02:10 INFO orm.CompilationManager: Writing jar file: /tmp/sqoop-root/compile/b796f2c0b0198c05bdcf8149b3f9a7fd/QueryResult.jar\n07-11-2017 16:02:11 CST run_dataimport INFO - 17/11/07 16:02:11 INFO tool.ImportTool: Maximal id query for free form incremental import: SELECT MAX(uid) FROM (select top 200000 * from jzj.dbo.jzj_test where uid&gt;=(select uid from (select top 1 uid from jzj.dbo.jzj_test where uid&gt;83222300 and uid&lt;=84598500) a) and uid &lt;=84598500 and (1 = 1)) sqoop_import_query_alias\n07-11-2017 16:02:11 CST run_dataimport INFO - 17/11/07 16:02:11 INFO tool.ImportTool: Incremental import based on column uid\n07-11-2017 16:02:11 CST run_dataimport INFO - 17/11/07 16:02:11 INFO tool.ImportTool: Upper bound value: 83422300\n07-11-2017 16:02:11 CST run_dataimport INFO - 17/11/07 16:02:11 INFO mapreduce.ImportJobBase: Beginning query import.\n07-11-2017 16:02:11 CST run_dataimport INFO - 17/11/07 16:02:11 INFO Configuration.deprecation: mapred.jar is deprecated. Instead, use mapreduce.job.jar\n07-11-2017 16:02:11 CST run_dataimport INFO - 17/11/07 16:02:11 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n07-11-2017 16:02:11 CST run_dataimport INFO - 17/11/07 16:02:11 INFO client.RMProxy: Connecting to ResourceManager at master/172.23.27.203:8032\n07-11-2017 16:02:18 CST run_dataimport INFO - 17/11/07 16:02:18 INFO db.DBInputFormat: Using read commited transaction isolation\n07-11-2017 16:02:18 CST run_dataimport INFO - 17/11/07 16:02:18 INFO db.DataDrivenDBInputFormat: BoundingValsQuery: SELECT MIN(uid), MAX(uid) FROM (select top 200000 * from jzj.dbo.jzj_test where uid&gt;=(select uid from (select top 1 uid from jzj.dbo.jzj_test where uid&gt;83222300 and uid&lt;=84598500) a) and uid &lt;=84598500 and uid &lt;= 83422300 AND  (1 = 1) ) AS t1\n07-11-2017 16:02:18 CST run_dataimport INFO - 17/11/07 16:02:18 INFO mapreduce.JobSubmitter: number of splits:4\n07-11-2017 16:02:18 CST run_dataimport INFO - 17/11/07 16:02:18 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1508671365448_2954\n07-11-2017 16:02:19 CST run_dataimport INFO - 17/11/07 16:02:19 INFO impl.YarnClientImpl: Submitted application application_1508671365448_2954\n07-11-2017 16:02:19 CST run_dataimport INFO - 17/11/07 16:02:19 INFO mapreduce.Job: The url to track the job: http://master:8088/proxy/application_1508671365448_2954/\n07-11-2017 16:02:19 CST run_dataimport INFO - 17/11/07 16:02:19 INFO mapreduce.Job: Running job: job_1508671365448_2954\n07-11-2017 16:02:26 CST run_dataimport INFO - 17/11/07 16:02:26 INFO mapreduce.Job: Job job_1508671365448_2954 running in uber mode : false\n07-11-2017 16:02:26 CST run_dataimport INFO - 17/11/07 16:02:26 INFO mapreduce.Job:  map 0% reduce 0%\n07-11-2017 16:02:37 CST run_dataimport INFO - 17/11/07 16:02:37 INFO mapreduce.Job:  map 25% reduce 0%\n07-11-2017 16:02:40 CST run_dataimport INFO - 17/11/07 16:02:40 INFO mapreduce.Job:  map 100% reduce 0%\n07-11-2017 16:02:41 CST run_dataimport INFO - 17/11/07 16:02:41 INFO mapreduce.Job: Job job_1508671365448_2954 completed successfully\n07-11-2017 16:02:41 CST run_dataimport INFO - 17/11/07 16:02:41 INFO mapreduce.Job: Counters: 30\n07-11-2017 16:02:41 CST run_dataimport INFO - \tFile System Counters\n07-11-2017 16:02:41 CST run_dataimport INFO - \t\tFILE: Number of bytes read=0\n07-11-2017 16:02:41 CST run_dataimport INFO - \t\tFILE: Number of bytes written=587024\n07-11-2017 16:02:41 CST run_dataimport INFO - \t\tFILE: Number of read operations=0\n07-11-2017 16:02:41 CST run_dataimport INFO - \t\tFILE: Number of large read operations=0\n07-11-2017 16:02:41 CST run_dataimport INFO - \t\tFILE: Number of write operations=0\n07-11-2017 16:02:41 CST run_dataimport INFO - \t\tHDFS: Number of bytes read=462\n07-11-2017 16:02:41 CST run_dataimport INFO - \t\tHDFS: Number of bytes written=93800000\n07-11-2017 16:02:41 CST run_dataimport INFO - \t\tHDFS: Number of read operations=16\n07-11-2017 16:02:41 CST run_dataimport INFO - \t\tHDFS: Number of large read operations=0\n07-11-2017 16:02:41 CST run_dataimport INFO - \t\tHDFS: Number of write operations=8\n07-11-2017 16:02:41 CST run_dataimport INFO - \tJob Counters \n07-11-2017 16:02:41 CST run_dataimport INFO - \t\tLaunched map tasks=4\n07-11-2017 16:02:41 CST run_dataimport INFO - \t\tOther local map tasks=4\n07-11-2017 16:02:41 CST run_dataimport INFO - \t\tTotal time spent by all maps in occupied slots (ms)=44342\n07-11-2017 16:02:41 CST run_dataimport INFO - \t\tTotal time spent by all reduces in occupied slots (ms)=0\n07-11-2017 16:02:41 CST run_dataimport INFO - \t\tTotal time spent by all map tasks (ms)=44342\n07-11-2017 16:02:41 CST run_dataimport INFO - \t\tTotal vcore-seconds taken by all map tasks=44342\n07-11-2017 16:02:41 CST run_dataimport INFO - \t\tTotal megabyte-seconds taken by all map tasks=45406208\n07-11-2017 16:02:41 CST run_dataimport INFO - \tMap-Reduce Framework\n07-11-2017 16:02:41 CST run_dataimport INFO - \t\tMap input records=200000\n07-11-2017 16:02:41 CST run_dataimport INFO - \t\tMap output records=200000\n07-11-2017 16:02:41 CST run_dataimport INFO - \t\tInput split bytes=462\n07-11-2017 16:02:41 CST run_dataimport INFO - \t\tSpilled Records=0\n07-11-2017 16:02:41 CST run_dataimport INFO - \t\tFailed Shuffles=0\n07-11-2017 16:02:41 CST run_dataimport INFO - \t\tMerged Map outputs=0\n07-11-2017 16:02:41 CST run_dataimport INFO - \t\tGC time elapsed (ms)=569\n07-11-2017 16:02:41 CST run_dataimport INFO - \t\tCPU time spent (ms)=36080\n07-11-2017 16:02:41 CST run_dataimport INFO - \t\tPhysical memory (bytes) snapshot=1458102272\n07-11-2017 16:02:41 CST run_dataimport INFO - \t\tVirtual memory (bytes) snapshot=6516502528\n07-11-2017 16:02:41 CST run_dataimport INFO - \t\tTotal committed heap usage (bytes)=3231186944\n07-11-2017 16:02:41 CST run_dataimport INFO - \tFile Input Format Counters \n07-11-2017 16:02:41 CST run_dataimport INFO - \t\tBytes Read=0\n07-11-2017 16:02:41 CST run_dataimport INFO - \tFile Output Format Counters \n07-11-2017 16:02:41 CST run_dataimport INFO - \t\tBytes Written=93800000\n07-11-2017 16:02:41 CST run_dataimport INFO - 17/11/07 16:02:41 INFO mapreduce.ImportJobBase: Transferred 89.4547 MB in 29.9824 seconds (2.9836 MB/sec)\n07-11-2017 16:02:41 CST run_dataimport INFO - 17/11/07 16:02:41 INFO mapreduce.ImportJobBase: Retrieved 200000 records.\n07-11-2017 16:02:41 CST run_dataimport INFO - 17/11/07 16:02:41 INFO util.AppendUtils: Appending to directory sql_to_hive\n07-11-2017 16:02:42 CST run_dataimport INFO - 17/11/07 16:02:42 INFO util.AppendUtils: Using found partition 6800\n07-11-2017 16:02:42 CST run_dataimport INFO - 17/11/07 16:02:42 INFO tool.ImportTool: Incremental import complete! To run another incremental import of all data following this import, supply the following arguments:\n07-11-2017 16:02:42 CST run_dataimport INFO - 17/11/07 16:02:42 INFO tool.ImportTool:  --incremental append\n07-11-2017 16:02:42 CST run_dataimport INFO - 17/11/07 16:02:42 INFO tool.ImportTool:   --check-column uid\n07-11-2017 16:02:42 CST run_dataimport INFO - 17/11/07 16:02:42 INFO tool.ImportTool:   --last-value 83422300\n07-11-2017 16:02:42 CST run_dataimport INFO - 17/11/07 16:02:42 INFO tool.ImportTool: (Consider saving this with 'sqoop job --create')\n07-11-2017 16:02:42 CST run_dataimport INFO - 6\n07-11-2017 16:02:42 CST run_dataimport INFO - Warning: /opt/cloudera/parcels/CDH-5.8.0-1.cdh5.8.0.p0.42/bin/../lib/sqoop/../accumulo does not exist! Accumulo imports will fail.\n07-11-2017 16:02:42 CST run_dataimport INFO - Please set $ACCUMULO_HOME to the root of your Accumulo installation.\n07-11-2017 16:02:44 CST run_dataimport INFO - 17/11/07 16:02:44 INFO sqoop.Sqoop: Running Sqoop version: 1.4.6-cdh5.8.0\n07-11-2017 16:02:44 CST run_dataimport INFO - 17/11/07 16:02:44 WARN sqoop.ConnFactory: Parameter --driver is set to an explicit driver however appropriate connection manager is not being set (via --connection-manager). Sqoop is going to fall back to org.apache.sqoop.manager.GenericJdbcManager. Please specify explicitly which connection manager should be used next time.\n07-11-2017 16:02:44 CST run_dataimport INFO - 17/11/07 16:02:44 INFO manager.SqlManager: Using default fetchSize of 1000\n07-11-2017 16:02:44 CST run_dataimport INFO - 17/11/07 16:02:44 INFO tool.CodeGenTool: Beginning code generation\n07-11-2017 16:02:45 CST run_dataimport INFO - 17/11/07 16:02:45 INFO manager.SqlManager: Executing SQL statement: select top 200000 * from jzj.dbo.jzj_test where uid&gt;(select uid from (select top 200000 uid from jzj.dbo.jzj_test where uid&gt;83222300 and uid&lt;=84598500) a except select uid from (select top 199999 uid from jzj.dbo.jzj_test where uid&gt;83222300 and uid&lt;=84598500) a) and uid &lt;=84598500 and  (1 = 0) \n07-11-2017 16:02:45 CST run_dataimport INFO - 17/11/07 16:02:45 INFO manager.SqlManager: Executing SQL statement: select top 200000 * from jzj.dbo.jzj_test where uid&gt;(select uid from (select top 200000 uid from jzj.dbo.jzj_test where uid&gt;83222300 and uid&lt;=84598500) a except select uid from (select top 199999 uid from jzj.dbo.jzj_test where uid&gt;83222300 and uid&lt;=84598500) a) and uid &lt;=84598500 and  (1 = 0) \n07-11-2017 16:02:45 CST run_dataimport INFO - 17/11/07 16:02:45 INFO orm.CompilationManager: HADOOP_MAPRED_HOME is /opt/cloudera/parcels/CDH/lib/hadoop-mapreduce\n07-11-2017 16:02:47 CST run_dataimport INFO - Note: /tmp/sqoop-root/compile/3e3fe91d38feffbabab3df86c4ddb2c4/QueryResult.java uses or overrides a deprecated API.\n07-11-2017 16:02:47 CST run_dataimport INFO - Note: Recompile with -Xlint:deprecation for details.\n07-11-2017 16:02:47 CST run_dataimport INFO - 17/11/07 16:02:47 INFO orm.CompilationManager: Writing jar file: /tmp/sqoop-root/compile/3e3fe91d38feffbabab3df86c4ddb2c4/QueryResult.jar\n07-11-2017 16:02:48 CST run_dataimport INFO - 17/11/07 16:02:48 INFO tool.ImportTool: Maximal id query for free form incremental import: SELECT MAX(uid) FROM (select top 200000 * from jzj.dbo.jzj_test where uid&gt;(select uid from (select top 200000 uid from jzj.dbo.jzj_test where uid&gt;83222300 and uid&lt;=84598500) a except select uid from (select top 199999 uid from jzj.dbo.jzj_test where uid&gt;83222300 and uid&lt;=84598500) a) and uid &lt;=84598500 and (1 = 1)) sqoop_import_query_alias\n07-11-2017 16:42:28 CST run_dataimport INFO - 17/11/07 16:42:28 INFO tool.ImportTool: Incremental import based on column uid\n07-11-2017 16:42:28 CST run_dataimport INFO - 17/11/07 16:42:28 INFO tool.ImportTool: Upper bound value: 83622300\n07-11-2017 16:42:28 CST run_dataimport INFO - 17/11/07 16:42:28 INFO mapreduce.ImportJobBase: Beginning query import.\n07-11-2017 16:42:28 CST run_dataimport INFO - 17/11/07 16:42:28 INFO Configuration.deprecation: mapred.jar is deprecated. Instead, use mapreduce.job.jar\n07-11-2017 16:42:28 CST run_dataimport INFO - 17/11/07 16:42:28 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n07-11-2017 16:42:28 CST run_dataimport INFO - 17/11/07 16:42:28 INFO client.RMProxy: Connecting to ResourceManager at master/172.23.27.203:8032\n07-11-2017 16:42:34 CST run_dataimport INFO - 17/11/07 16:42:34 INFO db.DBInputFormat: Using read commited transaction isolation\n07-11-2017 16:42:34 CST run_dataimport INFO - 17/11/07 16:42:34 INFO db.DataDrivenDBInputFormat: BoundingValsQuery: SELECT MIN(uid), MAX(uid) FROM (select top 200000 * from jzj.dbo.jzj_test where uid&gt;(select uid from (select top 200000 uid from jzj.dbo.jzj_test where uid&gt;83222300 and uid&lt;=84598500) a except select uid from (select top 199999 uid from jzj.dbo.jzj_test where uid&gt;83222300 and uid&lt;=84598500) a) and uid &lt;=84598500 and uid &lt;= 83622300 AND  (1 = 1) ) AS t1\n07-11-2017 17:22:49 CST run_dataimport INFO - 17/11/07 17:22:49 INFO mapreduce.JobSubmitter: number of splits:4\n07-11-2017 17:22:49 CST run_dataimport INFO - 17/11/07 17:22:49 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1508671365448_2955\n07-11-2017 17:22:49 CST run_dataimport INFO - 17/11/07 17:22:49 INFO impl.YarnClientImpl: Submitted application application_1508671365448_2955\n07-11-2017 17:22:49 CST run_dataimport INFO - 17/11/07 17:22:49 INFO mapreduce.Job: The url to track the job: http://master:8088/proxy/application_1508671365448_2955/\n07-11-2017 17:22:49 CST run_dataimport INFO - 17/11/07 17:22:49 INFO mapreduce.Job: Running job: job_1508671365448_2955\n07-11-2017 17:22:57 CST run_dataimport INFO - 17/11/07 17:22:57 INFO mapreduce.Job: Job job_1508671365448_2955 running in uber mode : false\n07-11-2017 17:22:57 CST run_dataimport INFO - 17/11/07 17:22:57 INFO mapreduce.Job:  map 0% reduce 0%\n07-11-2017 18:02:56 CST run_dataimport INFO - 17/11/07 18:02:56 INFO mapreduce.Job:  map 25% reduce 0%\n07-11-2017 18:03:23 CST run_dataimport INFO - 17/11/07 18:03:23 INFO mapreduce.Job:  map 50% reduce 0%\n07-11-2017 18:03:46 CST run_dataimport INFO - 17/11/07 18:03:46 INFO mapreduce.Job:  map 75% reduce 0%\n07-11-2017 18:03:47 CST run_dataimport INFO - 17/11/07 18:03:47 INFO mapreduce.Job:  map 100% reduce 0%\n07-11-2017 18:03:48 CST run_dataimport INFO - 17/11/07 18:03:48 INFO mapreduce.Job: Job job_1508671365448_2955 completed successfully\n07-11-2017 18:03:48 CST run_dataimport INFO - 17/11/07 18:03:48 INFO mapreduce.Job: Counters: 30\n07-11-2017 18:03:48 CST run_dataimport INFO - \tFile System Counters\n07-11-2017 18:03:48 CST run_dataimport INFO - \t\tFILE: Number of bytes read=0\n07-11-2017 18:03:48 CST run_dataimport INFO - \t\tFILE: Number of bytes written=588156\n07-11-2017 18:03:48 CST run_dataimport INFO - \t\tFILE: Number of read operations=0\n07-11-2017 18:03:48 CST run_dataimport INFO - \t\tFILE: Number of large read operations=0\n07-11-2017 18:03:48 CST run_dataimport INFO - \t\tFILE: Number of write operations=0\n07-11-2017 18:03:48 CST run_dataimport INFO - \t\tHDFS: Number of bytes read=462\n07-11-2017 18:03:48 CST run_dataimport INFO - \t\tHDFS: Number of bytes written=93800000\n07-11-2017 18:03:48 CST run_dataimport INFO - \t\tHDFS: Number of read operations=16\n07-11-2017 18:03:48 CST run_dataimport INFO - \t\tHDFS: Number of large read operations=0\n07-11-2017 18:03:48 CST run_dataimport INFO - \t\tHDFS: Number of write operations=8\n07-11-2017 18:03:48 CST run_dataimport INFO - \tJob Counters \n07-11-2017 18:03:48 CST run_dataimport INFO - \t\tLaunched map tasks=4\n07-11-2017 18:03:48 CST run_dataimport INFO - \t\tOther local map tasks=4\n07-11-2017 18:03:48 CST run_dataimport INFO - \t\tTotal time spent by all maps in occupied slots (ms)=9712787\n07-11-2017 18:03:48 CST run_dataimport INFO - \t\tTotal time spent by all reduces in occupied slots (ms)=0\n07-11-2017 18:03:48 CST run_dataimport INFO - \t\tTotal time spent by all map tasks (ms)=9712787\n07-11-2017 18:03:48 CST run_dataimport INFO - \t\tTotal vcore-seconds taken by all map tasks=9712787\n07-11-2017 18:03:48 CST run_dataimport INFO - \t\tTotal megabyte-seconds taken by all map tasks=9945893888\n07-11-2017 18:03:48 CST run_dataimport INFO - \tMap-Reduce Framework\n07-11-2017 18:03:48 CST run_dataimport INFO - \t\tMap input records=200000\n07-11-2017 18:03:48 CST run_dataimport INFO - \t\tMap output records=200000\n07-11-2017 18:03:48 CST run_dataimport INFO - \t\tInput split bytes=462\n07-11-2017 18:03:48 CST run_dataimport INFO - \t\tSpilled Records=0\n07-11-2017 18:03:48 CST run_dataimport INFO - \t\tFailed Shuffles=0\n07-11-2017 18:03:48 CST run_dataimport INFO - \t\tMerged Map outputs=0\n07-11-2017 18:03:48 CST run_dataimport INFO - \t\tGC time elapsed (ms)=764\n07-11-2017 18:03:48 CST run_dataimport INFO - \t\tCPU time spent (ms)=64470\n07-11-2017 18:03:48 CST run_dataimport INFO - \t\tPhysical memory (bytes) snapshot=1473638400\n07-11-2017 18:03:48 CST run_dataimport INFO - \t\tVirtual memory (bytes) snapshot=6498091008\n07-11-2017 18:03:48 CST run_dataimport INFO - \t\tTotal committed heap usage (bytes)=3240099840\n07-11-2017 18:03:48 CST run_dataimport INFO - \tFile Input Format Counters \n07-11-2017 18:03:48 CST run_dataimport INFO - \t\tBytes Read=0\n07-11-2017 18:03:48 CST run_dataimport INFO - \tFile Output Format Counters \n07-11-2017 18:03:48 CST run_dataimport INFO - \t\tBytes Written=93800000\n07-11-2017 18:03:48 CST run_dataimport INFO - 17/11/07 18:03:48 INFO mapreduce.ImportJobBase: Transferred 89.4547 MB in 4,879.8256 seconds (18.7715 KB/sec)\n07-11-2017 18:03:48 CST run_dataimport INFO - 17/11/07 18:03:48 INFO mapreduce.ImportJobBase: Retrieved 200000 records.\n07-11-2017 18:03:48 CST run_dataimport INFO - 17/11/07 18:03:48 INFO util.AppendUtils: Appending to directory sql_to_hive\n07-11-2017 18:03:48 CST run_dataimport INFO - 17/11/07 18:03:48 INFO util.AppendUtils: Using found partition 6804\n07-11-2017 18:03:49 CST run_dataimport INFO - 17/11/07 18:03:49 INFO tool.ImportTool: Incremental import complete! To run another incremental import of all data following this import, supply the following arguments:\n07-11-2017 18:03:49 CST run_dataimport INFO - 17/11/07 18:03:49 INFO tool.ImportTool:  --incremental append\n07-11-2017 18:03:49 CST run_dataimport INFO - 17/11/07 18:03:49 INFO tool.ImportTool:   --check-column uid\n07-11-2017 18:03:49 CST run_dataimport INFO - 17/11/07 18:03:49 INFO tool.ImportTool:   --last-value 83622300\n07-11-2017 18:03:49 CST run_dataimport INFO - 17/11/07 18:03:49 INFO tool.ImportTool: (Consider saving this with 'sqoop job --create')\n07-11-2017 18:03:49 CST run_dataimport INFO - 5\n07-11-2017 18:03:49 CST run_dataimport INFO - Warning: /opt/cloudera/parcels/CDH-5.8.0-1.cdh5.8.0.p0.42/bin/../lib/sqoop/../accumulo does not exist! Accumulo imports will fail.\n07-11-2017 18:03:49 CST run_dataimport INFO - Please set $ACCUMULO_HOME to the root of your Accumulo installation.\n07-11-2017 18:03:51 CST run_dataimport INFO - 17/11/07 18:03:51 INFO sqoop.Sqoop: Running Sqoop version: 1.4.6-cdh5.8.0\n07-11-2017 18:03:51 CST run_dataimport INFO - 17/11/07 18:03:51 WARN sqoop.ConnFactory: Parameter --driver is set to an explicit driver however appropriate connection manager is not being set (via --connection-manager). Sqoop is going to fall back to org.apache.sqoop.manager.GenericJdbcManager. Please specify explicitly which connection manager should be used next time.\n07-11-2017 18:03:51 CST run_dataimport INFO - 17/11/07 18:03:51 INFO manager.SqlManager: Using default fetchSize of 1000\n07-11-2017 18:03:51 CST run_dataimport INFO - 17/11/07 18:03:51 INFO tool.CodeGenTool: Beginning code generation\n07-11-2017 18:03:52 CST run_dataimport INFO - 17/11/07 18:03:52 INFO manager.SqlManager: Executing SQL statement: select top 200000 * from jzj.dbo.jzj_test where uid&gt;(select uid from (select top 400000 uid from jzj.dbo.jzj_test where uid&gt;83222300 and uid&lt;=84598500) a except select uid from (select top 399999 uid from jzj.dbo.jzj_test where uid&gt;83222300 and uid&lt;=84598500) a) and uid &lt;=84598500 and  (1 = 0) \n07-11-2017 18:03:52 CST run_dataimport INFO - 17/11/07 18:03:52 INFO manager.SqlManager: Executing SQL statement: select top 200000 * from jzj.dbo.jzj_test where uid&gt;(select uid from (select top 400000 uid from jzj.dbo.jzj_test where uid&gt;83222300 and uid&lt;=84598500) a except select uid from (select top 399999 uid from jzj.dbo.jzj_test where uid&gt;83222300 and uid&lt;=84598500) a) and uid &lt;=84598500 and  (1 = 0) \n07-11-2017 18:03:52 CST run_dataimport INFO - 17/11/07 18:03:52 INFO orm.CompilationManager: HADOOP_MAPRED_HOME is /opt/cloudera/parcels/CDH/lib/hadoop-mapreduce\n07-11-2017 18:03:54 CST run_dataimport INFO - Note: /tmp/sqoop-root/compile/21f23a42adb9bf71452da9a3c7187b06/QueryResult.java uses or overrides a deprecated API.\n07-11-2017 18:03:54 CST run_dataimport INFO - Note: Recompile with -Xlint:deprecation for details.\n07-11-2017 18:03:54 CST run_dataimport INFO - 17/11/07 18:03:54 INFO orm.CompilationManager: Writing jar file: /tmp/sqoop-root/compile/21f23a42adb9bf71452da9a3c7187b06/QueryResult.jar\n07-11-2017 18:03:55 CST run_dataimport INFO - 17/11/07 18:03:55 INFO tool.ImportTool: Maximal id query for free form incremental import: SELECT MAX(uid) FROM (select top 200000 * from jzj.dbo.jzj_test where uid&gt;(select uid from (select top 400000 uid from jzj.dbo.jzj_test where uid&gt;83222300 and uid&lt;=84598500) a except select uid from (select top 399999 uid from jzj.dbo.jzj_test where uid&gt;83222300 and uid&lt;=84598500) a) and uid &lt;=84598500 and (1 = 1)) sqoop_import_query_alias\n07-11-2017 20:43:27 CST run_dataimport INFO - 17/11/07 20:43:27 INFO tool.ImportTool: Incremental import based on column uid\n07-11-2017 20:43:27 CST run_dataimport INFO - 17/11/07 20:43:27 INFO tool.ImportTool: Upper bound value: 83822300\n07-11-2017 20:43:27 CST run_dataimport INFO - 17/11/07 20:43:27 INFO mapreduce.ImportJobBase: Beginning query import.\n07-11-2017 20:43:27 CST run_dataimport INFO - 17/11/07 20:43:27 INFO Configuration.deprecation: mapred.jar is deprecated. Instead, use mapreduce.job.jar\n07-11-2017 20:43:27 CST run_dataimport INFO - 17/11/07 20:43:27 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n07-11-2017 20:43:28 CST run_dataimport INFO - 17/11/07 20:43:28 INFO client.RMProxy: Connecting to ResourceManager at master/172.23.27.203:8032\n07-11-2017 20:43:34 CST run_dataimport INFO - 17/11/07 20:43:34 INFO db.DBInputFormat: Using read commited transaction isolation\n07-11-2017 20:43:34 CST run_dataimport INFO - 17/11/07 20:43:34 INFO db.DataDrivenDBInputFormat: BoundingValsQuery: SELECT MIN(uid), MAX(uid) FROM (select top 200000 * from jzj.dbo.jzj_test where uid&gt;(select uid from (select top 400000 uid from jzj.dbo.jzj_test where uid&gt;83222300 and uid&lt;=84598500) a except select uid from (select top 399999 uid from jzj.dbo.jzj_test where uid&gt;83222300 and uid&lt;=84598500) a) and uid &lt;=84598500 and uid &lt;= 83822300 AND  (1 = 1) ) AS t1\n07-11-2017 23:22:41 CST run_dataimport INFO - 17/11/07 23:22:41 INFO mapreduce.JobSubmitter: number of splits:4\n07-11-2017 23:22:41 CST run_dataimport INFO - 17/11/07 23:22:41 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1508671365448_2956\n07-11-2017 23:22:42 CST run_dataimport INFO - 17/11/07 23:22:42 INFO impl.YarnClientImpl: Submitted application application_1508671365448_2956\n07-11-2017 23:22:42 CST run_dataimport INFO - 17/11/07 23:22:42 INFO mapreduce.Job: The url to track the job: http://master:8088/proxy/application_1508671365448_2956/\n07-11-2017 23:22:42 CST run_dataimport INFO - 17/11/07 23:22:42 INFO mapreduce.Job: Running job: job_1508671365448_2956\n07-11-2017 23:22:49 CST run_dataimport INFO - 17/11/07 23:22:49 INFO mapreduce.Job: Job job_1508671365448_2956 running in uber mode : false\n07-11-2017 23:22:49 CST run_dataimport INFO - 17/11/07 23:22:49 INFO mapreduce.Job:  map 0% reduce 0%\n08-11-2017 02:00:13 CST run_dataimport INFO - 17/11/08 02:00:13 INFO mapreduce.Job:  map 25% reduce 0%\n08-11-2017 02:01:28 CST run_dataimport INFO - 17/11/08 02:01:28 INFO mapreduce.Job:  map 50% reduce 0%\n08-11-2017 02:03:21 CST run_dataimport INFO - 17/11/08 02:03:21 INFO mapreduce.Job:  map 75% reduce 0%\n08-11-2017 02:03:22 CST run_dataimport INFO - 17/11/08 02:03:22 INFO mapreduce.Job:  map 100% reduce 0%\n08-11-2017 02:03:23 CST run_dataimport INFO - 17/11/08 02:03:23 INFO mapreduce.Job: Job job_1508671365448_2956 completed successfully\n08-11-2017 02:03:23 CST run_dataimport INFO - 17/11/08 02:03:23 INFO mapreduce.Job: Counters: 30\n08-11-2017 02:03:23 CST run_dataimport INFO - \tFile System Counters\n08-11-2017 02:03:23 CST run_dataimport INFO - \t\tFILE: Number of bytes read=0\n08-11-2017 02:03:23 CST run_dataimport INFO - \t\tFILE: Number of bytes written=588300\n08-11-2017 02:03:23 CST run_dataimport INFO - \t\tFILE: Number of read operations=0\n08-11-2017 02:03:23 CST run_dataimport INFO - \t\tFILE: Number of large read operations=0\n08-11-2017 02:03:23 CST run_dataimport INFO - \t\tFILE: Number of write operations=0\n08-11-2017 02:03:23 CST run_dataimport INFO - \t\tHDFS: Number of bytes read=462\n08-11-2017 02:03:23 CST run_dataimport INFO - \t\tHDFS: Number of bytes written=93800000\n08-11-2017 02:03:23 CST run_dataimport INFO - \t\tHDFS: Number of read operations=16\n08-11-2017 02:03:23 CST run_dataimport INFO - \t\tHDFS: Number of large read operations=0\n08-11-2017 02:03:23 CST run_dataimport INFO - \t\tHDFS: Number of write operations=8\n08-11-2017 02:03:23 CST run_dataimport INFO - \tJob Counters \n08-11-2017 02:03:23 CST run_dataimport INFO - \t\tLaunched map tasks=4\n08-11-2017 02:03:23 CST run_dataimport INFO - \t\tOther local map tasks=4\n08-11-2017 02:03:23 CST run_dataimport INFO - \t\tTotal time spent by all maps in occupied slots (ms)=38216711\n08-11-2017 02:03:23 CST run_dataimport INFO - \t\tTotal time spent by all reduces in occupied slots (ms)=0\n08-11-2017 02:03:23 CST run_dataimport INFO - \t\tTotal time spent by all map tasks (ms)=38216711\n08-11-2017 02:03:23 CST run_dataimport INFO - \t\tTotal vcore-seconds taken by all map tasks=38216711\n08-11-2017 02:03:23 CST run_dataimport INFO - \t\tTotal megabyte-seconds taken by all map tasks=39133912064\n08-11-2017 02:03:23 CST run_dataimport INFO - \tMap-Reduce Framework\n08-11-2017 02:03:23 CST run_dataimport INFO - \t\tMap input records=200000\n08-11-2017 02:03:23 CST run_dataimport INFO - \t\tMap output records=200000\n08-11-2017 02:03:23 CST run_dataimport INFO - \t\tInput split bytes=462\n08-11-2017 02:03:23 CST run_dataimport INFO - \t\tSpilled Records=0\n08-11-2017 02:03:23 CST run_dataimport INFO - \t\tFailed Shuffles=0\n08-11-2017 02:03:23 CST run_dataimport INFO - \t\tMerged Map outputs=0\n08-11-2017 02:03:23 CST run_dataimport INFO - \t\tGC time elapsed (ms)=1588\n08-11-2017 02:03:23 CST run_dataimport INFO - \t\tCPU time spent (ms)=134830\n08-11-2017 02:03:23 CST run_dataimport INFO - \t\tPhysical memory (bytes) snapshot=1547542528\n08-11-2017 02:03:23 CST run_dataimport INFO - \t\tVirtual memory (bytes) snapshot=6530818048\n08-11-2017 02:03:23 CST run_dataimport INFO - \t\tTotal committed heap usage (bytes)=3172990976\n08-11-2017 02:03:23 CST run_dataimport INFO - \tFile Input Format Counters \n08-11-2017 02:03:23 CST run_dataimport INFO - \t\tBytes Read=0\n08-11-2017 02:03:23 CST run_dataimport INFO - \tFile Output Format Counters \n08-11-2017 02:03:23 CST run_dataimport INFO - \t\tBytes Written=93800000\n08-11-2017 02:03:23 CST run_dataimport INFO - 17/11/08 02:03:23 INFO mapreduce.ImportJobBase: Transferred 89.4547 MB in 19,195.7726 seconds (4.772 KB/sec)\n08-11-2017 02:03:23 CST run_dataimport INFO - 17/11/08 02:03:23 INFO mapreduce.ImportJobBase: Retrieved 200000 records.\n08-11-2017 02:03:23 CST run_dataimport INFO - 17/11/08 02:03:23 INFO util.AppendUtils: Appending to directory sql_to_hive\n08-11-2017 02:03:24 CST run_dataimport INFO - 17/11/08 02:03:24 INFO util.AppendUtils: Using found partition 6808\n08-11-2017 02:03:24 CST run_dataimport INFO - 17/11/08 02:03:24 INFO tool.ImportTool: Incremental import complete! To run another incremental import of all data following this import, supply the following arguments:\n08-11-2017 02:03:24 CST run_dataimport INFO - 17/11/08 02:03:24 INFO tool.ImportTool:  --incremental append\n08-11-2017 02:03:24 CST run_dataimport INFO - 17/11/08 02:03:24 INFO tool.ImportTool:   --check-column uid\n08-11-2017 02:03:24 CST run_dataimport INFO - 17/11/08 02:03:24 INFO tool.ImportTool:   --last-value 83822300\n08-11-2017 02:03:24 CST run_dataimport INFO - 17/11/08 02:03:24 INFO tool.ImportTool: (Consider saving this with 'sqoop job --create')\n08-11-2017 02:03:24 CST run_dataimport INFO - 4\n08-11-2017 02:03:24 CST run_dataimport INFO - Warning: /opt/cloudera/parcels/CDH-5.8.0-1.cdh5.8.0.p0.42/bin/../lib/sqoop/../accumulo does not exist! Accumulo imports will fail.\n08-11-2017 02:03:24 CST run_dataimport INFO - Please set $ACCUMULO_HOME to the root of your Accumulo installation.\n08-11-2017 02:03:26 CST run_dataimport INFO - 17/11/08 02:03:26 INFO sqoop.Sqoop: Running Sqoop version: 1.4.6-cdh5.8.0\n08-11-2017 02:03:26 CST run_dataimport INFO - 17/11/08 02:03:26 WARN sqoop.ConnFactory: Parameter --driver is set to an explicit driver however appropriate connection manager is not being set (via --connection-manager). Sqoop is going to fall back to org.apache.sqoop.manager.GenericJdbcManager. Please specify explicitly which connection manager should be used next time.\n08-11-2017 02:03:26 CST run_dataimport INFO - 17/11/08 02:03:26 INFO manager.SqlManager: Using default fetchSize of 1000\n08-11-2017 02:03:26 CST run_dataimport INFO - 17/11/08 02:03:26 INFO tool.CodeGenTool: Beginning code generation\n08-11-2017 02:03:27 CST run_dataimport INFO - 17/11/08 02:03:27 INFO manager.SqlManager: Executing SQL statement: select top 200000 * from jzj.dbo.jzj_test where uid&gt;(select uid from (select top 600000 uid from jzj.dbo.jzj_test where uid&gt;83222300 and uid&lt;=84598500) a except select uid from (select top 599999 uid from jzj.dbo.jzj_test where uid&gt;83222300 and uid&lt;=84598500) a) and uid &lt;=84598500 and  (1 = 0) \n08-11-2017 02:03:27 CST run_dataimport INFO - 17/11/08 02:03:27 INFO manager.SqlManager: Executing SQL statement: select top 200000 * from jzj.dbo.jzj_test where uid&gt;(select uid from (select top 600000 uid from jzj.dbo.jzj_test where uid&gt;83222300 and uid&lt;=84598500) a except select uid from (select top 599999 uid from jzj.dbo.jzj_test where uid&gt;83222300 and uid&lt;=84598500) a) and uid &lt;=84598500 and  (1 = 0) \n08-11-2017 02:03:27 CST run_dataimport INFO - 17/11/08 02:03:27 INFO orm.CompilationManager: HADOOP_MAPRED_HOME is /opt/cloudera/parcels/CDH/lib/hadoop-mapreduce\n08-11-2017 02:03:29 CST run_dataimport INFO - Note: /tmp/sqoop-root/compile/e8acb4e9422617ba257a228b6dfa0561/QueryResult.java uses or overrides a deprecated API.\n08-11-2017 02:03:29 CST run_dataimport INFO - Note: Recompile with -Xlint:deprecation for details.\n08-11-2017 02:03:29 CST run_dataimport INFO - 17/11/08 02:03:29 INFO orm.CompilationManager: Writing jar file: /tmp/sqoop-root/compile/e8acb4e9422617ba257a228b6dfa0561/QueryResult.jar\n08-11-2017 02:03:30 CST run_dataimport INFO - 17/11/08 02:03:30 INFO tool.ImportTool: Maximal id query for free form incremental import: SELECT MAX(uid) FROM (select top 200000 * from jzj.dbo.jzj_test where uid&gt;(select uid from (select top 600000 uid from jzj.dbo.jzj_test where uid&gt;83222300 and uid&lt;=84598500) a except select uid from (select top 599999 uid from jzj.dbo.jzj_test where uid&gt;83222300 and uid&lt;=84598500) a) and uid &lt;=84598500 and (1 = 1)) sqoop_import_query_alias\n08-11-2017 07:55:53 CST run_dataimport INFO - 17/11/08 07:55:53 INFO tool.ImportTool: Incremental import based on column uid\n08-11-2017 07:55:53 CST run_dataimport INFO - 17/11/08 07:55:53 INFO tool.ImportTool: Upper bound value: 84022300\n08-11-2017 07:55:53 CST run_dataimport INFO - 17/11/08 07:55:53 INFO mapreduce.ImportJobBase: Beginning query import.\n08-11-2017 07:55:53 CST run_dataimport INFO - 17/11/08 07:55:53 INFO Configuration.deprecation: mapred.jar is deprecated. Instead, use mapreduce.job.jar\n08-11-2017 07:55:53 CST run_dataimport INFO - 17/11/08 07:55:53 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n08-11-2017 07:55:53 CST run_dataimport INFO - 17/11/08 07:55:53 INFO client.RMProxy: Connecting to ResourceManager at master/172.23.27.203:8032\n08-11-2017 07:56:00 CST run_dataimport INFO - 17/11/08 07:56:00 INFO db.DBInputFormat: Using read commited transaction isolation\n08-11-2017 07:56:00 CST run_dataimport INFO - 17/11/08 07:56:00 INFO db.DataDrivenDBInputFormat: BoundingValsQuery: SELECT MIN(uid), MAX(uid) FROM (select top 200000 * from jzj.dbo.jzj_test where uid&gt;(select uid from (select top 600000 uid from jzj.dbo.jzj_test where uid&gt;83222300 and uid&lt;=84598500) a except select uid from (select top 599999 uid from jzj.dbo.jzj_test where uid&gt;83222300 and uid&lt;=84598500) a) and uid &lt;=84598500 and uid &lt;= 84022300 AND  (1 = 1) ) AS t1\n08-11-2017 12:11:01 CST run_dataimport ERROR - Kill has been called.\n08-11-2017 12:11:06 CST run_dataimport INFO - Process completed unsuccessfully in 72615 seconds.\n08-11-2017 12:11:06 CST run_dataimport ERROR - Job run killed!\njava.lang.RuntimeException: azkaban.jobExecutor.utils.process.ProcessFailureException\n\tat azkaban.jobExecutor.ProcessJob.run(ProcessJob.java:297)\n\tat azkaban.execapp.JobRunner.runJob(JobRunner.java:748)\n\tat azkaban.execapp.JobRunner.doRun(JobRunner.java:591)\n\tat azkaban.execapp.JobRunner.run(JobRunner.java:552)\n\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: azkaban.jobExecutor.utils.process.ProcessFailureException\n\tat azkaban.jobExecutor.utils.process.AzkabanProcess.run(AzkabanProcess.java:130)\n\tat azkaban.jobExecutor.ProcessJob.run(ProcessJob.java:289)\n\t... 8 more\n08-11-2017 12:11:06 CST run_dataimport ERROR - azkaban.jobExecutor.utils.process.ProcessFailureException cause: azkaban.jobExecutor.utils.process.ProcessFailureException\n08-11-2017 12:11:06 CST run_dataimport INFO - Finishing job run_dataimport at 1510114266951 with status KILLED\n", 'length': 37422}
[2017-11-20 21:00:08,916] app.hive_mysql.AzkabanMonitor [INFO] URL: https://172.23.27.203:8443
[2017-11-20 21:00:09,108] app.hive_mysql.AzkabanMonitor [INFO] Success: Login https://172.23.27.203:8443 with user azkaban
[2017-11-20 21:00:09,136] app.hive_mysql.AzkabanMonitor [INFO] Success: Fetch Executions of a Flow - run_dataimport
[2017-11-20 21:00:09,155] app.hive_mysql.AzkabanMonitor [INFO] Success: Fetch Execution Job Logs - 1670-run_dataimport
[2017-11-20 21:00:53,692] app.hive_mysql.AzkabanMonitor [INFO] URL: https://172.23.27.203:8443
[2017-11-20 21:00:53,877] app.hive_mysql.AzkabanMonitor [INFO] Success: Login https://172.23.27.203:8443 with user azkaban
[2017-11-20 21:00:53,889] app.hive_mysql.AzkabanMonitor [INFO] Success: Fetch Executions of a Flow - run_dataimport
[2017-11-20 21:00:53,905] app.hive_mysql.AzkabanMonitor [INFO] Success: Fetch Execution Job Logs - 1670-run_dataimport
[2017-11-20 21:01:09,669] app.hive_mysql.AzkabanMonitor [INFO] URL: https://172.23.27.203:8443
[2017-11-20 21:01:09,849] app.hive_mysql.AzkabanMonitor [INFO] Success: Login https://172.23.27.203:8443 with user azkaban
[2017-11-20 21:01:09,861] app.hive_mysql.AzkabanMonitor [INFO] Success: Fetch Executions of a Flow - run_dataimport
[2017-11-20 21:01:09,875] app.hive_mysql.AzkabanMonitor [INFO] Success: Fetch Execution Job Logs - 1670-run_dataimport
[2017-11-20 21:05:26,513] app.hive_mysql.AzkabanMonitor [INFO] URL: https://172.23.27.203:8443
[2017-11-20 21:05:26,695] app.hive_mysql.AzkabanMonitor [INFO] Success: Login https://172.23.27.203:8443 with user azkaban
[2017-11-20 21:05:26,695] app.hive_mysql.AzkabanMonitor [DEBUG] {'session.id': 'd2428d19-8946-4343-8cf6-1dd86d84ee52', 'status': 'success'}
[2017-11-20 21:05:26,716] app.hive_mysql.AzkabanMonitor [INFO] Success: Fetch Executions of a Flow - start-syn
[2017-11-20 21:05:26,717] app.hive_mysql.AzkabanMonitor [DEBUG] {'total': 7, 'executions': [{'submitTime': 1509534142141, 'submitUser': 'azkaban', 'startTime': 1509534142295, 'endTime': 1509534142537, 'flowId': 'start-syn', 'projectId': 66, 'execId': 1659, 'status': 'KILLED'}, {'submitTime': 1509438157461, 'submitUser': 'azkaban', 'startTime': 1509438157639, 'endTime': 1509438189521, 'flowId': 'start-syn', 'projectId': 66, 'execId': 1652, 'status': 'KILLED'}, {'submitTime': 1509438102634, 'submitUser': 'azkaban', 'startTime': 1509438102800, 'endTime': 1509438119797, 'flowId': 'start-syn', 'projectId': 66, 'execId': 1651, 'status': 'KILLED'}, {'submitTime': 1509435165409, 'submitUser': 'azkaban', 'startTime': 1509435165665, 'endTime': 1509435183654, 'flowId': 'start-syn', 'projectId': 66, 'execId': 1649, 'status': 'KILLED'}, {'submitTime': 1509435136471, 'submitUser': 'azkaban', 'startTime': 1509435136636, 'endTime': 1509435145535, 'flowId': 'start-syn', 'projectId': 66, 'execId': 1648, 'status': 'KILLED'}, {'submitTime': 1509435094803, 'submitUser': 'azkaban', 'startTime': 1509435095118, 'endTime': 1509435128253, 'flowId': 'start-syn', 'projectId': 66, 'execId': 1647, 'status': 'KILLED'}], 'length': 10, 'project': 'mysql_syn', 'from': 1, 'projectId': 66, 'flow': 'start-syn'}
[2017-11-20 21:05:26,732] app.hive_mysql.AzkabanMonitor [INFO] Success: Fetch Execution Job Logs - 1659-start-syn
[2017-11-20 21:05:26,732] app.hive_mysql.AzkabanMonitor [DEBUG] {'offset': 0, 'data': '', 'length': 0}
[2017-11-20 21:05:43,504] app.hive_mysql.AzkabanMonitor [INFO] URL: https://172.23.27.203:8443
[2017-11-20 21:05:43,682] app.hive_mysql.AzkabanMonitor [INFO] Success: Login https://172.23.27.203:8443 with user azkaban
[2017-11-20 21:05:43,682] app.hive_mysql.AzkabanMonitor [DEBUG] {'session.id': '559f84b2-693f-4baf-8817-84753b00d22d', 'status': 'success'}
[2017-11-20 21:05:43,690] app.hive_mysql.AzkabanMonitor [INFO] Success: Fetch Running Executions of a Flow - start-syn
[2017-11-20 21:05:43,691] app.hive_mysql.AzkabanMonitor [DEBUG] {}
[2017-11-20 21:05:43,691] app.hive_mysql.AzkabanMonitor [ERROR] Flow 'start-syn' is not running.
[2017-11-20 21:05:43,836] app.hive_mysql.AzkabanMonitor [INFO] Success: Execute Flow - start-syn
[2017-11-20 21:05:43,836] app.hive_mysql.AzkabanMonitor [DEBUG] {'project': 'mysql_syn', 'message': 'Execution submitted successfully with exec id 1707', 'flow': 'start-syn', 'execid': 1707}
[2017-11-20 21:05:43,844] app.hive_mysql.AzkabanMonitor [INFO] Success: Fetch Executions of a Flow - start-syn
[2017-11-20 21:05:43,844] app.hive_mysql.AzkabanMonitor [DEBUG] {'total': 8, 'executions': [{'submitTime': 1509589294715, 'submitUser': 'azkaban', 'startTime': 1509589294866, 'endTime': 1509589320878, 'flowId': 'start-syn', 'projectId': 66, 'execId': 1662, 'status': 'KILLED'}, {'submitTime': 1509534142141, 'submitUser': 'azkaban', 'startTime': 1509534142295, 'endTime': 1509534142537, 'flowId': 'start-syn', 'projectId': 66, 'execId': 1659, 'status': 'KILLED'}, {'submitTime': 1509438157461, 'submitUser': 'azkaban', 'startTime': 1509438157639, 'endTime': 1509438189521, 'flowId': 'start-syn', 'projectId': 66, 'execId': 1652, 'status': 'KILLED'}, {'submitTime': 1509438102634, 'submitUser': 'azkaban', 'startTime': 1509438102800, 'endTime': 1509438119797, 'flowId': 'start-syn', 'projectId': 66, 'execId': 1651, 'status': 'KILLED'}, {'submitTime': 1509435165409, 'submitUser': 'azkaban', 'startTime': 1509435165665, 'endTime': 1509435183654, 'flowId': 'start-syn', 'projectId': 66, 'execId': 1649, 'status': 'KILLED'}, {'submitTime': 1509435136471, 'submitUser': 'azkaban', 'startTime': 1509435136636, 'endTime': 1509435145535, 'flowId': 'start-syn', 'projectId': 66, 'execId': 1648, 'status': 'KILLED'}, {'submitTime': 1509435094803, 'submitUser': 'azkaban', 'startTime': 1509435095118, 'endTime': 1509435128253, 'flowId': 'start-syn', 'projectId': 66, 'execId': 1647, 'status': 'KILLED'}], 'length': 10, 'project': 'mysql_syn', 'from': 1, 'projectId': 66, 'flow': 'start-syn'}
[2017-11-20 21:05:43,875] app.hive_mysql.AzkabanMonitor [INFO] Success: Fetch Execution Job Logs - 1662-start-syn
[2017-11-20 21:05:43,875] app.hive_mysql.AzkabanMonitor [DEBUG] {'offset': 0, 'data': '02-11-2017 10:21:34 CST start-syn INFO - Starting job start-syn at 1509589294870\n02-11-2017 10:21:34 CST start-syn INFO - azkaban.webserver.url property was not set\n02-11-2017 10:21:34 CST start-syn INFO - job JVM args: -Dazkaban.flowid=start-syn -Dazkaban.execid=1662 -Dazkaban.jobid=start-syn\n02-11-2017 10:21:34 CST start-syn INFO - Building command job executor. \n02-11-2017 10:21:35 CST start-syn INFO - Memory granted for job start-syn\n02-11-2017 10:21:35 CST start-syn INFO - 1 commands to execute.\n02-11-2017 10:21:35 CST start-syn INFO - cwd=/home/azkaban/azkaban_bk/azkaban-exec-server-3.35.0-22-g5517d50/executions/1662\n02-11-2017 10:21:35 CST start-syn INFO - effective user is: azkaban\n02-11-2017 10:21:35 CST start-syn INFO - Command: sh /home/dm/sbin/start-syn.sh\n02-11-2017 10:21:35 CST start-syn INFO - Environment variables: {JOB_OUTPUT_PROP_FILE=/home/azkaban/azkaban_bk/azkaban-exec-server-3.35.0-22-g5517d50/executions/1662/start-syn_output_9056908567353890154_tmp, JOB_PROP_FILE=/home/azkaban/azkaban_bk/azkaban-exec-server-3.35.0-22-g5517d50/executions/1662/start-syn_props_5235610836328658369_tmp, KRB5CCNAME=/tmp/krb5cc__mysql_syn__start-syn__start-syn__1662__azkaban, JOB_NAME=start-syn}\n02-11-2017 10:21:35 CST start-syn INFO - Working directory: /home/azkaban/azkaban_bk/azkaban-exec-server-3.35.0-22-g5517d50/executions/1662\n02-11-2017 10:21:35 CST start-syn INFO - serverLatestID: 1004491 clientLatestID:993775\n02-11-2017 10:21:35 CST start-syn INFO - 2017-11-02 10:21:35 INFO  DBSyn2:53 - No data to insert\n02-11-2017 10:21:36 CST start-syn INFO - serverLatestID: 1004491 clientLatestID:993775\n02-11-2017 10:21:36 CST start-syn INFO - 2017-11-02 10:21:36 INFO  DBSyn2:53 - No data to insert\n02-11-2017 10:21:37 CST start-syn INFO - serverLatestID: 1004491 clientLatestID:993775\n02-11-2017 10:21:37 CST start-syn INFO - 2017-11-02 10:21:37 INFO  DBSyn2:53 - No data to insert\n02-11-2017 10:21:38 CST start-syn INFO - serverLatestID: 1004491 clientLatestID:993775\n02-11-2017 10:21:38 CST start-syn INFO - 2017-11-02 10:21:38 INFO  DBSyn2:53 - No data to insert\n02-11-2017 10:21:39 CST start-syn INFO - serverLatestID: 1004491 clientLatestID:993775\n02-11-2017 10:21:39 CST start-syn INFO - 2017-11-02 10:21:39 INFO  DBSyn2:53 - No data to insert\n02-11-2017 10:21:40 CST start-syn INFO - serverLatestID: 1004491 clientLatestID:993775\n02-11-2017 10:21:40 CST start-syn INFO - 2017-11-02 10:21:40 INFO  DBSyn2:53 - No data to insert\n02-11-2017 10:21:41 CST start-syn INFO - serverLatestID: 1004491 clientLatestID:993775\n02-11-2017 10:21:41 CST start-syn INFO - 2017-11-02 10:21:41 INFO  DBSyn2:53 - No data to insert\n02-11-2017 10:21:42 CST start-syn INFO - serverLatestID: 1004491 clientLatestID:993775\n02-11-2017 10:21:42 CST start-syn INFO - 2017-11-02 10:21:42 INFO  DBSyn2:53 - No data to insert\n02-11-2017 10:21:43 CST start-syn INFO - serverLatestID: 1004491 clientLatestID:993775\n02-11-2017 10:21:43 CST start-syn INFO - 2017-11-02 10:21:43 INFO  DBSyn2:53 - No data to insert\n02-11-2017 10:21:44 CST start-syn INFO - serverLatestID: 1004491 clientLatestID:993775\n02-11-2017 10:21:44 CST start-syn INFO - 2017-11-02 10:21:44 INFO  DBSyn2:53 - No data to insert\n02-11-2017 10:21:45 CST start-syn INFO - serverLatestID: 1004491 clientLatestID:993775\n02-11-2017 10:21:45 CST start-syn INFO - 2017-11-02 10:21:45 INFO  DBSyn2:53 - No data to insert\n02-11-2017 10:21:46 CST start-syn INFO - serverLatestID: 1004491 clientLatestID:993775\n02-11-2017 10:21:46 CST start-syn INFO - 2017-11-02 10:21:46 INFO  DBSyn2:53 - No data to insert\n02-11-2017 10:21:47 CST start-syn INFO - serverLatestID: 1004491 clientLatestID:993775\n02-11-2017 10:21:47 CST start-syn INFO - 2017-11-02 10:21:47 INFO  DBSyn2:53 - No data to insert\n02-11-2017 10:21:48 CST start-syn INFO - serverLatestID: 1004491 clientLatestID:993775\n02-11-2017 10:21:48 CST start-syn INFO - 2017-11-02 10:21:48 INFO  DBSyn2:53 - No data to insert\n02-11-2017 10:21:49 CST start-syn INFO - serverLatestID: 1004491 clientLatestID:993775\n02-11-2017 10:21:49 CST start-syn INFO - 2017-11-02 10:21:49 INFO  DBSyn2:53 - No data to insert\n02-11-2017 10:21:50 CST start-syn INFO - serverLatestID: 1004491 clientLatestID:993775\n02-11-2017 10:21:50 CST start-syn INFO - 2017-11-02 10:21:50 INFO  DBSyn2:53 - No data to insert\n02-11-2017 10:21:51 CST start-syn INFO - serverLatestID: 1004491 clientLatestID:993775\n02-11-2017 10:21:51 CST start-syn INFO - 2017-11-02 10:21:51 INFO  DBSyn2:53 - No data to insert\n02-11-2017 10:21:52 CST start-syn INFO - serverLatestID: 1004491 clientLatestID:993775\n02-11-2017 10:21:52 CST start-syn INFO - 2017-11-02 10:21:52 INFO  DBSyn2:53 - No data to insert\n02-11-2017 10:21:53 CST start-syn INFO - serverLatestID: 1004491 clientLatestID:993775\n02-11-2017 10:21:53 CST start-syn INFO - 2017-11-02 10:21:53 INFO  DBSyn2:53 - No data to insert\n02-11-2017 10:21:54 CST start-syn INFO - serverLatestID: 1004491 clientLatestID:993775\n02-11-2017 10:21:54 CST start-syn INFO - 2017-11-02 10:21:54 INFO  DBSyn2:53 - No data to insert\n02-11-2017 10:21:55 CST start-syn INFO - serverLatestID: 1004491 clientLatestID:993775\n02-11-2017 10:21:55 CST start-syn INFO - 2017-11-02 10:21:55 INFO  DBSyn2:53 - No data to insert\n02-11-2017 10:21:56 CST start-syn INFO - serverLatestID: 1004491 clientLatestID:993775\n02-11-2017 10:21:56 CST start-syn INFO - 2017-11-02 10:21:56 INFO  DBSyn2:53 - No data to insert\n02-11-2017 10:21:57 CST start-syn INFO - serverLatestID: 1004491 clientLatestID:993775\n02-11-2017 10:21:57 CST start-syn INFO - 2017-11-02 10:21:57 INFO  DBSyn2:53 - No data to insert\n02-11-2017 10:21:58 CST start-syn INFO - serverLatestID: 1004491 clientLatestID:993775\n02-11-2017 10:21:58 CST start-syn INFO - 2017-11-02 10:21:58 INFO  DBSyn2:53 - No data to insert\n02-11-2017 10:21:59 CST start-syn INFO - serverLatestID: 1004491 clientLatestID:993775\n02-11-2017 10:21:59 CST start-syn INFO - 2017-11-02 10:21:59 INFO  DBSyn2:53 - No data to insert\n02-11-2017 10:21:59 CST start-syn ERROR - Kill has been called.\n02-11-2017 10:22:00 CST start-syn INFO - serverLatestID: 1004491 clientLatestID:993775\n02-11-2017 10:22:00 CST start-syn INFO - Process completed unsuccessfully in 25 seconds.\n02-11-2017 10:22:00 CST start-syn ERROR - Job run killed!\njava.lang.RuntimeException: azkaban.jobExecutor.utils.process.ProcessFailureException\n\tat azkaban.jobExecutor.ProcessJob.run(ProcessJob.java:297)\n\tat azkaban.execapp.JobRunner.runJob(JobRunner.java:748)\n\tat azkaban.execapp.JobRunner.doRun(JobRunner.java:591)\n\tat azkaban.execapp.JobRunner.run(JobRunner.java:552)\n\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: azkaban.jobExecutor.utils.process.ProcessFailureException\n\tat azkaban.jobExecutor.utils.process.AzkabanProcess.run(AzkabanProcess.java:130)\n\tat azkaban.jobExecutor.ProcessJob.run(ProcessJob.java:289)\n\t... 8 more\n02-11-2017 10:22:00 CST start-syn ERROR - azkaban.jobExecutor.utils.process.ProcessFailureException cause: azkaban.jobExecutor.utils.process.ProcessFailureException\n02-11-2017 10:22:00 CST start-syn INFO - Finishing job start-syn at 1509589320780 with status KILLED\n', 'length': 7401}
[2017-11-20 21:10:53,883] app.hive_mysql.AzkabanMonitor [INFO] URL: https://172.23.27.203:8443
[2017-11-20 21:10:54,068] app.hive_mysql.AzkabanMonitor [INFO] Success: Login https://172.23.27.203:8443 with user azkaban
[2017-11-20 21:10:54,068] app.hive_mysql.AzkabanMonitor [DEBUG] {'session.id': '5a037865-bc2b-4a8c-a122-d77fac6c3c41', 'status': 'success'}
[2017-11-20 21:10:54,078] app.hive_mysql.AzkabanMonitor [INFO] Success: Fetch Running Executions of a Flow - start-syn
[2017-11-20 21:10:54,078] app.hive_mysql.AzkabanMonitor [DEBUG] {'execIds': [1707]}
[2017-11-20 21:10:54,088] app.hive_mysql.AzkabanMonitor [INFO] Success: Resume a Flow Execution - 1707
[2017-11-20 21:10:54,088] app.hive_mysql.AzkabanMonitor [DEBUG] {}
[2017-11-20 21:10:54,092] app.hive_mysql.AzkabanMonitor [INFO] Success: Fetch Running Executions of a Flow - start-syn
[2017-11-20 21:10:54,092] app.hive_mysql.AzkabanMonitor [DEBUG] {'execIds': [1707]}
[2017-11-20 21:10:54,140] app.hive_mysql.AzkabanMonitor [INFO] Success: Pause a Flow Execution - 1707
[2017-11-20 21:10:54,140] app.hive_mysql.AzkabanMonitor [DEBUG] {}
[2017-11-20 21:10:54,152] app.hive_mysql.AzkabanMonitor [INFO] Success: Fetch Executions of a Flow - start-syn
[2017-11-20 21:10:54,152] app.hive_mysql.AzkabanMonitor [DEBUG] {'total': 8, 'executions': [{'submitTime': 1509589294715, 'submitUser': 'azkaban', 'startTime': 1509589294866, 'endTime': 1509589320878, 'flowId': 'start-syn', 'projectId': 66, 'execId': 1662, 'status': 'KILLED'}, {'submitTime': 1509534142141, 'submitUser': 'azkaban', 'startTime': 1509534142295, 'endTime': 1509534142537, 'flowId': 'start-syn', 'projectId': 66, 'execId': 1659, 'status': 'KILLED'}, {'submitTime': 1509438157461, 'submitUser': 'azkaban', 'startTime': 1509438157639, 'endTime': 1509438189521, 'flowId': 'start-syn', 'projectId': 66, 'execId': 1652, 'status': 'KILLED'}, {'submitTime': 1509438102634, 'submitUser': 'azkaban', 'startTime': 1509438102800, 'endTime': 1509438119797, 'flowId': 'start-syn', 'projectId': 66, 'execId': 1651, 'status': 'KILLED'}, {'submitTime': 1509435165409, 'submitUser': 'azkaban', 'startTime': 1509435165665, 'endTime': 1509435183654, 'flowId': 'start-syn', 'projectId': 66, 'execId': 1649, 'status': 'KILLED'}, {'submitTime': 1509435136471, 'submitUser': 'azkaban', 'startTime': 1509435136636, 'endTime': 1509435145535, 'flowId': 'start-syn', 'projectId': 66, 'execId': 1648, 'status': 'KILLED'}, {'submitTime': 1509435094803, 'submitUser': 'azkaban', 'startTime': 1509435095118, 'endTime': 1509435128253, 'flowId': 'start-syn', 'projectId': 66, 'execId': 1647, 'status': 'KILLED'}], 'length': 10, 'project': 'mysql_syn', 'from': 1, 'projectId': 66, 'flow': 'start-syn'}
[2017-11-20 21:10:54,167] app.hive_mysql.AzkabanMonitor [INFO] Success: Fetch Execution Job Logs - 1662-start-syn
[2017-11-20 21:10:54,167] app.hive_mysql.AzkabanMonitor [DEBUG] {'offset': 0, 'data': '02-11-2017 10:21:34 CST start-syn INFO - Starting job start-syn at 1509589294870\n02-11-2017 10:21:34 CST start-syn INFO - azkaban.webserver.url property was not set\n02-11-2017 10:21:34 CST start-syn INFO - job JVM args: -Dazkaban.flowid=start-syn -Dazkaban.execid=1662 -Dazkaban.jobid=start-syn\n02-11-2017 10:21:34 CST start-syn INFO - Building command job executor. \n02-11-2017 10:21:35 CST start-syn INFO - Memory granted for job start-syn\n02-11-2017 10:21:35 CST start-syn INFO - 1 commands to execute.\n02-11-2017 10:21:35 CST start-syn INFO - cwd=/home/azkaban/azkaban_bk/azkaban-exec-server-3.35.0-22-g5517d50/executions/1662\n02-11-2017 10:21:35 CST start-syn INFO - effective user is: azkaban\n02-11-2017 10:21:35 CST start-syn INFO - Command: sh /home/dm/sbin/start-syn.sh\n02-11-2017 10:21:35 CST start-syn INFO - Environment variables: {JOB_OUTPUT_PROP_FILE=/home/azkaban/azkaban_bk/azkaban-exec-server-3.35.0-22-g5517d50/executions/1662/start-syn_output_9056908567353890154_tmp, JOB_PROP_FILE=/home/azkaban/azkaban_bk/azkaban-exec-server-3.35.0-22-g5517d50/executions/1662/start-syn_props_5235610836328658369_tmp, KRB5CCNAME=/tmp/krb5cc__mysql_syn__start-syn__start-syn__1662__azkaban, JOB_NAME=start-syn}\n02-11-2017 10:21:35 CST start-syn INFO - Working directory: /home/azkaban/azkaban_bk/azkaban-exec-server-3.35.0-22-g5517d50/executions/1662\n02-11-2017 10:21:35 CST start-syn INFO - serverLatestID: 1004491 clientLatestID:993775\n02-11-2017 10:21:35 CST start-syn INFO - 2017-11-02 10:21:35 INFO  DBSyn2:53 - No data to insert\n02-11-2017 10:21:36 CST start-syn INFO - serverLatestID: 1004491 clientLatestID:993775\n02-11-2017 10:21:36 CST start-syn INFO - 2017-11-02 10:21:36 INFO  DBSyn2:53 - No data to insert\n02-11-2017 10:21:37 CST start-syn INFO - serverLatestID: 1004491 clientLatestID:993775\n02-11-2017 10:21:37 CST start-syn INFO - 2017-11-02 10:21:37 INFO  DBSyn2:53 - No data to insert\n02-11-2017 10:21:38 CST start-syn INFO - serverLatestID: 1004491 clientLatestID:993775\n02-11-2017 10:21:38 CST start-syn INFO - 2017-11-02 10:21:38 INFO  DBSyn2:53 - No data to insert\n02-11-2017 10:21:39 CST start-syn INFO - serverLatestID: 1004491 clientLatestID:993775\n02-11-2017 10:21:39 CST start-syn INFO - 2017-11-02 10:21:39 INFO  DBSyn2:53 - No data to insert\n02-11-2017 10:21:40 CST start-syn INFO - serverLatestID: 1004491 clientLatestID:993775\n02-11-2017 10:21:40 CST start-syn INFO - 2017-11-02 10:21:40 INFO  DBSyn2:53 - No data to insert\n02-11-2017 10:21:41 CST start-syn INFO - serverLatestID: 1004491 clientLatestID:993775\n02-11-2017 10:21:41 CST start-syn INFO - 2017-11-02 10:21:41 INFO  DBSyn2:53 - No data to insert\n02-11-2017 10:21:42 CST start-syn INFO - serverLatestID: 1004491 clientLatestID:993775\n02-11-2017 10:21:42 CST start-syn INFO - 2017-11-02 10:21:42 INFO  DBSyn2:53 - No data to insert\n02-11-2017 10:21:43 CST start-syn INFO - serverLatestID: 1004491 clientLatestID:993775\n02-11-2017 10:21:43 CST start-syn INFO - 2017-11-02 10:21:43 INFO  DBSyn2:53 - No data to insert\n02-11-2017 10:21:44 CST start-syn INFO - serverLatestID: 1004491 clientLatestID:993775\n02-11-2017 10:21:44 CST start-syn INFO - 2017-11-02 10:21:44 INFO  DBSyn2:53 - No data to insert\n02-11-2017 10:21:45 CST start-syn INFO - serverLatestID: 1004491 clientLatestID:993775\n02-11-2017 10:21:45 CST start-syn INFO - 2017-11-02 10:21:45 INFO  DBSyn2:53 - No data to insert\n02-11-2017 10:21:46 CST start-syn INFO - serverLatestID: 1004491 clientLatestID:993775\n02-11-2017 10:21:46 CST start-syn INFO - 2017-11-02 10:21:46 INFO  DBSyn2:53 - No data to insert\n02-11-2017 10:21:47 CST start-syn INFO - serverLatestID: 1004491 clientLatestID:993775\n02-11-2017 10:21:47 CST start-syn INFO - 2017-11-02 10:21:47 INFO  DBSyn2:53 - No data to insert\n02-11-2017 10:21:48 CST start-syn INFO - serverLatestID: 1004491 clientLatestID:993775\n02-11-2017 10:21:48 CST start-syn INFO - 2017-11-02 10:21:48 INFO  DBSyn2:53 - No data to insert\n02-11-2017 10:21:49 CST start-syn INFO - serverLatestID: 1004491 clientLatestID:993775\n02-11-2017 10:21:49 CST start-syn INFO - 2017-11-02 10:21:49 INFO  DBSyn2:53 - No data to insert\n02-11-2017 10:21:50 CST start-syn INFO - serverLatestID: 1004491 clientLatestID:993775\n02-11-2017 10:21:50 CST start-syn INFO - 2017-11-02 10:21:50 INFO  DBSyn2:53 - No data to insert\n02-11-2017 10:21:51 CST start-syn INFO - serverLatestID: 1004491 clientLatestID:993775\n02-11-2017 10:21:51 CST start-syn INFO - 2017-11-02 10:21:51 INFO  DBSyn2:53 - No data to insert\n02-11-2017 10:21:52 CST start-syn INFO - serverLatestID: 1004491 clientLatestID:993775\n02-11-2017 10:21:52 CST start-syn INFO - 2017-11-02 10:21:52 INFO  DBSyn2:53 - No data to insert\n02-11-2017 10:21:53 CST start-syn INFO - serverLatestID: 1004491 clientLatestID:993775\n02-11-2017 10:21:53 CST start-syn INFO - 2017-11-02 10:21:53 INFO  DBSyn2:53 - No data to insert\n02-11-2017 10:21:54 CST start-syn INFO - serverLatestID: 1004491 clientLatestID:993775\n02-11-2017 10:21:54 CST start-syn INFO - 2017-11-02 10:21:54 INFO  DBSyn2:53 - No data to insert\n02-11-2017 10:21:55 CST start-syn INFO - serverLatestID: 1004491 clientLatestID:993775\n02-11-2017 10:21:55 CST start-syn INFO - 2017-11-02 10:21:55 INFO  DBSyn2:53 - No data to insert\n02-11-2017 10:21:56 CST start-syn INFO - serverLatestID: 1004491 clientLatestID:993775\n02-11-2017 10:21:56 CST start-syn INFO - 2017-11-02 10:21:56 INFO  DBSyn2:53 - No data to insert\n02-11-2017 10:21:57 CST start-syn INFO - serverLatestID: 1004491 clientLatestID:993775\n02-11-2017 10:21:57 CST start-syn INFO - 2017-11-02 10:21:57 INFO  DBSyn2:53 - No data to insert\n02-11-2017 10:21:58 CST start-syn INFO - serverLatestID: 1004491 clientLatestID:993775\n02-11-2017 10:21:58 CST start-syn INFO - 2017-11-02 10:21:58 INFO  DBSyn2:53 - No data to insert\n02-11-2017 10:21:59 CST start-syn INFO - serverLatestID: 1004491 clientLatestID:993775\n02-11-2017 10:21:59 CST start-syn INFO - 2017-11-02 10:21:59 INFO  DBSyn2:53 - No data to insert\n02-11-2017 10:21:59 CST start-syn ERROR - Kill has been called.\n02-11-2017 10:22:00 CST start-syn INFO - serverLatestID: 1004491 clientLatestID:993775\n02-11-2017 10:22:00 CST start-syn INFO - Process completed unsuccessfully in 25 seconds.\n02-11-2017 10:22:00 CST start-syn ERROR - Job run killed!\njava.lang.RuntimeException: azkaban.jobExecutor.utils.process.ProcessFailureException\n\tat azkaban.jobExecutor.ProcessJob.run(ProcessJob.java:297)\n\tat azkaban.execapp.JobRunner.runJob(JobRunner.java:748)\n\tat azkaban.execapp.JobRunner.doRun(JobRunner.java:591)\n\tat azkaban.execapp.JobRunner.run(JobRunner.java:552)\n\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: azkaban.jobExecutor.utils.process.ProcessFailureException\n\tat azkaban.jobExecutor.utils.process.AzkabanProcess.run(AzkabanProcess.java:130)\n\tat azkaban.jobExecutor.ProcessJob.run(ProcessJob.java:289)\n\t... 8 more\n02-11-2017 10:22:00 CST start-syn ERROR - azkaban.jobExecutor.utils.process.ProcessFailureException cause: azkaban.jobExecutor.utils.process.ProcessFailureException\n02-11-2017 10:22:00 CST start-syn INFO - Finishing job start-syn at 1509589320780 with status KILLED\n', 'length': 7401}
[2017-12-26 21:30:44,740] app.hive_mysql.AzkabanMonitor [INFO] URL: https://172.23.27.203:8443
[2017-12-26 21:30:45,078] app.hive_mysql.AzkabanMonitor [INFO] Success: Login https://172.23.27.203:8443 with user azkaban
[2017-12-26 21:30:45,096] app.hive_mysql.AzkabanMonitor [INFO] Success: Fetch Executions of a Flow - putInfo
[2017-12-26 21:31:21,467] app.hive_mysql.AzkabanMonitor [INFO] URL: https://172.23.27.203:8443
[2017-12-26 21:31:21,790] app.hive_mysql.AzkabanMonitor [INFO] Success: Login https://172.23.27.203:8443 with user azkaban
[2017-12-26 21:31:21,806] app.hive_mysql.AzkabanMonitor [INFO] Success: Fetch Executions of a Flow - putInfo
[2017-12-26 21:32:06,483] app.hive_mysql.AzkabanMonitor [INFO] URL: https://172.23.27.203:8443
[2017-12-26 21:32:06,776] app.hive_mysql.AzkabanMonitor [INFO] Success: Login https://172.23.27.203:8443 with user azkaban
[2017-12-26 21:32:06,788] app.hive_mysql.AzkabanMonitor [INFO] Success: Fetch Executions of a Flow - putInfo
[2017-12-26 21:32:06,802] app.hive_mysql.AzkabanMonitor [INFO] Success: Fetch Execution Job Logs - 1770-concat-table
[2017-12-26 21:32:06,809] app.hive_mysql.AzkabanMonitor [INFO] Success: Fetch Executions of a Flow - putInfo
[2017-12-26 21:32:06,814] app.hive_mysql.AzkabanMonitor [INFO] Success: Fetch Execution Job Logs - 1770-finish
[2017-12-26 21:32:06,820] app.hive_mysql.AzkabanMonitor [INFO] Success: Fetch Executions of a Flow - putInfo
[2017-12-26 21:32:06,825] app.hive_mysql.AzkabanMonitor [INFO] Success: Fetch Execution Job Logs - 1770-general_info
[2017-12-26 21:32:06,831] app.hive_mysql.AzkabanMonitor [INFO] Success: Fetch Executions of a Flow - putInfo
[2017-12-26 21:32:06,835] app.hive_mysql.AzkabanMonitor [INFO] Success: Fetch Execution Job Logs - 1770-putInfo
[2017-12-26 21:32:06,842] app.hive_mysql.AzkabanMonitor [INFO] Success: Fetch Executions of a Flow - putInfo
[2017-12-26 21:32:06,847] app.hive_mysql.AzkabanMonitor [INFO] Success: Fetch Execution Job Logs - 1770-start
[2017-12-26 21:32:06,853] app.hive_mysql.AzkabanMonitor [INFO] Success: Fetch Executions of a Flow - putInfo
[2017-12-26 21:32:06,859] app.hive_mysql.AzkabanMonitor [INFO] Success: Fetch Execution Job Logs - 1770-tables_0
[2017-12-26 21:32:06,864] app.hive_mysql.AzkabanMonitor [INFO] Success: Fetch Executions of a Flow - putInfo
[2017-12-26 21:32:06,870] app.hive_mysql.AzkabanMonitor [INFO] Success: Fetch Execution Job Logs - 1770-tables_1
[2017-12-26 21:32:06,875] app.hive_mysql.AzkabanMonitor [INFO] Success: Fetch Executions of a Flow - putInfo
[2017-12-26 21:32:06,880] app.hive_mysql.AzkabanMonitor [INFO] Success: Fetch Execution Job Logs - 1770-tables_2
[2017-12-26 21:32:20,645] app.hive_mysql.AzkabanMonitor [INFO] URL: https://172.23.27.203:8443
[2017-12-26 21:32:20,949] app.hive_mysql.AzkabanMonitor [INFO] Success: Login https://172.23.27.203:8443 with user azkaban
[2017-12-26 21:32:20,966] app.hive_mysql.AzkabanMonitor [INFO] Success: Fetch Executions of a Flow - putInfo
[2017-12-26 21:32:20,975] app.hive_mysql.AzkabanMonitor [INFO] Success: Fetch Execution Job Logs - 1770-concat-table
[2017-12-26 21:32:20,984] app.hive_mysql.AzkabanMonitor [INFO] Success: Fetch Executions of a Flow - putInfo
[2017-12-26 21:32:20,992] app.hive_mysql.AzkabanMonitor [INFO] Success: Fetch Execution Job Logs - 1770-finish
[2017-12-26 21:32:21,000] app.hive_mysql.AzkabanMonitor [INFO] Success: Fetch Executions of a Flow - putInfo
[2017-12-26 21:32:21,006] app.hive_mysql.AzkabanMonitor [INFO] Success: Fetch Execution Job Logs - 1770-general_info
[2017-12-26 21:32:21,014] app.hive_mysql.AzkabanMonitor [INFO] Success: Fetch Executions of a Flow - putInfo
[2017-12-26 21:32:21,020] app.hive_mysql.AzkabanMonitor [INFO] Success: Fetch Execution Job Logs - 1770-putInfo
[2017-12-26 21:32:21,028] app.hive_mysql.AzkabanMonitor [INFO] Success: Fetch Executions of a Flow - putInfo
[2017-12-26 21:32:21,034] app.hive_mysql.AzkabanMonitor [INFO] Success: Fetch Execution Job Logs - 1770-start
[2017-12-26 21:32:21,042] app.hive_mysql.AzkabanMonitor [INFO] Success: Fetch Executions of a Flow - putInfo
[2017-12-26 21:32:21,050] app.hive_mysql.AzkabanMonitor [INFO] Success: Fetch Execution Job Logs - 1770-tables_0
[2017-12-26 21:32:21,057] app.hive_mysql.AzkabanMonitor [INFO] Success: Fetch Executions of a Flow - putInfo
[2017-12-26 21:32:21,066] app.hive_mysql.AzkabanMonitor [INFO] Success: Fetch Execution Job Logs - 1770-tables_1
[2017-12-26 21:32:21,074] app.hive_mysql.AzkabanMonitor [INFO] Success: Fetch Executions of a Flow - putInfo
[2017-12-26 21:32:21,082] app.hive_mysql.AzkabanMonitor [INFO] Success: Fetch Execution Job Logs - 1770-tables_2
[2017-12-26 21:32:46,301] app.hive_mysql.AzkabanMonitor [INFO] URL: https://172.23.27.203:8443
[2017-12-26 21:32:46,620] app.hive_mysql.AzkabanMonitor [INFO] Success: Login https://172.23.27.203:8443 with user azkaban
[2017-12-26 21:32:46,637] app.hive_mysql.AzkabanMonitor [INFO] Success: Fetch Executions of a Flow - putInfo
[2017-12-26 21:32:46,646] app.hive_mysql.AzkabanMonitor [INFO] Success: Fetch Execution Job Logs - 1770-concat-table
[2017-12-26 21:32:46,654] app.hive_mysql.AzkabanMonitor [INFO] Success: Fetch Executions of a Flow - putInfo
[2017-12-26 21:32:46,661] app.hive_mysql.AzkabanMonitor [INFO] Success: Fetch Execution Job Logs - 1770-finish
[2017-12-26 21:32:46,669] app.hive_mysql.AzkabanMonitor [INFO] Success: Fetch Executions of a Flow - putInfo
[2017-12-26 21:32:46,676] app.hive_mysql.AzkabanMonitor [INFO] Success: Fetch Execution Job Logs - 1770-general_info
[2017-12-26 21:32:46,684] app.hive_mysql.AzkabanMonitor [INFO] Success: Fetch Executions of a Flow - putInfo
[2017-12-26 21:32:46,692] app.hive_mysql.AzkabanMonitor [INFO] Success: Fetch Execution Job Logs - 1770-putInfo
[2017-12-26 21:32:46,700] app.hive_mysql.AzkabanMonitor [INFO] Success: Fetch Executions of a Flow - putInfo
[2017-12-26 21:32:46,706] app.hive_mysql.AzkabanMonitor [INFO] Success: Fetch Execution Job Logs - 1770-start
[2017-12-26 21:32:46,713] app.hive_mysql.AzkabanMonitor [INFO] Success: Fetch Executions of a Flow - putInfo
[2017-12-26 21:32:46,721] app.hive_mysql.AzkabanMonitor [INFO] Success: Fetch Execution Job Logs - 1770-tables_0
[2017-12-26 21:32:46,729] app.hive_mysql.AzkabanMonitor [INFO] Success: Fetch Executions of a Flow - putInfo
[2017-12-26 21:32:46,737] app.hive_mysql.AzkabanMonitor [INFO] Success: Fetch Execution Job Logs - 1770-tables_1
[2017-12-26 21:32:46,745] app.hive_mysql.AzkabanMonitor [INFO] Success: Fetch Executions of a Flow - putInfo
[2017-12-26 21:32:46,752] app.hive_mysql.AzkabanMonitor [INFO] Success: Fetch Execution Job Logs - 1770-tables_2
[2018-01-19 16:18:17,490] __main__ [INFO] URL: https://172.23.27.203:8443
[2018-01-19 16:18:17,694] __main__ [INFO] Success: Login https://172.23.27.203:8443 with user azkaban
[2018-01-19 16:18:17,702] __main__ [INFO] Success: Fetch Running Executions of a Flow - putInfo
[2018-01-19 16:18:17,987] __main__ [INFO] Success: Execute Flow - putInfo
[2018-01-19 16:18:17,993] __main__ [INFO] Success: Fetch Running Executions of a Flow - putInfo
[2018-01-19 16:19:17,208] __main__ [INFO] URL: https://172.23.27.203:8443
[2018-01-19 16:19:17,385] __main__ [INFO] Success: Login https://172.23.27.203:8443 with user azkaban
[2018-01-19 16:19:17,394] __main__ [INFO] Success: Fetch Running Executions of a Flow - putInfo
[2018-01-19 16:19:44,825] __main__ [INFO] URL: https://172.23.27.203:8443
[2018-01-19 16:19:45,001] __main__ [INFO] Success: Login https://172.23.27.203:8443 with user azkaban
[2018-01-19 16:19:45,017] __main__ [INFO] Success: Fetch Running Executions of a Flow - putInfo
[2018-01-19 16:19:45,286] __main__ [INFO] Success: Execute Flow - putInfo
[2018-01-19 16:19:45,293] __main__ [INFO] Success: Fetch Running Executions of a Flow - putInfo
[2018-01-19 16:19:48,951] __main__ [INFO] URL: https://172.23.27.203:8443
[2018-01-19 16:19:49,121] __main__ [INFO] Success: Login https://172.23.27.203:8443 with user azkaban
[2018-01-19 16:19:49,128] __main__ [INFO] Success: Fetch Running Executions of a Flow - putInfo
[2018-01-19 16:19:51,481] __main__ [INFO] URL: https://172.23.27.203:8443
[2018-01-19 16:19:51,658] __main__ [INFO] Success: Login https://172.23.27.203:8443 with user azkaban
[2018-01-19 16:19:51,679] __main__ [INFO] Success: Fetch Running Executions of a Flow - putInfo
[2018-01-19 16:24:29,816] __main__ [INFO] URL: https://172.23.27.203:8443
[2018-01-19 16:24:29,987] __main__ [INFO] Success: Login https://172.23.27.203:8443 with user azkaban
[2018-01-19 16:24:29,995] __main__ [INFO] Success: Fetch Running Executions of a Flow - putInfo
[2018-01-19 17:02:56,408] __main__ [INFO] URL: https://172.23.27.203:8443
[2018-01-19 17:02:56,582] __main__ [INFO] Success: Login https://172.23.27.203:8443 with user azkaban
[2018-01-19 17:02:56,590] __main__ [INFO] Success: Fetch Running Executions of a Flow - putInfo
[2018-01-19 17:03:02,267] __main__ [INFO] URL: https://172.23.27.203:8443
[2018-01-19 17:03:02,446] __main__ [INFO] Success: Login https://172.23.27.203:8443 with user azkaban
[2018-01-19 17:03:02,463] __main__ [INFO] Success: Fetch Running Executions of a Flow - putInfo
[2018-01-28 13:24:17,256] __main__ [INFO] URL: https://172.23.27.203:8443
[2018-01-29 19:40:39,915] __main__ [INFO] URL: https://172.23.27.203:8443
[2018-01-29 19:40:40,102] __main__ [INFO] Success: Login https://172.23.27.203:8443 with user azkaban
[2018-01-29 19:40:40,111] __main__ [INFO] Success: Fetch Running Executions of a Flow - putInfo
